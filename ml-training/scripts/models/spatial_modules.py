"""
ç©ºé—´ç‰¹å¾æå–æ¨¡å—
å®ç°å¤šå°ºåº¦CNNã€ç©ºé—´é‡‘å­—å¡”æ± åŒ–ã€é€šé“æ³¨æ„åŠ›ç­‰ç©ºé—´ç‰¹å¾æå–ç»„ä»¶
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Tuple, List, Optional, Dict
import warnings
warnings.filterwarnings('ignore')

class ChannelAttention(nn.Module):
    """
    é€šé“æ³¨æ„åŠ›æ¨¡å— - è‡ªé€‚åº”åŠ æƒä¸åŒç‰¹å¾é€šé“
    åŸºäºSE-Net (Squeeze-and-Excitation Networks)
    """
    
    def __init__(self, in_channels: int, reduction_ratio: int = 16):
        """
        åˆå§‹åŒ–é€šé“æ³¨æ„åŠ›
        
        Args:
            in_channels: è¾“å…¥é€šé“æ•°
            reduction_ratio: é€šé“å‹ç¼©æ¯”ä¾‹
        """
        super(ChannelAttention, self).__init__()
        
        self.in_channels = in_channels
        self.reduction_ratio = reduction_ratio
        
        # å…¨å±€å¹³å‡æ± åŒ–
        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)
        
        # é€šé“å‹ç¼©å’Œæ¢å¤
        self.fc = nn.Sequential(
            nn.Linear(in_channels, in_channels // reduction_ratio),
            nn.ReLU(inplace=True),
            nn.Linear(in_channels // reduction_ratio, in_channels),
            nn.Sigmoid()
        )
        
        print(f"ğŸ“Š é€šé“æ³¨æ„åŠ›åˆå§‹åŒ–: {in_channels}é€šé“ â†’ {in_channels//reduction_ratio} â†’ {in_channels}")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥ç‰¹å¾ [batch, channels, height, width]
            
        Returns:
            åŠ æƒåçš„ç‰¹å¾
        """
        batch, channels, height, width = x.size()
        
        # å…¨å±€ä¿¡æ¯æå– [batch, channels, 1, 1]
        global_info = self.global_avg_pool(x)
        
        # é€šé“é‡è¦æ€§è®¡ç®— [batch, channels]
        channel_weights = self.fc(global_info.view(batch, channels))
        
        # é‡å¡‘å¹¶åº”ç”¨æƒé‡
        channel_weights = channel_weights.view(batch, channels, 1, 1)
        
        return x * channel_weights

class SpatialAttention(nn.Module):
    """
    ç©ºé—´æ³¨æ„åŠ›æ¨¡å— - å¼ºè°ƒé‡è¦çš„ç©ºé—´ä½ç½®
    """
    
    def __init__(self, kernel_size: int = 7):
        """
        åˆå§‹åŒ–ç©ºé—´æ³¨æ„åŠ›
        
        Args:
            kernel_size: å·ç§¯æ ¸å¤§å°
        """
        super(SpatialAttention, self).__init__()
        
        self.kernel_size = kernel_size
        
        # ç©ºé—´æ³¨æ„åŠ›å·ç§¯
        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, 
                             padding=kernel_size//2, bias=False)
        self.sigmoid = nn.Sigmoid()
        
        print(f"ğŸ—ºï¸  ç©ºé—´æ³¨æ„åŠ›åˆå§‹åŒ–: å·ç§¯æ ¸å¤§å° {kernel_size}Ã—{kernel_size}")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥ç‰¹å¾ [batch, channels, height, width]
            
        Returns:
            åŠ æƒåçš„ç‰¹å¾
        """
        # é€šé“ç»´åº¦å‹ç¼©ï¼šæœ€å¤§å€¼å’Œå¹³å‡å€¼
        max_pool = torch.max(x, dim=1, keepdim=True)[0]  # [batch, 1, height, width]
        avg_pool = torch.mean(x, dim=1, keepdim=True)    # [batch, 1, height, width]
        
        # æ‹¼æ¥ä¸¤ä¸ªé€šé“
        spatial_input = torch.cat([max_pool, avg_pool], dim=1)  # [batch, 2, height, width]
        
        # è®¡ç®—ç©ºé—´æ³¨æ„åŠ›æƒé‡
        spatial_weights = self.sigmoid(self.conv(spatial_input))  # [batch, 1, height, width]
        
        return x * spatial_weights

class MultiScaleConvBlock(nn.Module):
    """
    å¤šå°ºåº¦å·ç§¯å— - å¹¶è¡Œæå–ä¸åŒå°ºåº¦çš„ç©ºé—´ç‰¹å¾
    ç±»ä¼¼Inceptionæ¨¡å—çš„è®¾è®¡
    """
    
    def __init__(self, in_channels: int, out_channels: int):
        """
        åˆå§‹åŒ–å¤šå°ºåº¦å·ç§¯å—
        
        Args:
            in_channels: è¾“å…¥é€šé“æ•°
            out_channels: è¾“å‡ºé€šé“æ•°
        """
        super(MultiScaleConvBlock, self).__init__()
        
        self.in_channels = in_channels
        self.out_channels = out_channels
        
        # åˆ†é…è¾“å‡ºé€šé“
        branch_channels = out_channels // 4
        
        # åˆ†æ”¯1: 1Ã—1å·ç§¯ (ç‚¹ç‰¹å¾)
        self.branch1 = nn.Sequential(
            nn.Conv2d(in_channels, branch_channels, kernel_size=1, padding=0),
            nn.BatchNorm2d(branch_channels),
            nn.ReLU(inplace=True)
        )
        
        # åˆ†æ”¯2: 3Ã—3å·ç§¯ (å±€éƒ¨ç‰¹å¾)
        self.branch2 = nn.Sequential(
            nn.Conv2d(in_channels, branch_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(branch_channels),
            nn.ReLU(inplace=True)
        )
        
        # åˆ†æ”¯3: 5Ã—5å·ç§¯ (åŒºåŸŸç‰¹å¾)
        self.branch3 = nn.Sequential(
            nn.Conv2d(in_channels, branch_channels, kernel_size=5, padding=2),
            nn.BatchNorm2d(branch_channels),
            nn.ReLU(inplace=True)
        )
        
        # åˆ†æ”¯4: æœ€å¤§æ± åŒ– + 1Ã—1å·ç§¯
        self.branch4 = nn.Sequential(
            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),
            nn.Conv2d(in_channels, branch_channels, kernel_size=1, padding=0),
            nn.BatchNorm2d(branch_channels),
            nn.ReLU(inplace=True)
        )
        
        print(f"ğŸ”€ å¤šå°ºåº¦å·ç§¯å—: {in_channels} â†’ {out_channels} (4åˆ†æ”¯Ã—{branch_channels})")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥ç‰¹å¾ [batch, in_channels, height, width]
            
        Returns:
            å¤šå°ºåº¦ç‰¹å¾ [batch, out_channels, height, width]
        """
        # å¹¶è¡Œè®¡ç®—å››ä¸ªåˆ†æ”¯
        branch1_out = self.branch1(x)
        branch2_out = self.branch2(x)
        branch3_out = self.branch3(x)
        branch4_out = self.branch4(x)
        
        # æ‹¼æ¥æ‰€æœ‰åˆ†æ”¯
        output = torch.cat([branch1_out, branch2_out, branch3_out, branch4_out], dim=1)
        
        return output

class SpatialPyramidPooling(nn.Module):
    """
    ç©ºé—´é‡‘å­—å¡”æ± åŒ– - æ•æ‰å¤šç§ç©ºé—´å°ºåº¦çš„ä¿¡æ¯
    é€‚åº”ä¸åŒå¤§å°çš„åœ°éœ‡å½±å“åŒºåŸŸ
    """
    
    def __init__(self, in_channels: int, pool_sizes: List[int] = [1, 2, 3, 6]):
        """
        åˆå§‹åŒ–ç©ºé—´é‡‘å­—å¡”æ± åŒ–
        
        Args:
            in_channels: è¾“å…¥é€šé“æ•°
            pool_sizes: æ± åŒ–å°ºå¯¸åˆ—è¡¨
        """
        super(SpatialPyramidPooling, self).__init__()
        
        self.pool_sizes = pool_sizes
        self.pooling_layers = nn.ModuleList()
        
        for pool_size in pool_sizes:
            self.pooling_layers.append(
                nn.Sequential(
                    nn.AdaptiveAvgPool2d(pool_size),
                    nn.Conv2d(in_channels, in_channels // len(pool_sizes), 
                             kernel_size=1, bias=False),
                    nn.BatchNorm2d(in_channels // len(pool_sizes)),
                    nn.ReLU(inplace=True)
                )
            )
        
        # è¾“å‡ºé€šé“æ•°
        self.out_channels = in_channels
        
        print(f"ğŸ”ï¸  ç©ºé—´é‡‘å­—å¡”æ± åŒ–: æ± åŒ–å°ºå¯¸ {pool_sizes}")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥ç‰¹å¾ [batch, channels, height, width]
            
        Returns:
            é‡‘å­—å¡”æ± åŒ–ç‰¹å¾
        """
        batch, channels, height, width = x.size()
        pyramid_features = []
        
        for pooling_layer in self.pooling_layers:
            # æ± åŒ–åˆ°ä¸åŒå°ºå¯¸
            pooled = pooling_layer(x)
            
            # ä¸Šé‡‡æ ·å›åŸå§‹å°ºå¯¸
            upsampled = F.interpolate(pooled, size=(height, width), 
                                    mode='bilinear', align_corners=False)
            pyramid_features.append(upsampled)
        
        # æ‹¼æ¥æ‰€æœ‰é‡‘å­—å¡”ç‰¹å¾
        output = torch.cat(pyramid_features, dim=1)
        
        return output

class ResidualBlock(nn.Module):
    """
    æ®‹å·®å— - è§£å†³æ·±åº¦ç½‘ç»œæ¢¯åº¦æ¶ˆå¤±é—®é¢˜
    """
    
    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):
        """
        åˆå§‹åŒ–æ®‹å·®å—
        
        Args:
            in_channels: è¾“å…¥é€šé“æ•°
            out_channels: è¾“å‡ºé€šé“æ•°
            stride: å·ç§¯æ­¥é•¿
        """
        super(ResidualBlock, self).__init__()
        
        # ä¸»è·¯å¾„
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, 
                              stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, 
                              stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        # è·³è·ƒè¿æ¥
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, 
                         stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
        else:
            self.shortcut = nn.Identity()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        identity = self.shortcut(x)
        
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        
        out = self.conv2(out)
        out = self.bn2(out)
        
        out += identity
        out = self.relu(out)
        
        return out

class MultiScaleCNN(nn.Module):
    """
    å¤šå°ºåº¦CNNä¸»æ¨¡å— - æ•´åˆæ‰€æœ‰ç©ºé—´ç‰¹å¾æå–ç»„ä»¶
    """
    
    def __init__(self, 
                 in_channels: int = 8,
                 base_channels: int = 64,
                 num_scales: int = 3):
        """
        åˆå§‹åŒ–å¤šå°ºåº¦CNN
        
        Args:
            in_channels: è¾“å…¥é€šé“æ•° (8ä¸ªç‰¹å¾é€šé“)
            base_channels: åŸºç¡€é€šé“æ•°
            num_scales: å°ºåº¦æ•°é‡
        """
        super(MultiScaleCNN, self).__init__()
        
        self.in_channels = in_channels
        self.base_channels = base_channels
        self.num_scales = num_scales
        
        # åˆå§‹å·ç§¯å±‚
        self.initial_conv = nn.Sequential(
            nn.Conv2d(in_channels, base_channels, kernel_size=7, padding=3, bias=False),
            nn.BatchNorm2d(base_channels),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=1, padding=1)  # ä¿æŒç©ºé—´å°ºå¯¸
        )
        
        # å¤šå°ºåº¦ç‰¹å¾æå–å±‚
        self.multi_scale_blocks = nn.ModuleList()
        current_channels = base_channels
        
        for scale in range(num_scales):
            output_channels = base_channels * (2 ** scale)
            
            # å¤šå°ºåº¦å·ç§¯å—
            multi_scale_block = MultiScaleConvBlock(current_channels, output_channels)
            self.multi_scale_blocks.append(multi_scale_block)
            
            # æ®‹å·®å—å¢å¼ºç‰¹å¾
            residual_block = ResidualBlock(output_channels, output_channels)
            self.multi_scale_blocks.append(residual_block)
            
            current_channels = output_channels
        
        # ç©ºé—´é‡‘å­—å¡”æ± åŒ–
        self.spatial_pyramid = SpatialPyramidPooling(current_channels)
        
        # é€šé“æ³¨æ„åŠ›
        self.channel_attention = ChannelAttention(current_channels)
        
        # ç©ºé—´æ³¨æ„åŠ›
        self.spatial_attention = SpatialAttention()
        
        # è¾“å‡ºç‰¹å¾ç»´åº¦
        self.out_channels = current_channels
        
        print(f"ğŸ§  å¤šå°ºåº¦CNNåˆå§‹åŒ–:")
        print(f"  è¾“å…¥é€šé“: {in_channels}")
        print(f"  åŸºç¡€é€šé“: {base_channels}")
        print(f"  å°ºåº¦æ•°: {num_scales}")
        print(f"  è¾“å‡ºé€šé“: {self.out_channels}")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥ç‰¹å¾ [batch, time_steps, height, width, channels]
            
        Returns:
            ç©ºé—´ç‰¹å¾ [batch, time_steps, height', width', out_channels]
        """
        batch_size, time_steps, height, width, channels = x.size()
        
        # é‡å¡‘ä¸º2Då·ç§¯æ ¼å¼ [batch*time_steps, channels, height, width]
        x = x.view(batch_size * time_steps, channels, height, width)
        
        # åˆå§‹ç‰¹å¾æå–
        x = self.initial_conv(x)
        
        # å¤šå°ºåº¦ç‰¹å¾æå–
        for block in self.multi_scale_blocks:
            x = block(x)
        
        # ç©ºé—´é‡‘å­—å¡”æ± åŒ–
        x = self.spatial_pyramid(x)
        
        # é€šé“æ³¨æ„åŠ›
        x = self.channel_attention(x)
        
        # ç©ºé—´æ³¨æ„åŠ›
        x = self.spatial_attention(x)
        
        # é‡å¡‘å›æ—¶é—´åºåˆ—æ ¼å¼
        _, out_channels, out_height, out_width = x.size()
        x = x.view(batch_size, time_steps, out_height, out_width, out_channels)
        
        return x
    
    def get_output_shape(self, input_shape: Tuple[int, ...]) -> Tuple[int, ...]:
        """
        è®¡ç®—è¾“å‡ºå½¢çŠ¶
        
        Args:
            input_shape: è¾“å…¥å½¢çŠ¶ (time_steps, height, width, channels)
            
        Returns:
            è¾“å‡ºå½¢çŠ¶
        """
        time_steps, height, width, channels = input_shape
        
        # å‡è®¾ç©ºé—´ç»´åº¦ä¸å˜ (ä½¿ç”¨paddingä¿æŒ)
        output_shape = (time_steps, height, width, self.out_channels)
        
        return output_shape

def test_spatial_modules():
    """æµ‹è¯•ç©ºé—´ç‰¹å¾æå–æ¨¡å—"""
    print("ğŸ§ª æµ‹è¯•ç©ºé—´ç‰¹å¾æå–æ¨¡å—...")
    
    # æ¨¡æ‹Ÿè¾“å…¥æ•°æ® [batch, time_steps, height, width, channels]
    batch_size = 2
    time_steps = 90
    height, width = 22, 24
    in_channels = 8
    
    input_tensor = torch.randn(batch_size, time_steps, height, width, in_channels)
    print(f"ğŸ“Š è¾“å…¥æ•°æ®å½¢çŠ¶: {input_tensor.shape}")
    
    # 1. æµ‹è¯•é€šé“æ³¨æ„åŠ›
    print("\n1. æµ‹è¯•é€šé“æ³¨æ„åŠ›:")
    test_input_2d = torch.randn(4, 64, 22, 24)  # 2Dè¾“å…¥ç”¨äºæµ‹è¯•
    channel_attn = ChannelAttention(64, reduction_ratio=16)
    channel_output = channel_attn(test_input_2d)
    print(f"  è¾“å…¥: {test_input_2d.shape} â†’ è¾“å‡º: {channel_output.shape}")
    
    # 2. æµ‹è¯•ç©ºé—´æ³¨æ„åŠ›
    print("\n2. æµ‹è¯•ç©ºé—´æ³¨æ„åŠ›:")
    spatial_attn = SpatialAttention(kernel_size=7)
    spatial_output = spatial_attn(test_input_2d)
    print(f"  è¾“å…¥: {test_input_2d.shape} â†’ è¾“å‡º: {spatial_output.shape}")
    
    # 3. æµ‹è¯•å¤šå°ºåº¦å·ç§¯å—
    print("\n3. æµ‹è¯•å¤šå°ºåº¦å·ç§¯å—:")
    multi_scale_block = MultiScaleConvBlock(in_channels=64, out_channels=128)
    ms_output = multi_scale_block(test_input_2d)
    print(f"  è¾“å…¥: {test_input_2d.shape} â†’ è¾“å‡º: {ms_output.shape}")
    
    # 4. æµ‹è¯•ç©ºé—´é‡‘å­—å¡”æ± åŒ–
    print("\n4. æµ‹è¯•ç©ºé—´é‡‘å­—å¡”æ± åŒ–:")
    spp = SpatialPyramidPooling(in_channels=64, pool_sizes=[1, 2, 3, 6])
    spp_output = spp(test_input_2d)
    print(f"  è¾“å…¥: {test_input_2d.shape} â†’ è¾“å‡º: {spp_output.shape}")
    
    # 5. æµ‹è¯•æ®‹å·®å—
    print("\n5. æµ‹è¯•æ®‹å·®å—:")
    residual_block = ResidualBlock(in_channels=64, out_channels=128, stride=1)
    res_output = residual_block(test_input_2d)
    print(f"  è¾“å…¥: {test_input_2d.shape} â†’ è¾“å‡º: {res_output.shape}")
    
    # 6. æµ‹è¯•å®Œæ•´å¤šå°ºåº¦CNN
    print("\n6. æµ‹è¯•å®Œæ•´å¤šå°ºåº¦CNN:")
    multi_scale_cnn = MultiScaleCNN(
        in_channels=in_channels,
        base_channels=64,
        num_scales=3
    )
    
    # è®¡ç®—æ¨¡å‹å‚æ•°
    total_params = sum(p.numel() for p in multi_scale_cnn.parameters())
    print(f"  æ¨¡å‹å‚æ•°æ•°: {total_params:,}")
    
    # å‰å‘ä¼ æ’­æµ‹è¯•
    with torch.no_grad():
        output = multi_scale_cnn(input_tensor)
        print(f"  è¾“å…¥: {input_tensor.shape}")
        print(f"  è¾“å‡º: {output.shape}")
    
    # 7. è¾“å‡ºå½¢çŠ¶é¢„æµ‹
    print("\n7. è¾“å‡ºå½¢çŠ¶é¢„æµ‹:")
    predicted_shape = multi_scale_cnn.get_output_shape((time_steps, height, width, in_channels))
    print(f"  é¢„æµ‹è¾“å‡ºå½¢çŠ¶: {predicted_shape}")
    print(f"  å®é™…è¾“å‡ºå½¢çŠ¶: {tuple(output.shape[1:])}")
    
    # 8. å†…å­˜ä½¿ç”¨ä¼°ç®—
    print("\n8. å†…å­˜ä½¿ç”¨ä¼°ç®—:")
    input_memory = input_tensor.numel() * 4 / (1024**2)  # MB
    output_memory = output.numel() * 4 / (1024**2)  # MB
    print(f"  è¾“å…¥å†…å­˜: {input_memory:.2f} MB")
    print(f"  è¾“å‡ºå†…å­˜: {output_memory:.2f} MB")
    
    print("\nâœ… ç©ºé—´ç‰¹å¾æå–æ¨¡å—æµ‹è¯•å®Œæˆ!")
    return multi_scale_cnn, output

if __name__ == "__main__":
    test_spatial_modules()