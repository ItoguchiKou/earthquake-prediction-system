"""
æ³¨æ„åŠ›æœºåˆ¶æ¨¡å—
å®ç°å¤šå¤´æ³¨æ„åŠ›ã€è·¨æ¨¡æ€æ³¨æ„åŠ›ã€ç©ºé—´-æ—¶é—´æ³¨æ„åŠ›ç­‰é«˜çº§æ³¨æ„åŠ›æœºåˆ¶
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
from typing import Tuple, List, Optional, Dict, Union
import warnings
warnings.filterwarnings('ignore')

class MultiHeadAttention(nn.Module):
    """
    å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ - Transformeré£æ ¼çš„æ³¨æ„åŠ›
    """
    
    def __init__(self, 
                 embed_dim: int,
                 num_heads: int = 8,
                 dropout: float = 0.1,
                 bias: bool = True):
        """
        åˆå§‹åŒ–å¤šå¤´æ³¨æ„åŠ›
        
        Args:
            embed_dim: åµŒå…¥ç»´åº¦
            num_heads: æ³¨æ„åŠ›å¤´æ•°
            dropout: Dropoutæ¯”ä¾‹
            bias: æ˜¯å¦ä½¿ç”¨åç½®
        """
        super(MultiHeadAttention, self).__init__()
        
        assert embed_dim % num_heads == 0, "embed_dimå¿…é¡»èƒ½è¢«num_headsæ•´é™¤"
        
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.scale = math.sqrt(self.head_dim)
        
        # Q, K, VæŠ•å½±
        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)
        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)
        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)
        
        # è¾“å‡ºæŠ•å½±
        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
        print(f"ğŸ§  å¤šå¤´æ³¨æ„åŠ›åˆå§‹åŒ–: {embed_dim}ç»´, {num_heads}å¤´, æ¯å¤´{self.head_dim}ç»´")
    
    def forward(self, 
                query: torch.Tensor,
                key: torch.Tensor,
                value: torch.Tensor,
                mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            query: æŸ¥è¯¢å¼ é‡ [batch, seq_len, embed_dim]
            key: é”®å¼ é‡ [batch, seq_len, embed_dim]
            value: å€¼å¼ é‡ [batch, seq_len, embed_dim]
            mask: æ³¨æ„åŠ›æ©ç  [batch, seq_len, seq_len] (å¯é€‰)
            
        Returns:
            (æ³¨æ„åŠ›è¾“å‡º, æ³¨æ„åŠ›æƒé‡)
        """
        batch_size, seq_len, embed_dim = query.size()
        
        # çº¿æ€§æŠ•å½±
        Q = self.q_proj(query)  # [batch, seq_len, embed_dim]
        K = self.k_proj(key)    # [batch, seq_len, embed_dim]
        V = self.v_proj(value)  # [batch, seq_len, embed_dim]
        
        # é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼
        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        # ç°åœ¨å½¢çŠ¶: [batch, num_heads, seq_len, head_dim]
        
        # ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale
        # å½¢çŠ¶: [batch, num_heads, seq_len, seq_len]
        
        # åº”ç”¨æ©ç 
        if mask is not None:
            mask = mask.unsqueeze(1).expand(-1, self.num_heads, -1, -1)
            scores.masked_fill_(mask == 0, -1e9)
        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        attention_output = torch.matmul(attention_weights, V)
        # å½¢çŠ¶: [batch, num_heads, seq_len, head_dim]
        
        # é‡å¡‘å›åŸå§‹æ ¼å¼
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, embed_dim
        )
        
        # è¾“å‡ºæŠ•å½±
        output = self.out_proj(attention_output)
        
        # è¿”å›å¹³å‡æ³¨æ„åŠ›æƒé‡ (æ‰€æœ‰å¤´çš„å¹³å‡)
        avg_attention_weights = attention_weights.mean(dim=1)
        
        return output, avg_attention_weights

class SpatioTemporalAttention(nn.Module):
    """
    æ—¶ç©ºæ³¨æ„åŠ› - åŒæ—¶å»ºæ¨¡ç©ºé—´å’Œæ—¶é—´ç»´åº¦çš„æ³¨æ„åŠ›
    """
    
    def __init__(self,
                 spatial_dim: int,
                 temporal_dim: int,
                 hidden_dim: int = 128):
        """
        åˆå§‹åŒ–æ—¶ç©ºæ³¨æ„åŠ›
        
        Args:
            spatial_dim: ç©ºé—´ç‰¹å¾ç»´åº¦
            temporal_dim: æ—¶é—´ç‰¹å¾ç»´åº¦  
            hidden_dim: éšè—å±‚ç»´åº¦
        """
        super(SpatioTemporalAttention, self).__init__()
        
        self.spatial_dim = spatial_dim
        self.temporal_dim = temporal_dim
        self.hidden_dim = hidden_dim
        
        # ç©ºé—´æ³¨æ„åŠ›åˆ†æ”¯
        self.spatial_attention = nn.Sequential(
            nn.Conv2d(spatial_dim, hidden_dim, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(hidden_dim, 1, kernel_size=1),
            nn.Sigmoid()
        )
        
        # æ—¶é—´æ³¨æ„åŠ›åˆ†æ”¯
        self.temporal_attention = nn.Sequential(
            nn.Linear(temporal_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )
        
        # äº¤äº’æ³¨æ„åŠ› (ç©ºé—´-æ—¶é—´)
        self.interaction_attention = nn.Sequential(
            nn.Linear(spatial_dim + temporal_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Sigmoid()
        )
        
        print(f"ğŸŒ æ—¶ç©ºæ³¨æ„åŠ›åˆå§‹åŒ–: ç©ºé—´{spatial_dim}ç»´ + æ—¶é—´{temporal_dim}ç»´ â†’ {hidden_dim}ç»´")
    
    def forward(self, 
                spatial_features: torch.Tensor,
                temporal_features: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            spatial_features: ç©ºé—´ç‰¹å¾ [batch, time_steps, channels, height, width]
            temporal_features: æ—¶é—´ç‰¹å¾ [batch, time_steps, channels, height, width]
            
        Returns:
            (èåˆç‰¹å¾, æ³¨æ„åŠ›æƒé‡å­—å…¸)
        """
        batch_size, time_steps, channels, height, width = spatial_features.size()
        
        # 1. ç©ºé—´æ³¨æ„åŠ›è®¡ç®—
        spatial_weights = []
        for t in range(time_steps):
            # è®¡ç®—è¯¥æ—¶é—´æ­¥çš„ç©ºé—´æ³¨æ„åŠ›
            spatial_weight = self.spatial_attention(spatial_features[:, t, :, :, :])
            spatial_weights.append(spatial_weight)
        
        spatial_attention_weights = torch.stack(spatial_weights, dim=1)
        # å½¢çŠ¶: [batch, time_steps, 1, height, width]
        
        # 2. æ—¶é—´æ³¨æ„åŠ›è®¡ç®—
        # å°†ç©ºé—´ç»´åº¦å‹ç¼©ä¸ºç‰¹å¾å‘é‡
        spatial_pooled = F.adaptive_avg_pool2d(
            spatial_features.view(batch_size * time_steps, channels, height, width),
            (1, 1)
        ).view(batch_size, time_steps, channels)
        
        temporal_pooled = F.adaptive_avg_pool2d(
            temporal_features.view(batch_size * time_steps, channels, height, width),
            (1, 1)
        ).view(batch_size, time_steps, channels)
        
        # è®¡ç®—æ—¶é—´æ³¨æ„åŠ›æƒé‡
        temporal_attention_weights = self.temporal_attention(temporal_pooled)
        # å½¢çŠ¶: [batch, time_steps, 1]
        
        # 3. äº¤äº’æ³¨æ„åŠ›è®¡ç®—
        combined_features = torch.cat([spatial_pooled, temporal_pooled], dim=-1)
        interaction_weights = self.interaction_attention(combined_features)
        # å½¢çŠ¶: [batch, time_steps, hidden_dim]
        
        # 4. åº”ç”¨æ³¨æ„åŠ›æƒé‡
        # ç©ºé—´æ³¨æ„åŠ› - ç›´æ¥å¹¿æ’­ï¼Œå› ä¸ºç»´åº¦å·²ç»åŒ¹é…
        spatial_attended = spatial_features * spatial_attention_weights
        
        # æ—¶é—´æ³¨æ„åŠ› - éœ€è¦æ­£ç¡®æ‰©å±•ç»´åº¦åˆ° [batch, time_steps, 1, 1, 1]
        temporal_weight_expanded = temporal_attention_weights.unsqueeze(-1).unsqueeze(-1)
        # ç°åœ¨å½¢çŠ¶: [batch, time_steps, 1, 1, 1]
        temporal_attended = temporal_features * temporal_weight_expanded
        
        # ç‰¹å¾èåˆ
        fused_features = spatial_attended + temporal_attended
        
        # äº¤äº’æƒé‡åº”ç”¨ - å¹³å‡åæ‰©å±•ç»´åº¦
        interaction_weight_avg = interaction_weights.mean(dim=-1, keepdim=True)  # [batch, time_steps, 1]
        interaction_weight_expanded = interaction_weight_avg.unsqueeze(-1).unsqueeze(-1)
        # ç°åœ¨å½¢çŠ¶: [batch, time_steps, 1, 1, 1]
        fused_features = fused_features * interaction_weight_expanded
        
        attention_weights = {
            'spatial': spatial_attention_weights.squeeze(2),  # [batch, time_steps, height, width]
            'temporal': temporal_attention_weights.squeeze(-1),  # [batch, time_steps]
            'interaction': interaction_weights  # [batch, time_steps, hidden_dim]
        }
        
        return fused_features, attention_weights

class CrossModalAttention(nn.Module):
    """
    è·¨æ¨¡æ€æ³¨æ„åŠ› - ä¸åŒç‰¹å¾æ¨¡æ€ä¹‹é—´çš„æ³¨æ„åŠ›äº¤äº’
    """
    
    def __init__(self,
                 modality1_dim: int,
                 modality2_dim: int,
                 hidden_dim: int = 256,
                 num_heads: int = 8):
        """
        åˆå§‹åŒ–è·¨æ¨¡æ€æ³¨æ„åŠ›
        
        Args:
            modality1_dim: æ¨¡æ€1ç‰¹å¾ç»´åº¦
            modality2_dim: æ¨¡æ€2ç‰¹å¾ç»´åº¦
            hidden_dim: éšè—å±‚ç»´åº¦
            num_heads: æ³¨æ„åŠ›å¤´æ•°
        """
        super(CrossModalAttention, self).__init__()
        
        self.modality1_dim = modality1_dim
        self.modality2_dim = modality2_dim
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        
        # æ¨¡æ€1 â†’ æ¨¡æ€2 æ³¨æ„åŠ›
        self.m1_to_m2_attention = MultiHeadAttention(
            embed_dim=hidden_dim,
            num_heads=num_heads
        )
        
        # æ¨¡æ€2 â†’ æ¨¡æ€1 æ³¨æ„åŠ›
        self.m2_to_m1_attention = MultiHeadAttention(
            embed_dim=hidden_dim,
            num_heads=num_heads
        )
        
        # æŠ•å½±å±‚
        self.m1_proj = nn.Linear(modality1_dim, hidden_dim)
        self.m2_proj = nn.Linear(modality2_dim, hidden_dim)
        
        # è¾“å‡ºæŠ•å½±
        self.m1_out_proj = nn.Linear(hidden_dim, modality1_dim)
        self.m2_out_proj = nn.Linear(hidden_dim, modality2_dim)
        
        print(f"ğŸ”„ è·¨æ¨¡æ€æ³¨æ„åŠ›åˆå§‹åŒ–: {modality1_dim}ç»´ â†” {modality2_dim}ç»´")
    
    def forward(self,
                modality1_features: torch.Tensor,
                modality2_features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, Dict[str, torch.Tensor]]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            modality1_features: æ¨¡æ€1ç‰¹å¾ [batch, seq_len, modality1_dim]
            modality2_features: æ¨¡æ€2ç‰¹å¾ [batch, seq_len, modality2_dim]
            
        Returns:
            (å¢å¼ºçš„æ¨¡æ€1ç‰¹å¾, å¢å¼ºçš„æ¨¡æ€2ç‰¹å¾, æ³¨æ„åŠ›æƒé‡)
        """
        # æŠ•å½±åˆ°ç»Ÿä¸€ç»´åº¦
        m1_projected = self.m1_proj(modality1_features)
        m2_projected = self.m2_proj(modality2_features)
        
        # æ¨¡æ€1 â†’ æ¨¡æ€2 æ³¨æ„åŠ›
        m1_enhanced, m1_to_m2_weights = self.m1_to_m2_attention(
            query=m1_projected,
            key=m2_projected,
            value=m2_projected
        )
        
        # æ¨¡æ€2 â†’ æ¨¡æ€1 æ³¨æ„åŠ›
        m2_enhanced, m2_to_m1_weights = self.m2_to_m1_attention(
            query=m2_projected,
            key=m1_projected,
            value=m1_projected
        )
        
        # æ®‹å·®è¿æ¥
        m1_enhanced = m1_enhanced + m1_projected
        m2_enhanced = m2_enhanced + m2_projected
        
        # æŠ•å½±å›åŸå§‹ç»´åº¦
        m1_output = self.m1_out_proj(m1_enhanced)
        m2_output = self.m2_out_proj(m2_enhanced)
        
        # æ®‹å·®è¿æ¥åˆ°åŸå§‹ç‰¹å¾
        m1_output = m1_output + modality1_features
        m2_output = m2_output + modality2_features
        
        attention_weights = {
            'm1_to_m2': m1_to_m2_weights,
            'm2_to_m1': m2_to_m1_weights
        }
        
        return m1_output, m2_output, attention_weights

class GlobalLocalAttention(nn.Module):
    """
    å…¨å±€-å±€éƒ¨æ³¨æ„åŠ› - ç»“åˆå…¨å±€ä¸Šä¸‹æ–‡å’Œå±€éƒ¨ç»†èŠ‚
    """
    
    def __init__(self,
                 feature_dim: int,
                 num_scales: int = 3,
                 hidden_dim: int = 128):
        """
        åˆå§‹åŒ–å…¨å±€-å±€éƒ¨æ³¨æ„åŠ›
        
        Args:
            feature_dim: ç‰¹å¾ç»´åº¦
            num_scales: å°ºåº¦æ•°é‡
            hidden_dim: éšè—å±‚ç»´åº¦
        """
        super(GlobalLocalAttention, self).__init__()
        
        self.feature_dim = feature_dim
        self.num_scales = num_scales
        self.hidden_dim = hidden_dim
        
        # å…¨å±€æ³¨æ„åŠ› (å…¨å±€å¹³å‡æ± åŒ–)
        self.global_attention = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(feature_dim, hidden_dim, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(hidden_dim, feature_dim, kernel_size=1),
            nn.Sigmoid()
        )
        
        # å¤šå°ºåº¦å±€éƒ¨æ³¨æ„åŠ›
        self.local_attentions = nn.ModuleList()
        for scale in range(num_scales):
            kernel_size = 2 ** (scale + 1) + 1  # 3, 5, 9
            padding = kernel_size // 2
            
            local_attn = nn.Sequential(
                nn.Conv2d(feature_dim, hidden_dim, kernel_size=kernel_size, padding=padding),
                nn.ReLU(inplace=True),
                nn.Conv2d(hidden_dim, 1, kernel_size=1),
                nn.Sigmoid()
            )
            self.local_attentions.append(local_attn)
        
        # å°ºåº¦èåˆ
        self.scale_fusion = nn.Conv2d(num_scales, 1, kernel_size=1)
        
        print(f"ğŸŒ å…¨å±€-å±€éƒ¨æ³¨æ„åŠ›åˆå§‹åŒ–: {feature_dim}ç»´, {num_scales}ä¸ªå°ºåº¦")
    
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥ç‰¹å¾ [batch, channels, height, width]
            
        Returns:
            (æ³¨æ„åŠ›å¢å¼ºç‰¹å¾, æ³¨æ„åŠ›æƒé‡)
        """
        # å…¨å±€æ³¨æ„åŠ›
        global_weights = self.global_attention(x)
        
        # å¤šå°ºåº¦å±€éƒ¨æ³¨æ„åŠ›
        local_weights_list = []
        for local_attention in self.local_attentions:
            local_weight = local_attention(x)
            local_weights_list.append(local_weight)
        
        # èåˆå¤šå°ºåº¦å±€éƒ¨æ³¨æ„åŠ›
        local_weights_stacked = torch.cat(local_weights_list, dim=1)
        fused_local_weights = self.scale_fusion(local_weights_stacked)
        
        # ç»„åˆå…¨å±€å’Œå±€éƒ¨æ³¨æ„åŠ›
        combined_weights = global_weights * fused_local_weights
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        attended_features = x * combined_weights
        
        attention_weights = {
            'global': global_weights,
            'local_scales': local_weights_list,
            'fused_local': fused_local_weights,
            'combined': combined_weights
        }
        
        return attended_features, attention_weights

class EarthquakeAttentionFusion(nn.Module):
    """
    åœ°éœ‡é¢„æµ‹ä¸“ç”¨æ³¨æ„åŠ›èåˆæ¨¡å—
    æ•´åˆå¤šç§æ³¨æ„åŠ›æœºåˆ¶ç”¨äºåœ°éœ‡ç‰¹å¾å¢å¼º
    """
    
    def __init__(self,
                 spatial_channels: int = 256,
                 temporal_channels: int = 256,
                 fusion_dim: int = 512,
                 num_heads: int = 8):
        """
        åˆå§‹åŒ–åœ°éœ‡æ³¨æ„åŠ›èåˆ
        
        Args:
            spatial_channels: ç©ºé—´ç‰¹å¾é€šé“æ•°
            temporal_channels: æ—¶é—´ç‰¹å¾é€šé“æ•°
            fusion_dim: èåˆåç‰¹å¾ç»´åº¦
            num_heads: å¤šå¤´æ³¨æ„åŠ›å¤´æ•°
        """
        super(EarthquakeAttentionFusion, self).__init__()
        
        self.spatial_channels = spatial_channels
        self.temporal_channels = temporal_channels
        self.fusion_dim = fusion_dim
        
        # æ—¶ç©ºæ³¨æ„åŠ›
        self.spatiotemporal_attention = SpatioTemporalAttention(
            spatial_dim=spatial_channels,
            temporal_dim=temporal_channels,
            hidden_dim=128
        )
        
        # å…¨å±€-å±€éƒ¨æ³¨æ„åŠ›
        self.global_local_attention = GlobalLocalAttention(
            feature_dim=spatial_channels,
            num_scales=3,
            hidden_dim=64
        )
        
        # ç‰¹å¾èåˆæŠ•å½±
        self.fusion_proj = nn.Sequential(
            nn.Conv2d(spatial_channels + temporal_channels, fusion_dim, kernel_size=1),
            nn.BatchNorm2d(fusion_dim),
            nn.ReLU(inplace=True),
            nn.Dropout2d(0.1)
        )
        
        # è‡ªæ³¨æ„åŠ›å¢å¼º
        self.self_attention_2d = nn.Sequential(
            nn.Conv2d(fusion_dim, fusion_dim // 8, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(fusion_dim // 8, fusion_dim, kernel_size=1),
            nn.Sigmoid()
        )
        
        print(f"ğŸŒ åœ°éœ‡æ³¨æ„åŠ›èåˆåˆå§‹åŒ–:")
        print(f"  ç©ºé—´é€šé“: {spatial_channels}")
        print(f"  æ—¶é—´é€šé“: {temporal_channels}")
        print(f"  èåˆç»´åº¦: {fusion_dim}")
        print(f"  æ³¨æ„åŠ›å¤´æ•°: {num_heads}")
    
    def forward(self,
                spatial_features: torch.Tensor,
                temporal_features: torch.Tensor) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            spatial_features: ç©ºé—´ç‰¹å¾ [batch, time_steps, spatial_channels, height, width]
            temporal_features: æ—¶é—´ç‰¹å¾ [batch, time_steps, temporal_channels, height, width]
            
        Returns:
            (èåˆç‰¹å¾, æ‰€æœ‰æ³¨æ„åŠ›æƒé‡)
        """
        batch_size, time_steps, _, height, width = spatial_features.size()
        
        # 1. æ—¶ç©ºæ³¨æ„åŠ›å¢å¼º
        st_features, st_attention = self.spatiotemporal_attention(
            spatial_features, temporal_features
        )
        
        # 2. å…¨å±€-å±€éƒ¨æ³¨æ„åŠ› (å¯¹æ¯ä¸ªæ—¶é—´æ­¥åº”ç”¨)
        gl_features = []
        gl_attentions = []
        
        for t in range(time_steps):
            gl_feat, gl_attn = self.global_local_attention(st_features[:, t, :, :, :])
            gl_features.append(gl_feat)
            gl_attentions.append(gl_attn)
        
        gl_features = torch.stack(gl_features, dim=1)
        
        # 3. ç‰¹å¾èåˆ
        # æ‹¼æ¥ç©ºé—´å’Œæ—¶é—´ç‰¹å¾
        combined_features = torch.cat([spatial_features, temporal_features], dim=2)
        
        # é€å¸§èåˆæŠ•å½±
        fused_frames = []
        for t in range(time_steps):
            fused_frame = self.fusion_proj(combined_features[:, t, :, :, :])
            fused_frames.append(fused_frame)
        
        fused_features = torch.stack(fused_frames, dim=1)
        
        # 4. è‡ªæ³¨æ„åŠ›å¢å¼º
        self_attended_frames = []
        for t in range(time_steps):
            self_attn_weight = self.self_attention_2d(fused_features[:, t, :, :, :])
            self_attended_frame = fused_features[:, t, :, :, :] * self_attn_weight
            self_attended_frames.append(self_attended_frame)
        
        final_features = torch.stack(self_attended_frames, dim=1)
        
        # 5. æ®‹å·®è¿æ¥ (å¦‚æœç»´åº¦åŒ¹é…)
        if gl_features.size(2) == final_features.size(2):
            final_features = final_features + gl_features
        
        # æ”¶é›†æ‰€æœ‰æ³¨æ„åŠ›æƒé‡
        all_attention_weights = {
            'spatiotemporal': st_attention,
            'global_local': gl_attentions[0] if gl_attentions else None,  # åªè¿”å›ç¬¬ä¸€å¸§çš„ç¤ºä¾‹
            'self_attention': self_attended_frames  # è‡ªæ³¨æ„åŠ›æƒé‡åŒ…å«åœ¨ç‰¹å¾ä¸­
        }
        
        return final_features, all_attention_weights

def test_attention_modules():
    """æµ‹è¯•æ³¨æ„åŠ›æœºåˆ¶æ¨¡å—"""
    print("ğŸ§ª æµ‹è¯•æ³¨æ„åŠ›æœºåˆ¶æ¨¡å—...")
    
    # æ¨¡æ‹Ÿæ•°æ®
    batch_size = 2
    seq_len = 10
    height, width = 22, 24
    embed_dim = 256
    
    # 1. æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›
    print("\n1. æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›:")
    multi_head_attn = MultiHeadAttention(embed_dim=embed_dim, num_heads=8)
    
    # åˆ›å»ºåºåˆ—æ•°æ® [batch, seq_len, embed_dim]
    sequence_data = torch.randn(batch_size, seq_len, embed_dim)
    attn_output, attn_weights = multi_head_attn(sequence_data, sequence_data, sequence_data)
    
    print(f"  è¾“å…¥: {sequence_data.shape}")
    print(f"  è¾“å‡º: {attn_output.shape}")
    print(f"  æ³¨æ„åŠ›æƒé‡: {attn_weights.shape}")
    
    # 2. æµ‹è¯•æ—¶ç©ºæ³¨æ„åŠ›
    print("\n2. æµ‹è¯•æ—¶ç©ºæ³¨æ„åŠ›:")
    spatial_features = torch.randn(batch_size, seq_len, 128, height, width)
    temporal_features = torch.randn(batch_size, seq_len, 128, height, width)
    
    st_attention = SpatioTemporalAttention(spatial_dim=128, temporal_dim=128, hidden_dim=64)
    st_output, st_weights = st_attention(spatial_features, temporal_features)
    
    print(f"  ç©ºé—´ç‰¹å¾: {spatial_features.shape}")
    print(f"  æ—¶é—´ç‰¹å¾: {temporal_features.shape}")
    print(f"  èåˆè¾“å‡º: {st_output.shape}")
    print(f"  ç©ºé—´æ³¨æ„åŠ›: {st_weights['spatial'].shape}")
    print(f"  æ—¶é—´æ³¨æ„åŠ›: {st_weights['temporal'].shape}")
    
    # 3. æµ‹è¯•è·¨æ¨¡æ€æ³¨æ„åŠ›
    print("\n3. æµ‹è¯•è·¨æ¨¡æ€æ³¨æ„åŠ›:")
    modality1 = torch.randn(batch_size, seq_len, 256)
    modality2 = torch.randn(batch_size, seq_len, 128)
    
    cross_modal_attn = CrossModalAttention(modality1_dim=256, modality2_dim=128, hidden_dim=256)
    m1_enhanced, m2_enhanced, cm_weights = cross_modal_attn(modality1, modality2)
    
    print(f"  æ¨¡æ€1è¾“å…¥: {modality1.shape} â†’ è¾“å‡º: {m1_enhanced.shape}")
    print(f"  æ¨¡æ€2è¾“å…¥: {modality2.shape} â†’ è¾“å‡º: {m2_enhanced.shape}")
    
    # 4. æµ‹è¯•å…¨å±€-å±€éƒ¨æ³¨æ„åŠ›
    print("\n4. æµ‹è¯•å…¨å±€-å±€éƒ¨æ³¨æ„åŠ›:")
    feature_map = torch.randn(batch_size, 256, height, width)
    
    gl_attention = GlobalLocalAttention(feature_dim=256, num_scales=3)
    gl_output, gl_weights = gl_attention(feature_map)
    
    print(f"  è¾“å…¥ç‰¹å¾å›¾: {feature_map.shape}")
    print(f"  è¾“å‡ºç‰¹å¾å›¾: {gl_output.shape}")
    print(f"  å…¨å±€æ³¨æ„åŠ›: {gl_weights['global'].shape}")
    print(f"  å±€éƒ¨å°ºåº¦æ•°: {len(gl_weights['local_scales'])}")
    
    # 5. æµ‹è¯•å®Œæ•´åœ°éœ‡æ³¨æ„åŠ›èåˆ
    print("\n5. æµ‹è¯•åœ°éœ‡æ³¨æ„åŠ›èåˆ:")
    spatial_seq = torch.randn(batch_size, seq_len, 256, height, width)
    temporal_seq = torch.randn(batch_size, seq_len, 256, height, width)
    
    earthquake_fusion = EarthquakeAttentionFusion(
        spatial_channels=256,
        temporal_channels=256,
        fusion_dim=512,
        num_heads=8
    )
    
    # è®¡ç®—å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in earthquake_fusion.parameters())
    print(f"  æ¨¡å‹å‚æ•°æ•°: {total_params:,}")
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        fusion_output, all_weights = earthquake_fusion(spatial_seq, temporal_seq)
    
    print(f"  ç©ºé—´è¾“å…¥: {spatial_seq.shape}")
    print(f"  æ—¶é—´è¾“å…¥: {temporal_seq.shape}")
    print(f"  èåˆè¾“å‡º: {fusion_output.shape}")
    print(f"  æ³¨æ„åŠ›ç±»å‹æ•°: {len(all_weights)}")
    
    # 6. æ€§èƒ½åˆ†æ
    print("\n6. æ€§èƒ½åˆ†æ:")
    input_memory = (spatial_seq.numel() + temporal_seq.numel()) * 4 / (1024**2)
    output_memory = fusion_output.numel() * 4 / (1024**2)
    param_memory = total_params * 4 / (1024**2)
    
    print(f"  è¾“å…¥å†…å­˜: {input_memory:.2f} MB")
    print(f"  è¾“å‡ºå†…å­˜: {output_memory:.2f} MB")
    print(f"  å‚æ•°å†…å­˜: {param_memory:.2f} MB")
    print(f"  æ€»å†…å­˜ä¼°ç®—: {input_memory + output_memory + param_memory:.2f} MB")
    
    print("\nâœ… æ³¨æ„åŠ›æœºåˆ¶æ¨¡å—æµ‹è¯•å®Œæˆ!")
    return earthquake_fusion, fusion_output, all_weights

if __name__ == "__main__":
    test_attention_modules()