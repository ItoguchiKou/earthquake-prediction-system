"""
æŸå¤±å‡½æ•°å®šä¹‰æ¨¡å— - å›å½’ç‰ˆæœ¬
é’ˆå¯¹åœ°éœ‡æ¦‚ç‡é¢„æµ‹çš„å›å½’ä»»åŠ¡ä¼˜åŒ–
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional, Union, Any
import warnings
warnings.filterwarnings('ignore')

class HuberLoss(nn.Module):
    """Huber Loss - å¯¹å¼‚å¸¸å€¼æ›´é²æ£’çš„å›å½’æŸå¤±"""
    
    def __init__(self, delta: float = 1.0, reduction: str = 'mean'):
        super().__init__()
        self.delta = delta
        self.reduction = reduction
        
    def forward(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        diff = torch.abs(predictions - targets)
        
        # Huber loss: å°è¯¯å·®ç”¨L2ï¼Œå¤§è¯¯å·®ç”¨L1
        mask = diff < self.delta
        loss = torch.where(
            mask,
            0.5 * diff ** 2,
            self.delta * (diff - 0.5 * self.delta)
        )
        
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:
            return loss

class WeightedMSELoss(nn.Module):
    """åŠ æƒMSEæŸå¤± - å¯¹ä¸åŒæ¦‚ç‡èŒƒå›´ç»™äºˆä¸åŒæƒé‡"""
    
    def __init__(self, 
                 probability_weights: Optional[torch.Tensor] = None,
                 task_weights: Optional[torch.Tensor] = None,
                 reduction: str = 'mean'):
        super().__init__()
        self.probability_weights = probability_weights
        self.task_weights = task_weights
        self.reduction = reduction
        
    def forward(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        # åŸºæœ¬MSE
        mse = (predictions - targets) ** 2
        
        # æ ¹æ®ç›®æ ‡æ¦‚ç‡å€¼åŠ æƒï¼ˆé«˜æ¦‚ç‡äº‹ä»¶æ›´é‡è¦ï¼‰
        if self.probability_weights is not None:
            # æ ¹æ®ç›®æ ‡å€¼è®¡ç®—æƒé‡
            weights = 1.0 + targets * self.probability_weights
            mse = mse * weights
        
        # ä»»åŠ¡çº§åˆ«åŠ æƒ
        if self.task_weights is not None:
            mse = mse * self.task_weights.unsqueeze(0)
        
        if self.reduction == 'mean':
            return mse.mean()
        elif self.reduction == 'sum':
            return mse.sum()
        else:
            return mse

class ProbabilityRegressionLoss(nn.Module):
    """ä¸“é—¨ä¸ºæ¦‚ç‡å›å½’è®¾è®¡çš„æŸå¤±å‡½æ•°"""
    
    def __init__(self,
                 base_loss: str = 'huber',  # 'mse', 'huber', 'mae'
                 huber_delta: float = 0.1,
                 probability_threshold: float = 0.3,  # é‡è¦äº‹ä»¶é˜ˆå€¼
                 high_prob_weight: float = 5.0,  # é«˜æ¦‚ç‡äº‹ä»¶æƒé‡
                 consistency_weight: float = 0.1):
        super().__init__()
        
        self.base_loss = base_loss
        self.huber_delta = huber_delta
        self.probability_threshold = probability_threshold
        self.high_prob_weight = high_prob_weight
        self.consistency_weight = consistency_weight
        
        # åŸºç¡€æŸå¤±å‡½æ•°
        if base_loss == 'huber':
            self.loss_fn = HuberLoss(delta=huber_delta, reduction='none')
        elif base_loss == 'mae':
            self.loss_fn = nn.L1Loss(reduction='none')
        else:  # mse
            self.loss_fn = nn.MSELoss(reduction='none')
    
    def forward(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        # åŸºç¡€æŸå¤±
        base_loss = self.loss_fn(predictions, targets)
        
        # å¯¹é«˜æ¦‚ç‡äº‹ä»¶åŠ æƒ
        high_prob_mask = targets > self.probability_threshold
        weights = torch.ones_like(targets)
        weights[high_prob_mask] = self.high_prob_weight
        
        weighted_loss = base_loss * weights
        
        # æ—¶é—´ä¸€è‡´æ€§æŸå¤±ï¼ˆå¯é€‰ï¼‰
        if self.consistency_weight > 0:
            consistency_loss = self._compute_consistency_loss(predictions)
            total_loss = weighted_loss.mean() + self.consistency_weight * consistency_loss
        else:
            total_loss = weighted_loss.mean()
        
        return total_loss
    
    def _compute_consistency_loss(self, predictions: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—æ—¶é—´ä¸€è‡´æ€§æŸå¤±"""
        batch_size = predictions.size(0)
        pred_reshaped = predictions.view(batch_size, 3, 4)  # 3ä¸ªæ—¶é—´çª—å£ï¼Œ4ä¸ªéœ‡çº§
        
        consistency_loss = 0.0
        
        # æ—¶é—´ä¸€è‡´æ€§ï¼šåç»­æ—¶é—´çª—å£çš„æ¦‚ç‡åº”è¯¥ >= å‰é¢çš„
        for t in range(2):
            # ä½¿ç”¨è½¯çº¦æŸè€Œä¸æ˜¯ç¡¬çº¦æŸ
            diff = F.relu(pred_reshaped[:, t, :] - pred_reshaped[:, t+1, :])
            consistency_loss = consistency_loss + diff.mean()
        
        # éœ‡çº§ä¸€è‡´æ€§ï¼šå¤§éœ‡çº§çš„æ¦‚ç‡åº”è¯¥ <= å°éœ‡çº§
        for t in range(3):
            for m in range(3):
                diff = F.relu(pred_reshaped[:, t, m+1] - pred_reshaped[:, t, m])
                consistency_loss = consistency_loss + diff.mean() * 0.5
        
        return consistency_loss

class EarthquakeRegressionLoss(nn.Module):
    """åœ°éœ‡é¢„æµ‹å›å½’ç»¼åˆæŸå¤±å‡½æ•°"""
    
    def __init__(self,
                 loss_type: str = 'huber',
                 task_weights: Optional[List[float]] = None,
                 consistency_weight: float = 0.1,
                 ignore_tasks: Optional[List[int]] = None):
        """
        åˆå§‹åŒ–å›å½’æŸå¤±å‡½æ•°
        
        Args:
            loss_type: æŸå¤±ç±»å‹ ('mse', 'huber', 'mae', 'weighted_mse')
            task_weights: ä»»åŠ¡æƒé‡
            consistency_weight: ä¸€è‡´æ€§æŸå¤±æƒé‡
            ignore_tasks: è¦å¿½ç•¥çš„ä»»åŠ¡åˆ—è¡¨
        """
        super().__init__()
        
        self.loss_type = loss_type
        self.consistency_weight = consistency_weight
        self.ignore_tasks = ignore_tasks or []
        
        # åŸºäºä»»åŠ¡ç»Ÿè®¡è®¾ç½®é»˜è®¤æƒé‡
        if task_weights is None:
            # åŸºäºå¹³å‡æ¦‚ç‡å€¼çš„åæ¯”ä¾‹æƒé‡
            task_mean_probs = torch.tensor([
                0.0796, 0.2516, 0.1322, 0.0274,  # 7å¤©
                0.0996, 0.3163, 0.2228, 0.0529,  # 14å¤©
                0.1275, 0.4163, 0.3453, 0.1047   # 30å¤©
            ])
            # åæ¯”ä¾‹æƒé‡ï¼Œå¹¶è¿›è¡Œå½’ä¸€åŒ–
            self.task_weights = 1.0 / (task_mean_probs + 0.01)
            self.task_weights = self.task_weights / self.task_weights.mean()
        else:
            self.task_weights = torch.tensor(task_weights)
        
        # å°†å¿½ç•¥ä»»åŠ¡çš„æƒé‡è®¾ä¸º0
        for idx in self.ignore_tasks:
            self.task_weights[idx] = 0.0
        
        # åˆ›å»ºä¸»æŸå¤±å‡½æ•°
        if loss_type == 'huber':
            self.main_loss = ProbabilityRegressionLoss(
                base_loss='huber',
                huber_delta=0.1,
                consistency_weight=0
            )
        elif loss_type == 'weighted_mse':
            self.main_loss = WeightedMSELoss(
                probability_weights=torch.tensor(5.0),
                task_weights=self.task_weights
            )
        elif loss_type == 'mae':
            self.main_loss = nn.L1Loss(reduction='none')
        else:  # mse
            self.main_loss = nn.MSELoss(reduction='none')
        
        # ä¸€è‡´æ€§æŸå¤±
        if consistency_weight > 0:
            self.consistency_loss = ConsistencyLoss(weight=consistency_weight)
        
        print(f"ğŸ“Š åœ°éœ‡å›å½’æŸå¤±åˆå§‹åŒ–:")
        print(f"  æŸå¤±ç±»å‹: {loss_type}")
        print(f"  ä¸€è‡´æ€§æƒé‡: {consistency_weight}")
        print(f"  å¿½ç•¥ä»»åŠ¡: {self.ignore_tasks}")
        print(f"  ä»»åŠ¡æƒé‡èŒƒå›´: [{self.task_weights.min():.2f}, {self.task_weights.max():.2f}]")
    
    def forward(self, predictions: torch.Tensor, targets: torch.Tensor) -> Dict[str, torch.Tensor]:
        """è®¡ç®—ç»¼åˆæŸå¤±"""
        device = predictions.device
        batch_size = predictions.size(0)
        
        # è®¡ç®—ä¸»æŸå¤±
        if isinstance(self.main_loss, (nn.MSELoss, nn.L1Loss)):
            task_losses = self.main_loss(predictions, targets)
            # åº”ç”¨ä»»åŠ¡æƒé‡
            task_losses = task_losses * self.task_weights.to(device)
            main_loss = task_losses.mean()
        else:
            main_loss = self.main_loss(predictions, targets)
            task_losses = torch.zeros(12, device=device)  # ç®€åŒ–ç‰ˆï¼Œä¸åˆ†è§£åˆ°ä»»åŠ¡
        
        # ä¸€è‡´æ€§æŸå¤±
        if self.consistency_weight > 0 and hasattr(self, 'consistency_loss'):
            consistency_loss = self.consistency_loss(predictions)
            total_loss = main_loss + consistency_loss
        else:
            consistency_loss = torch.tensor(0.0, device=device)
            total_loss = main_loss
        
        # è®¡ç®—é¢å¤–çš„å›å½’æŒ‡æ ‡
        with torch.no_grad():
            mae = torch.abs(predictions - targets).mean()
            rmse = torch.sqrt(((predictions - targets) ** 2).mean())
            
            # è®¡ç®—ç›¸å…³ç³»æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
            pred_mean = predictions.mean()
            target_mean = targets.mean()
            pred_std = predictions.std()
            target_std = targets.std()
            
            if pred_std > 0 and target_std > 0:
                correlation = ((predictions - pred_mean) * (targets - target_mean)).mean() / (pred_std * target_std)
            else:
                correlation = torch.tensor(0.0, device=device)
        
        return {
            'total_loss': total_loss,
            'main_loss': main_loss,
            'consistency_loss': consistency_loss,
            'task_losses': task_losses,
            'mae': mae,
            'rmse': rmse,
            'correlation': correlation
        }

class ConsistencyLoss(nn.Module):
    """æ—¶é—´å’Œéœ‡çº§ä¸€è‡´æ€§æŸå¤±"""
    
    def __init__(self, weight: float = 0.1):
        super().__init__()
        self.weight = weight
    
    def forward(self, predictions: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—ä¸€è‡´æ€§æŸå¤±"""
        batch_size = predictions.size(0)
        pred_reshaped = predictions.view(batch_size, 3, 4)
        
        consistency_loss = 0.0
        
        # æ—¶é—´ä¸€è‡´æ€§ï¼ˆè½¯çº¦æŸï¼‰
        for t in range(2):
            # 14å¤©/30å¤©çš„æ¦‚ç‡åº”è¯¥è¶‹å‘äºå¤§äºç­‰äº7å¤©/14å¤©
            diff = F.relu(pred_reshaped[:, t, :] - pred_reshaped[:, t+1, :] - 0.05)  # å…è®¸å°çš„åè½¬
            consistency_loss = consistency_loss + diff.mean()
        
        # éœ‡çº§ä¸€è‡´æ€§ï¼ˆè½¯çº¦æŸï¼‰
        for t in range(3):
            for m in range(3):
                # å¤§éœ‡çº§æ¦‚ç‡åº”è¯¥å°äºç­‰äºå°éœ‡çº§
                diff = F.relu(pred_reshaped[:, t, m+1] - pred_reshaped[:, t, m] - 0.02)  # å…è®¸å°çš„åè½¬
                consistency_loss = consistency_loss + diff.mean() * 0.5
        
        return consistency_loss * self.weight

# å‘åå…¼å®¹çš„åˆ«å
EarthquakeLoss = EarthquakeRegressionLoss
BalancedBCELoss = WeightedMSELoss  # ç®€å•æ˜ å°„ï¼Œå®é™…å·²æ”¹ä¸ºMSE
ImprovedMultiTaskLoss = EarthquakeRegressionLoss
ImprovedFocalLoss = HuberLoss  # ç®€å•æ˜ å°„