"""
åœ°éœ‡é¢„æµ‹ä¸»æ¨¡å‹æ¶æ„
æ•´åˆæ‰€æœ‰å­æ¨¡å—æ„å»ºå®Œæ•´çš„CNN+LSTM+Attention+å¤šä»»åŠ¡å­¦ä¹ æ¨¡å‹
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, List, Dict, Optional, Union
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥æ‰€æœ‰å­æ¨¡å—
from .model_utils import init_weights
from .loss_functions import EarthquakeLoss
from .spatial_modules import MultiScaleCNN
from .temporal_modules import TemporalEncoder
from .attention_modules import EarthquakeAttentionFusion
from .multi_task_heads import MultiTaskOutputLayer, PredictionPostProcessor

class EarthquakePredictionModel(nn.Module):
    """
    åœ°éœ‡é¢„æµ‹ä¸»æ¨¡å‹
    CNN + ConvLSTM + Attention + å¤šä»»åŠ¡å­¦ä¹ çš„å®Œæ•´æ¶æ„
    """
    
    def __init__(self,
                 # è¾“å…¥å‚æ•°
                 input_channels: int = 8,
                 input_time_steps: int = 90,
                 input_height: int = 22,
                 input_width: int = 24,
                 
                 # ç©ºé—´CNNå‚æ•°
                 spatial_base_channels: int = 64,
                 spatial_num_scales: int = 3,
                 
                 # æ—¶é—´ç¼–ç å™¨å‚æ•°
                 temporal_hidden_channels: int = 128,
                 use_bidirectional_lstm: bool = True,
                 use_temporal_attention: bool = True,
                 
                 # æ³¨æ„åŠ›èåˆå‚æ•°
                 attention_fusion_dim: int = 512,
                 attention_num_heads: int = 8,
                 
                 # å¤šä»»åŠ¡è¾“å‡ºå‚æ•°
                 shared_feature_dim: int = 128,
                 task_hidden_dim: int = 64,
                 prediction_windows: List[int] = [7, 14, 30],
                 magnitude_ranges: List[Tuple[float, float]] = [(3.0, 5.0), (5.0, 6.0), (6.0, 7.0), (7.0, 10.0)],
                 
                 # è®­ç»ƒå‚æ•°
                 dropout_rate: float = 0.2,
                 use_post_processing: bool = True):
        """
        åˆå§‹åŒ–åœ°éœ‡é¢„æµ‹æ¨¡å‹
        
        Args:
            input_channels: è¾“å…¥ç‰¹å¾é€šé“æ•° (8ä¸ªåœ°éœ‡ç‰¹å¾)
            input_time_steps: è¾“å…¥æ—¶é—´æ­¥é•¿ (90å¤©å†å²)
            input_height: è¾“å…¥ç½‘æ ¼é«˜åº¦ (22ä¸ªçº¬åº¦ç½‘æ ¼)
            input_width: è¾“å…¥ç½‘æ ¼å®½åº¦ (24ä¸ªç»åº¦ç½‘æ ¼)
            spatial_base_channels: ç©ºé—´CNNåŸºç¡€é€šé“æ•°
            spatial_num_scales: ç©ºé—´å¤šå°ºåº¦æ•°é‡
            temporal_hidden_channels: æ—¶é—´ç¼–ç å™¨éšè—é€šé“æ•°
            use_bidirectional_lstm: æ˜¯å¦ä½¿ç”¨åŒå‘LSTM
            use_temporal_attention: æ˜¯å¦ä½¿ç”¨æ—¶é—´æ³¨æ„åŠ›
            attention_fusion_dim: æ³¨æ„åŠ›èåˆåç»´åº¦
            attention_num_heads: å¤šå¤´æ³¨æ„åŠ›å¤´æ•°
            shared_feature_dim: å¤šä»»åŠ¡å…±äº«ç‰¹å¾ç»´åº¦
            task_hidden_dim: ä»»åŠ¡ç‰¹å®šéšè—å±‚ç»´åº¦
            prediction_windows: é¢„æµ‹æ—¶é—´çª—å£åˆ—è¡¨
            magnitude_ranges: éœ‡çº§èŒƒå›´åˆ—è¡¨
            dropout_rate: Dropoutæ¯”ä¾‹
            use_post_processing: æ˜¯å¦ä½¿ç”¨é¢„æµ‹åå¤„ç†
        """
        super(EarthquakePredictionModel, self).__init__()
        
        # ä¿å­˜é…ç½®å‚æ•°
        self.config = {
            'input_channels': input_channels,
            'input_time_steps': input_time_steps,
            'input_height': input_height,
            'input_width': input_width,
            'spatial_base_channels': spatial_base_channels,
            'spatial_num_scales': spatial_num_scales,
            'temporal_hidden_channels': temporal_hidden_channels,
            'use_bidirectional_lstm': use_bidirectional_lstm,
            'use_temporal_attention': use_temporal_attention,
            'attention_fusion_dim': attention_fusion_dim,
            'attention_num_heads': attention_num_heads,
            'shared_feature_dim': shared_feature_dim,
            'task_hidden_dim': task_hidden_dim,
            'prediction_windows': prediction_windows,
            'magnitude_ranges': magnitude_ranges,
            'dropout_rate': dropout_rate,
            'use_post_processing': use_post_processing
        }
        
        # è®¡ç®—ä¸­é—´ç»´åº¦
        self.spatial_output_channels = spatial_base_channels * (2 ** (spatial_num_scales - 1))
        self.temporal_output_channels = self.spatial_output_channels  # TemporalEncoderä¿æŒç»´åº¦
        
        # 1. ç©ºé—´ç‰¹å¾æå–æ¨¡å— (CNN)
        self.spatial_encoder = MultiScaleCNN(
            in_channels=input_channels,
            base_channels=spatial_base_channels,
            num_scales=spatial_num_scales
        )
        
        # 2. æ—¶é—´ç‰¹å¾æå–æ¨¡å— (ConvLSTM + Attention)
        self.temporal_encoder = TemporalEncoder(
            input_channels=self.spatial_output_channels,
            lstm_hidden_channels=temporal_hidden_channels,
            use_bidirectional=use_bidirectional_lstm,
            use_attention=use_temporal_attention,
            dropout_rate=dropout_rate
        )
        
        # 3. æ³¨æ„åŠ›èåˆæ¨¡å—
        self.attention_fusion = EarthquakeAttentionFusion(
            spatial_channels=self.spatial_output_channels,
            temporal_channels=self.temporal_output_channels,
            fusion_dim=attention_fusion_dim,
            num_heads=attention_num_heads
        )
        
        # 4. å¤šä»»åŠ¡è¾“å‡ºæ¨¡å—
        self.multi_task_head = MultiTaskOutputLayer(
            input_channels=attention_fusion_dim,
            shared_feature_dim=shared_feature_dim,
            task_hidden_dim=task_hidden_dim,
            prediction_windows=prediction_windows,
            magnitude_ranges=magnitude_ranges
        )
        
        # 5. é¢„æµ‹åå¤„ç†æ¨¡å— (å¯é€‰)
        if use_post_processing:
            self.post_processor = PredictionPostProcessor(
                num_tasks=len(prediction_windows) * len(magnitude_ranges)
            )
        else:
            self.post_processor = None
        
        # 6. æŸå¤±å‡½æ•°
        self.loss_function = EarthquakeLoss(
            focal_weight=1.0,
            consistency_weight=0.1,
            uncertainty_weighting=True
        )
        
        # === 3x3ç½‘æ ¼ä¼˜åŒ–ï¼šæ·»åŠ é¢å¤–çš„æ­£åˆ™åŒ– ===
        # æ£€æµ‹æ˜¯å¦ä½¿ç”¨3x3ç½‘æ ¼ï¼ˆè¾“å…¥é«˜åº¦å’Œå®½åº¦è¾ƒå°ï¼‰
        self.is_coarse_grid = (input_height <= 10 and input_width <= 10)
        
        if self.is_coarse_grid:
            print("ğŸ”§ æ£€æµ‹åˆ°3x3ç½‘æ ¼é…ç½®ï¼Œå¯ç”¨é¢å¤–æ­£åˆ™åŒ–")
            
            # ä¸ºç²—ç½‘æ ¼æ·»åŠ é¢å¤–çš„dropoutå±‚
            self.extra_spatial_dropout = nn.Dropout2d(0.5)
            self.extra_temporal_dropout = nn.Dropout(0.5)
            
            # è°ƒæ•´dropoutç‡
            enhanced_dropout_rate = min(dropout_rate * 2.0, 0.6)
            
            # ä¸ºå…³é”®å±‚æ·»åŠ å™ªå£°æ³¨å…¥ï¼ˆè®­ç»ƒæ—¶çš„æ­£åˆ™åŒ–ï¼‰
            self.training_noise_std = 0.05
            
            # å®šä¹‰L2æ­£åˆ™åŒ–å¼ºåº¦å’Œç‰¹å¾èŒƒæ•°çº¦æŸ
            self.l2_regularization_strength = 0.001
            self.feature_norm_constraint = 10.0
            
            print(f"  - é¢å¤–ç©ºé—´Dropout: 0.5")
            print(f"  - é¢å¤–æ—¶é—´Dropout: 0.5")
            print(f"  - å¢å¼ºDropoutç‡: {enhanced_dropout_rate}")
            print(f"  - è®­ç»ƒå™ªå£°æ ‡å‡†å·®: {self.training_noise_std}")
            print(f"  - L2æ­£åˆ™åŒ–å¼ºåº¦: {self.l2_regularization_strength}")
            print(f"  - ç‰¹å¾èŒƒæ•°çº¦æŸ: {self.feature_norm_constraint}")

            # æ·»åŠ è°±å½’ä¸€åŒ–åˆ°å…³é”®å±‚ï¼ˆç¨³å®šè®­ç»ƒï¼‰
            self._apply_spectral_norm()
        else:
            self.extra_spatial_dropout = None
            self.extra_temporal_dropout = None
            self.training_noise_std = 0.0
            self.l2_regularization_strength = 0.0
            self.feature_norm_constraint = float('inf')
        
        # åˆå§‹åŒ–æƒé‡
        self.apply(lambda m: init_weights(m, 'kaiming_uniform'))
        
        self.float()  # ç¡®ä¿æ‰€æœ‰å‚æ•°ä¸ºfloat32
        torch.set_default_tensor_type(torch.FloatTensor)  # è®¾ç½®é»˜è®¤å¼ é‡ç±»å‹
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        self._print_model_info()
    
    # åº”ç”¨è°±å½’ä¸€åŒ–
    def _apply_spectral_norm(self):
        """å¯¹å…³é”®å±‚åº”ç”¨è°±å½’ä¸€åŒ–"""
        # å¯¹ç©ºé—´ç¼–ç å™¨çš„å·ç§¯å±‚åº”ç”¨è°±å½’ä¸€åŒ–
        for module in self.spatial_encoder.modules():
            if isinstance(module, nn.Conv2d):
                torch.nn.utils.spectral_norm(module)
        
        # å¯¹æ³¨æ„åŠ›èåˆçš„å…³é”®å±‚åº”ç”¨è°±å½’ä¸€åŒ–
        for module in self.attention_fusion.modules():
            if isinstance(module, nn.Linear) and module.out_features > 256:
                torch.nn.utils.spectral_norm(module)


    def _print_model_info(self):
        """æ‰“å°æ¨¡å‹æ¶æ„ä¿¡æ¯"""
        print("\n" + "="*80)
        print("ğŸŒ åœ°éœ‡é¢„æµ‹æ¨¡å‹æ¶æ„ä¿¡æ¯")
        print("="*80)
        
        print(f"ğŸ“Š è¾“å…¥é…ç½®:")
        print(f"  è¾“å…¥ç»´åº¦: [{self.config['input_time_steps']}, {self.config['input_height']}, {self.config['input_width']}, {self.config['input_channels']}]")
        print(f"  ç½‘æ ¼åˆ†è¾¨ç‡: 1Â°Ã—1Â° (å…±{self.config['input_height']}Ã—{self.config['input_width']}={self.config['input_height']*self.config['input_width']}ä¸ªç½‘æ ¼)")
        print(f"  æ—¶é—´è·¨åº¦: {self.config['input_time_steps']}å¤©å†å²æ•°æ®")
        
        print(f"\nğŸ§  ç½‘ç»œæ¶æ„:")
        print(f"  ç©ºé—´ç¼–ç å™¨: MultiScaleCNN ({self.config['input_channels']} â†’ {self.spatial_output_channels}é€šé“)")
        print(f"  æ—¶é—´ç¼–ç å™¨: TemporalEncoder (åŒå‘LSTM: {self.config['use_bidirectional_lstm']}, æ³¨æ„åŠ›: {self.config['use_temporal_attention']})")
        print(f"  æ³¨æ„åŠ›èåˆ: EarthquakeAttentionFusion ({self.config['attention_fusion_dim']}ç»´)")
        print(f"  å¤šä»»åŠ¡è¾“å‡º: {len(self.config['prediction_windows'])}Ã—{len(self.config['magnitude_ranges'])}={len(self.config['prediction_windows'])*len(self.config['magnitude_ranges'])}ä¸ªä»»åŠ¡")
        
        print(f"\nğŸ¯ é¢„æµ‹ä»»åŠ¡:")
        print(f"  æ—¶é—´çª—å£: {self.config['prediction_windows']} å¤©")
        print(f"  éœ‡çº§èŒƒå›´: {self.config['magnitude_ranges']}")
        
        # è®¡ç®—æ€»å‚æ•°æ•°
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        print(f"\nğŸ“ˆ æ¨¡å‹è§„æ¨¡:")
        print(f"  æ€»å‚æ•°æ•°: {total_params:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"  æ¨¡å‹å¤§å°: {total_params * 4 / (1024**2):.2f} MB")
        
        print("="*80)
    
    def forward(self, x: torch.Tensor, return_intermediates: bool = False) -> Union[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        å‰å‘ä¼ æ’­ - æ·»åŠ äº†3x3ç½‘æ ¼çš„é¢å¤–æ­£åˆ™åŒ–
        
        Args:
            x: è¾“å…¥æ•°æ® [batch, time_steps, height, width, channels]
            return_intermediates: æ˜¯å¦è¿”å›ä¸­é—´ç»“æœ
            
        Returns:
            é¢„æµ‹ç»“æœæˆ–åŒ…å«ä¸­é—´ç»“æœçš„å­—å…¸
        """
        batch_size, time_steps, height, width, channels = x.size()
        
        # éªŒè¯è¾“å…¥ç»´åº¦
        if input_height != 10 or input_width != 8:
            print(f"âš ï¸ è­¦å‘Šï¼šè¾“å…¥ç»´åº¦({input_height}Ã—{input_width})ä¸æ˜¯æ ‡å‡†çš„10Ã—8ä¸è§„åˆ™ç½‘æ ¼")
            print("   æ¨¡å‹å¯èƒ½æ— æ³•æ­£ç¡®å¤„ç†æ•°æ®ï¼")
        
        # === 3x3ç½‘æ ¼ä¼˜åŒ–ï¼šè®­ç»ƒæ—¶æ·»åŠ è¾“å…¥å™ªå£° ===
        if self.training and self.is_coarse_grid and self.training_noise_std > 0:
            # ä½¿ç”¨æ›´å¤æ‚çš„å™ªå£°æ¨¡å¼
            noise = torch.randn_like(x) * self.training_noise_std
            # æ·»åŠ ç©ºé—´ç›¸å…³å™ªå£°
            spatial_noise = F.interpolate(
                torch.randn(x.size(0), x.size(1), 4, 4, x.size(4)).to(x.device),
                size=(x.size(2), x.size(3)),
                mode='bilinear',
                align_corners=False
            )
            x = x + noise + spatial_noise * 0.02
  
        
        # 1. ç©ºé—´ç‰¹å¾æå–
        spatial_features = self.spatial_encoder(x)
        # è¾“å‡º: [batch, time_steps, height, width, spatial_output_channels]
        
        # === 3x3ç½‘æ ¼ä¼˜åŒ–ï¼šåº”ç”¨é¢å¤–çš„ç©ºé—´dropout ===
        if self.training and self.is_coarse_grid and self.extra_spatial_dropout is not None:
            # å¯¹æ¯ä¸ªæ—¶é—´æ­¥åº”ç”¨ç©ºé—´dropout
            spatial_features_list = []
            for t in range(time_steps):
                feat = spatial_features[:, t, :, :, :].permute(0, 3, 1, 2)  # [B, C, H, W]
                feat = self.extra_spatial_dropout(feat)
                feat = feat.permute(0, 2, 3, 1)  # [B, H, W, C]
                spatial_features_list.append(feat)
            spatial_features = torch.stack(spatial_features_list, dim=1)
        
        # 2. æ—¶é—´ç‰¹å¾æå–
        temporal_features, temporal_attention = self.temporal_encoder(spatial_features)
        # è¾“å‡º: [batch, time_steps, height, width, temporal_output_channels]
        
        # === 3x3ç½‘æ ¼ä¼˜åŒ–ï¼šåº”ç”¨é¢å¤–çš„æ—¶é—´dropout ===
        if self.training and self.is_coarse_grid and self.extra_temporal_dropout is not None:
            # å¯¹æ—¶é—´ç»´åº¦åº”ç”¨dropout
            b, t, h, w, c = temporal_features.shape
            temporal_features = temporal_features.view(b, t, -1)
            temporal_features = self.extra_temporal_dropout(temporal_features)
            temporal_features = temporal_features.view(b, t, h, w, c)
        
        # === æ·»åŠ ç‰¹å¾èŒƒæ•°çº¦æŸ ===
        if self.training and self.is_coarse_grid:
            # å¯¹ä¸­é—´ç‰¹å¾åº”ç”¨èŒƒæ•°çº¦æŸ
            with torch.no_grad():
                spatial_norm = torch.norm(spatial_features, p=2, dim=(2,3,4), keepdim=True)
                spatial_features = spatial_features / torch.clamp(spatial_norm / self.feature_norm_constraint, min=1.0)
                
                temporal_norm = torch.norm(temporal_features, p=2, dim=(2,3,4), keepdim=True)
                temporal_features = temporal_features / torch.clamp(temporal_norm / self.feature_norm_constraint, min=1.0)

        # è½¬æ¢ä¸ºæ³¨æ„åŠ›èåˆéœ€è¦çš„æ ¼å¼ [batch, time_steps, channels, height, width]
        spatial_features_for_attention = spatial_features.permute(0, 1, 4, 2, 3)
        temporal_features_for_attention = temporal_features.permute(0, 1, 4, 2, 3)
        

        # 3. æ³¨æ„åŠ›èåˆ
        fused_features, attention_weights = self.attention_fusion(
            spatial_features_for_attention, temporal_features_for_attention
        )
        # è¾“å‡º: [batch, time_steps, fusion_dim, height, width]
        
        # 4. å¤šä»»åŠ¡é¢„æµ‹
        task_predictions = self.multi_task_head(fused_features)
    
        # 5. é¢„æµ‹åå¤„ç† (å¯é€‰)
        all_predictions = task_predictions['all_tasks']  # [batch, num_tasks]
        
        if self.post_processor is not None:
            # ç¡®ä¿ä¼ é€’çš„æ˜¯è¿ç»­å†…å­˜çš„å¼ é‡
            final_predictions = self.post_processor(all_predictions.contiguous())
        else:
            final_predictions = all_predictions
        
        # ç¡®ä¿è¿”å›çš„å¼ é‡æ˜¯è¿ç»­çš„ä¸”ç‹¬ç«‹çš„
        final_predictions = final_predictions.contiguous()
        
        if return_intermediates:
            return {
                'predictions': final_predictions,
                'raw_predictions': all_predictions,
                'task_predictions': task_predictions,
                'spatial_features': spatial_features,
                'temporal_features': temporal_features,
                'fused_features': fused_features,
                'temporal_attention': temporal_attention,
                'attention_weights': attention_weights
            }
        else:
            return final_predictions
    
    # è®¡ç®—L2æ­£åˆ™åŒ–æŸå¤±
    def get_l2_regularization_loss(self) -> torch.Tensor:
        """è®¡ç®—L2æ­£åˆ™åŒ–æŸå¤±"""
        if not self.is_coarse_grid:
            return torch.tensor(0.0, device=next(self.parameters()).device)
        
        l2_loss = 0.0
        for name, param in self.named_parameters():
            if 'weight' in name and param.requires_grad:
                l2_loss += torch.norm(param, p=2)
        
        return l2_loss * self.l2_regularization_strength

    def compute_loss(self, predictions: torch.Tensor, targets: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—æŸå¤± - åŒ…å«L2æ­£åˆ™åŒ–
        
        Args:
            predictions: æ¨¡å‹é¢„æµ‹ [batch, num_tasks]
            targets: çœŸå®æ ‡ç­¾ [batch, num_tasks]
            
        Returns:
            æŸå¤±ä¿¡æ¯å­—å…¸
        """
        # è·å–åŸºç¡€æŸå¤±
        loss_dict = self.loss_function(predictions, targets)
        
        # æ·»åŠ L2æ­£åˆ™åŒ–æŸå¤±
        if self.training and self.is_coarse_grid:
            l2_loss = self.get_l2_regularization_loss()
            loss_dict['l2_loss'] = l2_loss
            loss_dict['total_loss'] = loss_dict['total_loss'] + l2_loss
        
        return loss_dict
    
    def predict_probabilities(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        é¢„æµ‹åœ°éœ‡å‘ç”Ÿæ¦‚ç‡
        
        Args:
            x: è¾“å…¥æ•°æ® [batch, time_steps, height, width, channels]
            
        Returns:
            é¢„æµ‹æ¦‚ç‡å­—å…¸
        """
        self.eval()
        with torch.no_grad():
            predictions = self.forward(x)
            
            # æŒ‰æ—¶é—´çª—å£åˆ†ç»„
            time_grouped = self.multi_task_head.get_predictions_by_time_window(predictions)
            
            # æŒ‰éœ‡çº§åˆ†ç»„  
            mag_grouped = self.multi_task_head.get_predictions_by_magnitude(predictions)
            
            return {
                'all_predictions': predictions,
                'by_time_window': time_grouped,
                'by_magnitude': mag_grouped,
                'task_names': self.multi_task_head.task_names
            }
    
    def predict_single_region(self, x: torch.Tensor, lat_range: Tuple[int, int], lon_range: Tuple[int, int]) -> Dict[str, torch.Tensor]:
        """
        é¢„æµ‹ç‰¹å®šåŒºåŸŸçš„åœ°éœ‡æ¦‚ç‡
        
        Args:
            x: è¾“å…¥æ•°æ® [batch, time_steps, height, width, channels]
            lat_range: çº¬åº¦ç½‘æ ¼èŒƒå›´ (start, end)
            lon_range: ç»åº¦ç½‘æ ¼èŒƒå›´ (start, end)
            
        Returns:
            åŒºåŸŸé¢„æµ‹ç»“æœ
        """
        # æå–åŒºåŸŸæ•°æ®
        lat_start, lat_end = lat_range
        lon_start, lon_end = lon_range
        
        region_data = x[:, :, lat_start:lat_end, lon_start:lon_end, :]
        
        # å¦‚æœåŒºåŸŸå¤ªå°ï¼Œè¿›è¡Œå¡«å……
        if region_data.size(2) < self.config['input_height'] or region_data.size(3) < self.config['input_width']:
            # ä½¿ç”¨é›¶å¡«å……æ‰©å±•åˆ°æ ‡å‡†å°ºå¯¸
            padded_data = torch.zeros_like(x)
            padded_data[:, :, :region_data.size(2), :region_data.size(3), :] = region_data
            region_data = padded_data
        
        # è¿›è¡Œé¢„æµ‹
        predictions = self.predict_probabilities(region_data)
        
        # æ·»åŠ åŒºåŸŸä¿¡æ¯
        predictions['region_info'] = {
            'lat_range': lat_range,
            'lon_range': lon_range,
            'region_size': (lat_end - lat_start, lon_end - lon_start)
        }
        
        return predictions
    
    def get_model_summary(self) -> Dict[str, any]:
        """
        è·å–æ¨¡å‹æ‘˜è¦ä¿¡æ¯
        
        Returns:
            æ¨¡å‹æ‘˜è¦å­—å…¸
        """
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        # è®¡ç®—å„æ¨¡å—å‚æ•°
        module_params = {}
        for name, module in self.named_children():
            module_params[name] = sum(p.numel() for p in module.parameters())
        
        return {
            'model_name': 'EarthquakePredictionModel',
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'model_size_mb': total_params * 4 / (1024**2),
            'module_parameters': module_params,
            'config': self.config,
            'task_info': self.multi_task_head.get_task_info()
        }
    
    def save_model(self, filepath: str, save_optimizer: bool = False, optimizer: Optional[torch.optim.Optimizer] = None):
        """
        ä¿å­˜æ¨¡å‹
        
        Args:
            filepath: ä¿å­˜è·¯å¾„
            save_optimizer: æ˜¯å¦ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€
            optimizer: ä¼˜åŒ–å™¨å®ä¾‹
        """
        save_dict = {
            'model_state_dict': self.state_dict(),
            'model_config': self.config,
            'model_summary': self.get_model_summary()
        }
        
        if save_optimizer and optimizer is not None:
            save_dict['optimizer_state_dict'] = optimizer.state_dict()
        
        torch.save(save_dict, filepath)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {filepath}")
    
    @classmethod
    def load_model(cls, filepath: str, device: Optional[torch.device] = None):
        """
        åŠ è½½æ¨¡å‹
        
        Args:
            filepath: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            device: ç›®æ ‡è®¾å¤‡
            
        Returns:
            åŠ è½½çš„æ¨¡å‹å®ä¾‹
        """
        if device is None:
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        checkpoint = torch.load(filepath, map_location=device)
        config = checkpoint['model_config']
        
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model = cls(**config)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(device)
        
        print(f"ğŸ“‚ æ¨¡å‹å·²åŠ è½½: {filepath}")
        print(f"   è®¾å¤‡: {device}")
        print(f"   å‚æ•°æ•°: {checkpoint['model_summary']['total_parameters']:,}")
        
        return model, checkpoint.get('optimizer_state_dict', None)

def test_earthquake_prediction_model():
    """æµ‹è¯•å®Œæ•´çš„åœ°éœ‡é¢„æµ‹æ¨¡å‹"""
    print("ğŸ§ª æµ‹è¯•å®Œæ•´åœ°éœ‡é¢„æµ‹æ¨¡å‹...")
    
    # æ¨¡æ‹Ÿè¾“å…¥æ•°æ® [batch, time_steps, height, width, channels]
    batch_size = 2
    time_steps = 90
    height, width = 22, 24
    channels = 8
    
    input_data = torch.randn(batch_size, time_steps, height, width, channels)
    print(f"ğŸ“Š è¾“å…¥æ•°æ®å½¢çŠ¶: {input_data.shape}")
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ—ï¸  åˆ›å»ºåœ°éœ‡é¢„æµ‹æ¨¡å‹:")
    model = EarthquakePredictionModel(
        input_channels=channels,
        input_time_steps=time_steps,
        input_height=height,
        input_width=width,
        spatial_base_channels=32,  # å‡å°‘é€šé“æ•°ä»¥åŠ å¿«æµ‹è¯•
        spatial_num_scales=2,
        temporal_hidden_channels=64,
        attention_fusion_dim=256,
        shared_feature_dim=64,
        task_hidden_dim=32
    )
    
    # è·å–æ¨¡å‹æ‘˜è¦
    summary = model.get_model_summary()
    print(f"\nğŸ“‹ æ¨¡å‹æ‘˜è¦:")
    print(f"  æ€»å‚æ•°æ•°: {summary['total_parameters']:,}")
    print(f"  æ¨¡å‹å¤§å°: {summary['model_size_mb']:.2f} MB")
    
    # 1. æµ‹è¯•å‰å‘ä¼ æ’­
    print("\nğŸ”„ æµ‹è¯•å‰å‘ä¼ æ’­:")
    model.eval()
    with torch.no_grad():
        # åŸºç¡€é¢„æµ‹
        predictions = model(input_data)
        print(f"  é¢„æµ‹è¾“å‡º: {predictions.shape}")
        print(f"  é¢„æµ‹èŒƒå›´: [{predictions.min():.3f}, {predictions.max():.3f}]")
        
        # è¯¦ç»†é¢„æµ‹ (åŒ…å«ä¸­é—´ç»“æœ)
        detailed_results = model(input_data, return_intermediates=True)
        print(f"  ä¸­é—´ç»“æœæ•°: {len(detailed_results)}")
    
    # 2. æµ‹è¯•æ¦‚ç‡é¢„æµ‹
    print("\nğŸ¯ æµ‹è¯•æ¦‚ç‡é¢„æµ‹:")
    prob_results = model.predict_probabilities(input_data)
    print(f"  å…¨ä»»åŠ¡é¢„æµ‹: {prob_results['all_predictions'].shape}")
    print(f"  æ—¶é—´çª—å£åˆ†ç»„: {len(prob_results['by_time_window'])} ç»„")
    print(f"  éœ‡çº§åˆ†ç»„: {len(prob_results['by_magnitude'])} ç»„")
    
    # 3. æµ‹è¯•åŒºåŸŸé¢„æµ‹
    print("\nğŸ—ºï¸  æµ‹è¯•åŒºåŸŸé¢„æµ‹:")
    region_results = model.predict_single_region(
        input_data, 
        lat_range=(5, 15), 
        lon_range=(8, 18)
    )
    print(f"  åŒºåŸŸé¢„æµ‹: {region_results['all_predictions'].shape}")
    print(f"  åŒºåŸŸå¤§å°: {region_results['region_info']['region_size']}")
    
    # 4. æµ‹è¯•æŸå¤±è®¡ç®—
    print("\nğŸ’” æµ‹è¯•æŸå¤±è®¡ç®—:")
    # åˆ›å»ºæ¨¡æ‹Ÿæ ‡ç­¾
    targets = torch.randint(0, 2, (batch_size, 12)).float()
    loss_info = model.compute_loss(predictions, targets)
    print(f"  æ€»æŸå¤±: {loss_info['total_loss']:.4f}")
    print(f"  FocalæŸå¤±: {loss_info['focal_loss']:.4f}")
    print(f"  ä¸€è‡´æ€§æŸå¤±: {loss_info['consistency_loss']:.4f}")
    
    # 5. æµ‹è¯•æ¨¡å‹ä¿å­˜/åŠ è½½
    print("\nğŸ’¾ æµ‹è¯•æ¨¡å‹ä¿å­˜/åŠ è½½:")
    save_path = "test_earthquake_model.pth"
    
    # ä¿å­˜æ¨¡å‹
    model.save_model(save_path)
    
    # åŠ è½½æ¨¡å‹
    loaded_model, _ = EarthquakePredictionModel.load_model(save_path)
    
    # éªŒè¯åŠ è½½çš„æ¨¡å‹
    with torch.no_grad():
        original_pred = model(input_data)
        loaded_pred = loaded_model(input_data)
        difference = torch.abs(original_pred - loaded_pred).max()
        print(f"  åŠ è½½éªŒè¯: æœ€å¤§å·®å¼‚ = {difference:.6f}")
    
    # 6. æ€§èƒ½åˆ†æ
    print("\nğŸ“Š æ€§èƒ½åˆ†æ:")
    input_memory = input_data.numel() * 4 / (1024**2)
    output_memory = predictions.numel() * 4 / (1024**2)
    model_memory = summary['model_size_mb']
    
    print(f"  è¾“å…¥å†…å­˜: {input_memory:.2f} MB")
    print(f"  è¾“å‡ºå†…å­˜: {output_memory:.2f} MB")
    print(f"  æ¨¡å‹å†…å­˜: {model_memory:.2f} MB")
    print(f"  æ€»å†…å­˜ä¼°ç®—: {input_memory + output_memory + model_memory:.2f} MB")
    
    print("\nâœ… å®Œæ•´åœ°éœ‡é¢„æµ‹æ¨¡å‹æµ‹è¯•å®Œæˆ!")
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    import os
    if os.path.exists(save_path):
        os.remove(save_path)
    
    return model, predictions, prob_results

if __name__ == "__main__":
    test_earthquake_prediction_model()