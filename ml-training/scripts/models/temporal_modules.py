"""
æ—¶é—´åºåˆ—ç‰¹å¾æå–æ¨¡å—
å®ç°ConvLSTMã€æ—¶é—´æ³¨æ„åŠ›ã€åŒå‘LSTMç­‰æ—¶åºå»ºæ¨¡ç»„ä»¶
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
from typing import Tuple, List, Optional, Dict
import warnings
warnings.filterwarnings('ignore')

class ConvLSTMCell(nn.Module):
    """
    å·ç§¯LSTMå•å…ƒ - ç»“åˆå·ç§¯å’ŒLSTMçš„æ—¶ç©ºå»ºæ¨¡èƒ½åŠ›
    """
    
    def __init__(self, 
                 input_channels: int,
                 hidden_channels: int,
                 kernel_size: int = 3,
                 bias: bool = True):
        """
        åˆå§‹åŒ–ConvLSTMå•å…ƒ
        
        Args:
            input_channels: è¾“å…¥é€šé“æ•°
            hidden_channels: éšè—çŠ¶æ€é€šé“æ•°
            kernel_size: å·ç§¯æ ¸å¤§å°
            bias: æ˜¯å¦ä½¿ç”¨åç½®
        """
        super(ConvLSTMCell, self).__init__()
        
        self.input_channels = input_channels
        self.hidden_channels = hidden_channels
        self.kernel_size = kernel_size
        self.padding = kernel_size // 2
        self.bias = bias
        
        # è¾“å…¥åˆ°éšè—çŠ¶æ€çš„å·ç§¯ (Input-to-Hidden)
        self.conv_ih = nn.Conv2d(
            input_channels, 4 * hidden_channels,
            kernel_size, padding=self.padding, bias=bias
        )
        
        # éšè—çŠ¶æ€åˆ°éšè—çŠ¶æ€çš„å·ç§¯ (Hidden-to-Hidden)
        self.conv_hh = nn.Conv2d(
            hidden_channels, 4 * hidden_channels,
            kernel_size, padding=self.padding, bias=bias
        )
        
        # åˆå§‹åŒ–æƒé‡
        self.init_weights()
    
    def init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for name, param in self.named_parameters():
            if 'weight' in name:
                nn.init.kaiming_normal_(param)
            elif 'bias' in name:
                nn.init.constant_(param, 0)
    
    def forward(self, input_tensor: torch.Tensor, 
                hidden_state: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            input_tensor: è¾“å…¥å¼ é‡ [batch, channels, height, width]
            hidden_state: (h_t, c_t) éšè—çŠ¶æ€å’Œç»†èƒçŠ¶æ€
            
        Returns:
            æ–°çš„éšè—çŠ¶æ€ (h_t+1, c_t+1)
        """
        h_prev, c_prev = hidden_state
        
        # è¾“å…¥å˜æ¢
        conv_input = self.conv_ih(input_tensor)
        
        # éšè—çŠ¶æ€å˜æ¢
        conv_hidden = self.conv_hh(h_prev)
        
        # ç»„åˆè¾“å…¥å’Œéšè—çŠ¶æ€
        conv_combined = conv_input + conv_hidden
        
        # åˆ†ç¦»å››ä¸ªé—¨
        i, f, o, g = torch.split(conv_combined, self.hidden_channels, dim=1)
        
        # è®¡ç®—é—¨å€¼
        input_gate = torch.sigmoid(i)      # è¾“å…¥é—¨
        forget_gate = torch.sigmoid(f)     # é—å¿˜é—¨
        output_gate = torch.sigmoid(o)     # è¾“å‡ºé—¨
        candidate = torch.tanh(g)          # å€™é€‰å€¼
        
        # æ›´æ–°ç»†èƒçŠ¶æ€
        c_next = forget_gate * c_prev + input_gate * candidate
        
        # æ›´æ–°éšè—çŠ¶æ€
        h_next = output_gate * torch.tanh(c_next)
        
        return h_next, c_next
    
    def init_hidden(self, batch_size: int, height: int, width: int, device: torch.device):
        """
        åˆå§‹åŒ–éšè—çŠ¶æ€
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            height: ç‰¹å¾å›¾é«˜åº¦
            width: ç‰¹å¾å›¾å®½åº¦
            device: è®¾å¤‡
            
        Returns:
            åˆå§‹éšè—çŠ¶æ€ (h_0, c_0)
        """
        h = torch.zeros(batch_size, self.hidden_channels, height, width, device=device)
        c = torch.zeros(batch_size, self.hidden_channels, height, width, device=device)
        return h, c

class ConvLSTM(nn.Module):
    """
    å¤šå±‚å·ç§¯LSTM - å¤„ç†æ—¶ç©ºåºåˆ—æ•°æ®
    """
    
    def __init__(self,
                 input_channels: int,
                 hidden_channels: List[int],
                 kernel_sizes: List[int],
                 num_layers: int,
                 bias: bool = True,
                 return_all_layers: bool = False):
        """
        åˆå§‹åŒ–ConvLSTM
        
        Args:
            input_channels: è¾“å…¥é€šé“æ•°
            hidden_channels: å„å±‚éšè—é€šé“æ•°åˆ—è¡¨
            kernel_sizes: å„å±‚å·ç§¯æ ¸å¤§å°åˆ—è¡¨
            num_layers: å±‚æ•°
            bias: æ˜¯å¦ä½¿ç”¨åç½®
            return_all_layers: æ˜¯å¦è¿”å›æ‰€æœ‰å±‚çš„è¾“å‡º
        """
        super(ConvLSTM, self).__init__()
        
        self.input_channels = input_channels
        self.hidden_channels = hidden_channels
        self.kernel_sizes = kernel_sizes
        self.num_layers = num_layers
        self.bias = bias
        self.return_all_layers = return_all_layers
        
        # æ„å»ºå„å±‚
        cell_list = []
        for i in range(num_layers):
            cur_input_channels = input_channels if i == 0 else hidden_channels[i-1]
            cell_list.append(ConvLSTMCell(
                input_channels=cur_input_channels,
                hidden_channels=hidden_channels[i],
                kernel_size=kernel_sizes[i],
                bias=bias
            ))
        
        self.cell_list = nn.ModuleList(cell_list)
        
        print(f"â° ConvLSTMåˆå§‹åŒ–:")
        print(f"  å±‚æ•°: {num_layers}")
        print(f"  éšè—é€šé“: {hidden_channels}")
        print(f"  å·ç§¯æ ¸å¤§å°: {kernel_sizes}")
    
    def forward(self, input_tensor: torch.Tensor, 
                hidden_state: Optional[List] = None) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            input_tensor: è¾“å…¥åºåˆ— [batch, time_steps, channels, height, width]
            hidden_state: åˆå§‹éšè—çŠ¶æ€
            
        Returns:
            (layer_output_list, last_state_list)
        """
        batch_size, seq_len, _, height, width = input_tensor.size()
        device = input_tensor.device
        
        # åˆå§‹åŒ–éšè—çŠ¶æ€
        if hidden_state is None:
            hidden_state = self._init_hidden(batch_size, height, width, device)
        
        layer_output_list = []
        last_state_list = []
        
        cur_layer_input = input_tensor
        
        for layer_idx in range(self.num_layers):
            h, c = hidden_state[layer_idx]
            output_inner = []
            
            for t in range(seq_len):
                h, c = self.cell_list[layer_idx](cur_layer_input[:, t, :, :, :], (h, c))
                output_inner.append(h)
            
            layer_output = torch.stack(output_inner, dim=1)
            cur_layer_input = layer_output
            
            layer_output_list.append(layer_output)
            last_state_list.append((h, c))
        
        if not self.return_all_layers:
            layer_output_list = layer_output_list[-1:]
        
        return layer_output_list, last_state_list
    
    def _init_hidden(self, batch_size: int, height: int, width: int, device: torch.device):
        """åˆå§‹åŒ–æ‰€æœ‰å±‚çš„éšè—çŠ¶æ€"""
        init_states = []
        for i in range(self.num_layers):
            init_states.append(self.cell_list[i].init_hidden(batch_size, height, width, device))
        return init_states

class TemporalAttention(nn.Module):
    """
    æ—¶é—´æ³¨æ„åŠ›æ¨¡å— - è¯†åˆ«é‡è¦çš„æ—¶é—´æ­¥
    """
    
    def __init__(self, 
                 input_channels: int,
                 attention_dim: int = 128,
                 use_position_encoding: bool = True):
        """
        åˆå§‹åŒ–æ—¶é—´æ³¨æ„åŠ›
        
        Args:
            input_channels: è¾“å…¥é€šé“æ•°
            attention_dim: æ³¨æ„åŠ›ç»´åº¦
            use_position_encoding: æ˜¯å¦ä½¿ç”¨ä½ç½®ç¼–ç 
        """
        super(TemporalAttention, self).__init__()
        
        self.input_channels = input_channels
        self.attention_dim = attention_dim
        self.use_position_encoding = use_position_encoding
        
        # æ³¨æ„åŠ›è®¡ç®—ç½‘ç»œ
        self.attention_conv = nn.Sequential(
            nn.Conv2d(input_channels, attention_dim, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(attention_dim, 1, kernel_size=1)
        )
        
        # ä½ç½®ç¼–ç 
        if use_position_encoding:
            self.position_encoding = PositionalEncoding(input_channels)
        
        print(f"â° æ—¶é—´æ³¨æ„åŠ›åˆå§‹åŒ–: {input_channels} â†’ {attention_dim} â†’ 1")
    
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥åºåˆ— [batch, time_steps, channels, height, width]
            
        Returns:
            (åŠ æƒåºåˆ—, æ³¨æ„åŠ›æƒé‡)
        """
        batch_size, time_steps, channels, height, width = x.size()
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        if self.use_position_encoding:
            x = self.position_encoding(x)
        
        # è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„æ³¨æ„åŠ›åˆ†æ•°
        attention_scores = []
        
        for t in range(time_steps):
            # è®¡ç®—è¯¥æ—¶é—´æ­¥çš„æ³¨æ„åŠ›åˆ†æ•° [batch, 1, height, width]
            score = self.attention_conv(x[:, t, :, :, :])
            attention_scores.append(score)
        
        # å †å å¹¶è®¡ç®—softmax [batch, time_steps, height, width]
        attention_scores = torch.stack(attention_scores, dim=1).squeeze(2)
        attention_weights = F.softmax(attention_scores, dim=1)
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        weighted_sequence = []
        for t in range(time_steps):
            # è·å–å½“å‰æ—¶é—´æ­¥çš„æƒé‡ [batch, height, width]
            weight = attention_weights[:, t, :, :]  # [batch, height, width]
            # æ‰©å±•ç»´åº¦ä»¥åŒ¹é…ç‰¹å¾ [batch, 1, height, width]
            weight = weight.unsqueeze(1)
            
            # åº”ç”¨æƒé‡åˆ°å½“å‰æ—¶é—´æ­¥çš„ç‰¹å¾
            weighted_frame = x[:, t, :, :, :] * weight  # å¹¿æ’­ä¹˜æ³•
            weighted_sequence.append(weighted_frame)
        
        weighted_sequence = torch.stack(weighted_sequence, dim=1)
        
        return weighted_sequence, attention_weights

class PositionalEncoding(nn.Module):
    """
    ä½ç½®ç¼–ç  - ä¸ºæ—¶é—´åºåˆ—æ·»åŠ ä½ç½®ä¿¡æ¯
    """
    
    def __init__(self, channels: int, max_len: int = 200):
        """
        åˆå§‹åŒ–ä½ç½®ç¼–ç 
        
        Args:
            channels: é€šé“æ•°
            max_len: æœ€å¤§åºåˆ—é•¿åº¦
        """
        super(PositionalEncoding, self).__init__()
        
        self.channels = channels
        
        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ
        pe = torch.zeros(max_len, channels)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        
        div_term = torch.exp(torch.arange(0, channels, 2).float() * 
                           (-math.log(10000.0) / channels))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        æ·»åŠ ä½ç½®ç¼–ç 
        
        Args:
            x: è¾“å…¥åºåˆ— [batch, time_steps, channels, height, width]
            
        Returns:
            æ·»åŠ ä½ç½®ç¼–ç çš„åºåˆ—
        """
        batch_size, time_steps, channels, height, width = x.size()
        
        # è·å–ä½ç½®ç¼–ç  [time_steps, channels]
        pos_encoding = self.pe[:time_steps, :]
        
        # é‡å¡‘ä¸º [1, time_steps, channels, 1, 1] ä»¥ä¾¿å¹¿æ’­
        pos_encoding = pos_encoding.unsqueeze(0).unsqueeze(-1).unsqueeze(-1)
        
        return x + pos_encoding

class BidirectionalConvLSTM(nn.Module):
    """
    åŒå‘å·ç§¯LSTM - åŒæ—¶å¤„ç†å‰å‘å’Œåå‘æ—¶åºä¿¡æ¯
    """
    
    def __init__(self, 
                 input_channels: int,
                 hidden_channels: int,
                 kernel_size: int = 3,
                 bias: bool = True):
        """
        åˆå§‹åŒ–åŒå‘ConvLSTM
        
        Args:
            input_channels: è¾“å…¥é€šé“æ•°
            hidden_channels: éšè—é€šé“æ•°
            kernel_size: å·ç§¯æ ¸å¤§å°
            bias: æ˜¯å¦ä½¿ç”¨åç½®
        """
        super(BidirectionalConvLSTM, self).__init__()
        
        self.input_channels = input_channels
        self.hidden_channels = hidden_channels
        
        # å‰å‘LSTM
        self.forward_lstm = ConvLSTM(
            input_channels=input_channels,
            hidden_channels=[hidden_channels],
            kernel_sizes=[kernel_size],
            num_layers=1,
            bias=bias
        )
        
        # åå‘LSTM
        self.backward_lstm = ConvLSTM(
            input_channels=input_channels,
            hidden_channels=[hidden_channels],
            kernel_sizes=[kernel_size],
            num_layers=1,
            bias=bias
        )
        
        # è¾“å‡ºé€šé“æ•°ä¸ºåŒå€
        self.out_channels = hidden_channels * 2
        
        print(f"â†”ï¸  åŒå‘ConvLSTM: {input_channels} â†’ {hidden_channels}Ã—2 = {self.out_channels}")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥åºåˆ— [batch, time_steps, channels, height, width]
            
        Returns:
            åŒå‘ç‰¹å¾ [batch, time_steps, hidden_channels*2, height, width]
        """
        # å‰å‘å¤„ç†
        forward_out, _ = self.forward_lstm(x)
        forward_features = forward_out[0]  # [batch, time_steps, hidden_channels, height, width]
        
        # åå‘å¤„ç† (ç¿»è½¬æ—¶é—´ç»´åº¦)
        x_reversed = torch.flip(x, dims=[1])
        backward_out, _ = self.backward_lstm(x_reversed)
        backward_features = backward_out[0]
        
        # å†æ¬¡ç¿»è½¬ä»¥å¯¹é½æ—¶é—´
        backward_features = torch.flip(backward_features, dims=[1])
        
        # æ‹¼æ¥å‰å‘å’Œåå‘ç‰¹å¾
        bidirectional_features = torch.cat([forward_features, backward_features], dim=2)
        
        return bidirectional_features

class TemporalEncoder(nn.Module):
    """
    æ—¶é—´ç¼–ç å™¨ - æ•´åˆæ—¶é—´ç‰¹å¾æå–ç»„ä»¶
    """
    
    def __init__(self,
                 input_channels: int = 256,
                 lstm_hidden_channels: int = 128,
                 use_bidirectional: bool = True,
                 use_attention: bool = True,
                 dropout_rate: float = 0.1):
        """
        åˆå§‹åŒ–æ—¶é—´ç¼–ç å™¨
        
        Args:
            input_channels: è¾“å…¥é€šé“æ•° (æ¥è‡ªç©ºé—´CNN)
            lstm_hidden_channels: LSTMéšè—é€šé“æ•°
            use_bidirectional: æ˜¯å¦ä½¿ç”¨åŒå‘LSTM
            use_attention: æ˜¯å¦ä½¿ç”¨æ—¶é—´æ³¨æ„åŠ›
            dropout_rate: Dropoutæ¯”ä¾‹
        """
        super(TemporalEncoder, self).__init__()
        
        self.input_channels = input_channels
        self.lstm_hidden_channels = lstm_hidden_channels
        self.use_bidirectional = use_bidirectional
        self.use_attention = use_attention
        
        # è¾“å…¥é™ç»´ (å‡å°‘è®¡ç®—é‡)
        self.input_projection = nn.Sequential(
            nn.Conv2d(input_channels, lstm_hidden_channels, kernel_size=1),
            nn.BatchNorm2d(lstm_hidden_channels),
            nn.ReLU(inplace=True),
            nn.Dropout2d(dropout_rate)
        )
        
        # æ—¶åºå»ºæ¨¡å±‚
        if use_bidirectional:
            self.temporal_layer = BidirectionalConvLSTM(
                input_channels=lstm_hidden_channels,
                hidden_channels=lstm_hidden_channels//2,  # åŒå‘åä¼šç¿»å€
                kernel_size=3
            )
            temporal_out_channels = lstm_hidden_channels
        else:
            self.temporal_layer = ConvLSTM(
                input_channels=lstm_hidden_channels,
                hidden_channels=[lstm_hidden_channels],
                kernel_sizes=[3],
                num_layers=1
            )
            temporal_out_channels = lstm_hidden_channels
        
        # æ—¶é—´æ³¨æ„åŠ›
        if use_attention:
            self.temporal_attention = TemporalAttention(
                input_channels=temporal_out_channels,
                attention_dim=64
            )
        
        # è¾“å‡ºæŠ•å½±
        self.output_projection = nn.Sequential(
            nn.Conv2d(temporal_out_channels, input_channels, kernel_size=1),
            nn.BatchNorm2d(input_channels),
            nn.ReLU(inplace=True)
        )
        
        self.out_channels = input_channels
        
        print(f"ğŸ• æ—¶é—´ç¼–ç å™¨åˆå§‹åŒ–:")
        print(f"  è¾“å…¥é€šé“: {input_channels}")
        print(f"  LSTMéšè—é€šé“: {lstm_hidden_channels}")
        print(f"  åŒå‘LSTM: {use_bidirectional}")
        print(f"  æ—¶é—´æ³¨æ„åŠ›: {use_attention}")
        print(f"  è¾“å‡ºé€šé“: {self.out_channels}")
    
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: ç©ºé—´ç‰¹å¾åºåˆ— [batch, time_steps, height, width, channels]
            
        Returns:
            (æ—¶åºç‰¹å¾, æ³¨æ„åŠ›æƒé‡)
        """
        batch_size, time_steps, height, width, channels = x.size()
        
        # è¾“å…¥æŠ•å½± - é€å¸§å¤„ç†
        # x shape: [batch, time_steps, height, width, channels]
        # éœ€è¦è½¬æ¢ä¸º [batch, time_steps, channels, height, width]
        x = x.permute(0, 1, 4, 2, 3)  # [batch, time_steps, channels, height, width]
        
        projected_frames = []
        for t in range(time_steps):
            frame = self.input_projection(x[:, t, :, :, :])  # [batch, channels, height, width]
            projected_frames.append(frame)
        
        projected_sequence = torch.stack(projected_frames, dim=1)
        # projected_sequence: [batch, time_steps, lstm_hidden_channels, height, width]
        
        # æ—¶åºå»ºæ¨¡
        if self.use_bidirectional:
            temporal_features = self.temporal_layer(projected_sequence)
        else:
            temporal_out, _ = self.temporal_layer(projected_sequence)
            temporal_features = temporal_out[0]
        
        # æ—¶é—´æ³¨æ„åŠ›
        attention_weights = None
        if self.use_attention:
            temporal_features, attention_weights = self.temporal_attention(temporal_features)
        
        # è¾“å‡ºæŠ•å½± - é€å¸§å¤„ç†
        output_frames = []
        for t in range(time_steps):
            frame = self.output_projection(temporal_features[:, t, :, :, :])
            output_frames.append(frame)
        
        output_sequence = torch.stack(output_frames, dim=1)
        
        # è½¬æ¢å›åŸå§‹æ ¼å¼ [batch, time_steps, height, width, channels]
        output_sequence = output_sequence.permute(0, 1, 3, 4, 2)
        
        return output_sequence, attention_weights

def test_temporal_modules():
    """æµ‹è¯•æ—¶é—´åºåˆ—ç‰¹å¾æå–æ¨¡å—"""
    print("ğŸ§ª æµ‹è¯•æ—¶é—´åºåˆ—ç‰¹å¾æå–æ¨¡å—...")
    
    # æ¨¡æ‹Ÿè¾“å…¥æ•°æ® [batch, time_steps, channels, height, width]
    batch_size = 2
    time_steps = 10  # å‡å°‘æ—¶é—´æ­¥ä»¥åŠ å¿«æµ‹è¯•
    channels = 64
    height, width = 22, 24
    
    input_tensor = torch.randn(batch_size, time_steps, channels, height, width)
    print(f"ğŸ“Š è¾“å…¥æ•°æ®å½¢çŠ¶: {input_tensor.shape}")
    
    # 1. æµ‹è¯•ConvLSTMå•å…ƒ
    print("\n1. æµ‹è¯•ConvLSTMå•å…ƒ:")
    conv_lstm_cell = ConvLSTMCell(input_channels=channels, hidden_channels=32, kernel_size=3)
    
    # åˆå§‹åŒ–éšè—çŠ¶æ€
    device = input_tensor.device
    h_0, c_0 = conv_lstm_cell.init_hidden(batch_size, height, width, device)
    
    # æµ‹è¯•å•æ­¥
    h_1, c_1 = conv_lstm_cell(input_tensor[:, 0, :, :, :], (h_0, c_0))
    print(f"  éšè—çŠ¶æ€: {h_0.shape} â†’ {h_1.shape}")
    print(f"  ç»†èƒçŠ¶æ€: {c_0.shape} â†’ {c_1.shape}")
    
    # 2. æµ‹è¯•å®Œæ•´ConvLSTM
    print("\n2. æµ‹è¯•ConvLSTM:")
    conv_lstm = ConvLSTM(
        input_channels=channels,
        hidden_channels=[32, 64],
        kernel_sizes=[3, 3],
        num_layers=2
    )
    
    lstm_out, lstm_states = conv_lstm(input_tensor)
    print(f"  è¾“å…¥: {input_tensor.shape}")
    print(f"  è¾“å‡º: {lstm_out[0].shape}")
    print(f"  å±‚æ•°: {len(lstm_out)}")
    
    # 3. æµ‹è¯•ä½ç½®ç¼–ç 
    print("\n3. æµ‹è¯•ä½ç½®ç¼–ç :")
    pos_encoding = PositionalEncoding(channels=channels)
    encoded_input = pos_encoding(input_tensor)
    print(f"  ç¼–ç å‰: {input_tensor.shape}")
    print(f"  ç¼–ç å: {encoded_input.shape}")
    print(f"  æ•°å€¼å˜åŒ–: {torch.mean(torch.abs(encoded_input - input_tensor)):.6f}")
    
    # 4. æµ‹è¯•æ—¶é—´æ³¨æ„åŠ›
    print("\n4. æµ‹è¯•æ—¶é—´æ³¨æ„åŠ›:")
    temporal_attention = TemporalAttention(input_channels=channels, attention_dim=32)
    attended_seq, attention_weights = temporal_attention(input_tensor)
    print(f"  è¾“å…¥: {input_tensor.shape}")
    print(f"  åŠ æƒåºåˆ—: {attended_seq.shape}")
    print(f"  æ³¨æ„åŠ›æƒé‡: {attention_weights.shape}")
    print(f"  æƒé‡æ€»å’Œ: {attention_weights.sum(dim=1).mean():.3f} (åº”æ¥è¿‘1.0)")
    
    # 5. æµ‹è¯•åŒå‘ConvLSTM
    print("\n5. æµ‹è¯•åŒå‘ConvLSTM:")
    bi_conv_lstm = BidirectionalConvLSTM(
        input_channels=channels,
        hidden_channels=32,
        kernel_size=3
    )
    
    bi_output = bi_conv_lstm(input_tensor)
    print(f"  è¾“å…¥: {input_tensor.shape}")
    print(f"  åŒå‘è¾“å‡º: {bi_output.shape}")
    print(f"  é€šé“æ‰©å±•: {channels} â†’ {bi_conv_lstm.out_channels}")
    
    # 6. æµ‹è¯•å®Œæ•´æ—¶é—´ç¼–ç å™¨
    print("\n6. æµ‹è¯•å®Œæ•´æ—¶é—´ç¼–ç å™¨:")
    
    # æ¨¡æ‹Ÿæ¥è‡ªç©ºé—´CNNçš„ç‰¹å¾ (æ›´å¤§çš„é€šé“æ•°)
    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä½¿ç”¨æ­£ç¡®çš„æ ¼å¼ [batch, time_steps, height, width, channels]
    spatial_features = torch.randn(batch_size, time_steps, height, width, 256)
    
    temporal_encoder = TemporalEncoder(
        input_channels=256,
        lstm_hidden_channels=128,
        use_bidirectional=True,
        use_attention=True,
        dropout_rate=0.1
    )
    
    # è®¡ç®—å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in temporal_encoder.parameters())
    print(f"  æ¨¡å‹å‚æ•°æ•°: {total_params:,}")
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        temporal_output, attention_weights = temporal_encoder(spatial_features)
        
    print(f"  è¾“å…¥: {spatial_features.shape}")
    print(f"  è¾“å‡º: {temporal_output.shape}")
    if attention_weights is not None:
        print(f"  æ³¨æ„åŠ›: {attention_weights.shape}")
    
    # 7. å†…å­˜å’Œæ€§èƒ½åˆ†æ
    print("\n7. æ€§èƒ½åˆ†æ:")
    input_memory = spatial_features.numel() * 4 / (1024**2)  # MB
    output_memory = temporal_output.numel() * 4 / (1024**2)  # MB
    print(f"  è¾“å…¥å†…å­˜: {input_memory:.2f} MB")
    print(f"  è¾“å‡ºå†…å­˜: {output_memory:.2f} MB")
    print(f"  å‚æ•°å†…å­˜: {total_params * 4 / (1024**2):.2f} MB")
    
    print("\nâœ… æ—¶é—´åºåˆ—ç‰¹å¾æå–æ¨¡å—æµ‹è¯•å®Œæˆ!")
    return temporal_encoder, temporal_output, attention_weights

if __name__ == "__main__":
    test_temporal_modules()