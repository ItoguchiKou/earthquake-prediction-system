"""
æ¨¡å‹å·¥å…·å‡½æ•°æ¨¡å—
æä¾›æƒé‡åˆå§‹åŒ–ã€æ¨¡å‹ä¿å­˜/åŠ è½½ã€å¯è§†åŒ–ç­‰é€šç”¨å·¥å…·å‡½æ•°
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import json
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
import warnings
warnings.filterwarnings('ignore')

class ModelUtils:
    """æ¨¡å‹å·¥å…·ç±» - æä¾›å„ç§å®ç”¨åŠŸèƒ½"""
    
    @staticmethod
    def get_device():
        """è·å–å¯ç”¨è®¾å¤‡"""
        if torch.cuda.is_available():
            device = torch.device('cuda')
            print(f"ğŸš€ ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
        else:
            device = torch.device('cpu')
            print("ğŸ’» ä½¿ç”¨CPU")
        return device
    
    @staticmethod
    def count_parameters(model: nn.Module) -> Dict[str, int]:
        """
        ç»Ÿè®¡æ¨¡å‹å‚æ•°æ•°é‡
        
        Args:
            model: PyTorchæ¨¡å‹
            
        Returns:
            å‚æ•°ç»Ÿè®¡å­—å…¸
        """
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        return {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'non_trainable_parameters': total_params - trainable_params
        }
    
    @staticmethod
    def print_model_info(model: nn.Module, input_shape: Tuple[int, ...]):
        """
        æ‰“å°æ¨¡å‹è¯¦ç»†ä¿¡æ¯
        
        Args:
            model: PyTorchæ¨¡å‹
            input_shape: è¾“å…¥æ•°æ®å½¢çŠ¶ (ä¸åŒ…å«batch_size)
        """
        print("\n" + "="*60)
        print("ğŸ§  æ¨¡å‹æ¶æ„ä¿¡æ¯")
        print("="*60)
        
        # å‚æ•°ç»Ÿè®¡
        param_stats = ModelUtils.count_parameters(model)
        print(f"ğŸ“Š å‚æ•°ç»Ÿè®¡:")
        print(f"  æ€»å‚æ•°æ•°: {param_stats['total_parameters']:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°: {param_stats['trainable_parameters']:,}")
        print(f"  å›ºå®šå‚æ•°: {param_stats['non_trainable_parameters']:,}")
        
        # æ¨¡å‹å¤§å°ä¼°ç®—
        param_size_mb = param_stats['total_parameters'] * 4 / (1024 * 1024)  # float32
        print(f"  æ¨¡å‹å¤§å°: {param_size_mb:.2f} MB")
        
        # è¾“å…¥è¾“å‡ºå½¢çŠ¶
        print(f"\nğŸ”„ è¾“å…¥è¾“å‡ºä¿¡æ¯:")
        print(f"  è¾“å…¥å½¢çŠ¶: {input_shape}")
        
        # å°è¯•æ¨æ–­è¾“å‡ºå½¢çŠ¶
        try:
            model.eval()
            with torch.no_grad():
                dummy_input = torch.randn(1, *input_shape)
                if torch.cuda.is_available():
                    dummy_input = dummy_input.cuda()
                    model = model.cuda()
                output = model(dummy_input)
                if isinstance(output, torch.Tensor):
                    print(f"  è¾“å‡ºå½¢çŠ¶: {tuple(output.shape[1:])}")
                elif isinstance(output, (list, tuple)):
                    for i, out in enumerate(output):
                        print(f"  è¾“å‡º{i}å½¢çŠ¶: {tuple(out.shape[1:])}")
        except Exception as e:
            print(f"  è¾“å‡ºå½¢çŠ¶: æ— æ³•æ¨æ–­ ({e})")
        
        print("="*60)

def init_weights(module: nn.Module, init_type: str = 'xavier_uniform'):
    """
    æƒé‡åˆå§‹åŒ–å‡½æ•°
    
    Args:
        module: ç½‘ç»œæ¨¡å—
        init_type: åˆå§‹åŒ–ç±»å‹ ['xavier_uniform', 'xavier_normal', 'kaiming_uniform', 'kaiming_normal']
    """
    if isinstance(module, (nn.Conv1d, nn.Conv2d, nn.Conv3d)):
        if init_type == 'xavier_uniform':
            nn.init.xavier_uniform_(module.weight)
        elif init_type == 'xavier_normal':
            nn.init.xavier_normal_(module.weight)
        elif init_type == 'kaiming_uniform':
            nn.init.kaiming_uniform_(module.weight, mode='fan_out', nonlinearity='relu')
        elif init_type == 'kaiming_normal':
            nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
        
        if module.bias is not None:
            nn.init.constant_(module.bias, 0)
    
    elif isinstance(module, (nn.Linear)):
        if init_type == 'xavier_uniform':
            nn.init.xavier_uniform_(module.weight)
        elif init_type == 'xavier_normal':
            nn.init.xavier_normal_(module.weight)
        elif init_type == 'kaiming_uniform':
            nn.init.kaiming_uniform_(module.weight, mode='fan_out', nonlinearity='relu')
        elif init_type == 'kaiming_normal':
            nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
        
        if module.bias is not None:
            nn.init.constant_(module.bias, 0)
    
    elif isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.LayerNorm)):
        nn.init.constant_(module.weight, 1)
        nn.init.constant_(module.bias, 0)
    
    elif isinstance(module, (nn.LSTM, nn.GRU)):
        for name, param in module.named_parameters():
            if 'weight' in name:
                if init_type == 'xavier_uniform':
                    nn.init.xavier_uniform_(param)
                elif init_type == 'xavier_normal':
                    nn.init.xavier_normal_(param)
                elif 'kaiming' in init_type:
                    nn.init.kaiming_uniform_(param, mode='fan_out', nonlinearity='tanh')
            elif 'bias' in name:
                nn.init.constant_(param, 0)

def save_checkpoint(model: nn.Module, 
                   optimizer: torch.optim.Optimizer,
                   scheduler: torch.optim.lr_scheduler._LRScheduler,
                   epoch: int,
                   loss: float,
                   metrics: Dict[str, float],
                   save_path: str,
                   is_best: bool = False):
    """
    ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
    
    Args:
        model: æ¨¡å‹
        optimizer: ä¼˜åŒ–å™¨
        scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
        epoch: å½“å‰è½®æ¬¡
        loss: æŸå¤±å€¼
        metrics: è¯„ä¼°æŒ‡æ ‡
        save_path: ä¿å­˜è·¯å¾„
        is_best: æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
    """
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    
    # å‡†å¤‡æ£€æŸ¥ç‚¹æ•°æ®
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'loss': loss,
        'metrics': metrics,
        'timestamp': datetime.now().isoformat()
    }
    
    # ä¿å­˜æ£€æŸ¥ç‚¹
    torch.save(checkpoint, save_path)
    
    # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œé¢å¤–ä¿å­˜ä¸€ä»½
    if is_best:
        best_path = save_path.replace('.pth', '_best.pth')
        torch.save(checkpoint, best_path)
        print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_path}")
    
    print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {save_path}")

def load_checkpoint(model: nn.Module,
                   checkpoint_path: str,
                   optimizer: torch.optim.Optimizer = None,
                   scheduler: torch.optim.lr_scheduler._LRScheduler = None,
                   device: torch.device = None) -> Dict[str, Any]:
    """
    åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹
    
    Args:
        model: æ¨¡å‹
        checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
        optimizer: ä¼˜åŒ–å™¨(å¯é€‰)
        scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨(å¯é€‰)
        device: è®¾å¤‡
        
    Returns:
        æ£€æŸ¥ç‚¹ä¿¡æ¯å­—å…¸
    """
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    if device is None:
        device = ModelUtils.get_device()
    
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # åŠ è½½æ¨¡å‹æƒé‡
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
    if optimizer and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    # åŠ è½½è°ƒåº¦å™¨çŠ¶æ€
    if scheduler and 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict']:
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
    
    print(f"ğŸ“‚ æ£€æŸ¥ç‚¹å·²åŠ è½½: {checkpoint_path}")
    print(f"   è½®æ¬¡: {checkpoint.get('epoch', 'Unknown')}")
    print(f"   æŸå¤±: {checkpoint.get('loss', 'Unknown')}")
    
    return checkpoint

def save_model_architecture(model: nn.Module, save_path: str):
    """
    ä¿å­˜æ¨¡å‹æ¶æ„ä¿¡æ¯
    
    Args:
        model: æ¨¡å‹
        save_path: ä¿å­˜è·¯å¾„
    """
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    
    # æ”¶é›†æ¨¡å‹ä¿¡æ¯
    param_stats = ModelUtils.count_parameters(model)
    
    model_info = {
        'model_class': model.__class__.__name__,
        'model_structure': str(model),
        'parameter_stats': param_stats,
        'timestamp': datetime.now().isoformat()
    }
    
    # ä¿å­˜ä¸ºJSON
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(model_info, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“‹ æ¨¡å‹æ¶æ„å·²ä¿å­˜: {save_path}")

class AttentionVisualizer:
    """æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–å·¥å…·"""
    
    @staticmethod
    def plot_temporal_attention(attention_weights: np.ndarray,
                              time_labels: List[str] = None,
                              title: str = "æ—¶é—´æ³¨æ„åŠ›æƒé‡",
                              save_path: str = None):
        """
        å¯è§†åŒ–æ—¶é—´æ³¨æ„åŠ›æƒé‡
        
        Args:
            attention_weights: æ³¨æ„åŠ›æƒé‡ [time_steps] æˆ– [batch, time_steps]
            time_labels: æ—¶é—´æ ‡ç­¾
            title: å›¾è¡¨æ ‡é¢˜
            save_path: ä¿å­˜è·¯å¾„
        """
        if attention_weights.ndim > 1:
            attention_weights = attention_weights.mean(axis=0)  # å¹³å‡å¤šä¸ªæ ·æœ¬
        
        plt.figure(figsize=(12, 6))
        
        time_steps = len(attention_weights)
        x_pos = np.arange(time_steps)
        
        # ç»˜åˆ¶æŸ±çŠ¶å›¾
        bars = plt.bar(x_pos, attention_weights, alpha=0.7, color='skyblue', edgecolor='navy')
        
        # çªå‡ºæ˜¾ç¤ºé«˜æƒé‡åŒºåŸŸ
        max_weight = np.max(attention_weights)
        for i, (bar, weight) in enumerate(zip(bars, attention_weights)):
            if weight > 0.8 * max_weight:
                bar.set_color('orange')
        
        # è®¾ç½®æ ‡ç­¾
        if time_labels:
            plt.xticks(x_pos[::5], time_labels[::5], rotation=45)
        else:
            plt.xticks(x_pos[::10], [f"T-{time_steps-i}" for i in x_pos[::10]])
        
        plt.xlabel('æ—¶é—´æ­¥')
        plt.ylabel('æ³¨æ„åŠ›æƒé‡')
        plt.title(title)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š æ—¶é—´æ³¨æ„åŠ›å›¾å·²ä¿å­˜: {save_path}")
        
        plt.show()
    
    @staticmethod
    def plot_spatial_attention(attention_weights: np.ndarray,
                             title: str = "ç©ºé—´æ³¨æ„åŠ›æƒé‡",
                             save_path: str = None):
        """
        å¯è§†åŒ–ç©ºé—´æ³¨æ„åŠ›æƒé‡
        
        Args:
            attention_weights: ç©ºé—´æ³¨æ„åŠ›æƒé‡ [height, width] æˆ– [batch, height, width]
            title: å›¾è¡¨æ ‡é¢˜
            save_path: ä¿å­˜è·¯å¾„
        """
        if attention_weights.ndim > 2:
            attention_weights = attention_weights.mean(axis=0)  # å¹³å‡å¤šä¸ªæ ·æœ¬
        
        plt.figure(figsize=(10, 8))
        
        # ç»˜åˆ¶çƒ­åŠ›å›¾
        sns.heatmap(attention_weights, 
                   cmap='YlOrRd', 
                   annot=False,
                   cbar_kws={'label': 'æ³¨æ„åŠ›æƒé‡'},
                   xticklabels=False,
                   yticklabels=False)
        
        plt.title(title)
        plt.xlabel('ç»åº¦ç½‘æ ¼')
        plt.ylabel('çº¬åº¦ç½‘æ ¼')
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š ç©ºé—´æ³¨æ„åŠ›å›¾å·²ä¿å­˜: {save_path}")
        
        plt.show()
    
    @staticmethod
    def plot_channel_attention(attention_weights: np.ndarray,
                             channel_names: List[str] = None,
                             title: str = "é€šé“æ³¨æ„åŠ›æƒé‡",
                             save_path: str = None):
        """
        å¯è§†åŒ–é€šé“æ³¨æ„åŠ›æƒé‡
        
        Args:
            attention_weights: é€šé“æ³¨æ„åŠ›æƒé‡ [channels] æˆ– [batch, channels]
            channel_names: é€šé“åç§°
            title: å›¾è¡¨æ ‡é¢˜
            save_path: ä¿å­˜è·¯å¾„
        """
        if attention_weights.ndim > 1:
            attention_weights = attention_weights.mean(axis=0)  # å¹³å‡å¤šä¸ªæ ·æœ¬
        
        plt.figure(figsize=(10, 6))
        
        channels = len(attention_weights)
        x_pos = np.arange(channels)
        
        # ç»˜åˆ¶æŸ±çŠ¶å›¾
        bars = plt.bar(x_pos, attention_weights, alpha=0.7, color='lightgreen', edgecolor='darkgreen')
        
        # çªå‡ºæ˜¾ç¤ºé‡è¦é€šé“
        max_weight = np.max(attention_weights)
        for i, (bar, weight) in enumerate(zip(bars, attention_weights)):
            if weight > 0.8 * max_weight:
                bar.set_color('red')
        
        # è®¾ç½®æ ‡ç­¾
        if channel_names:
            plt.xticks(x_pos, channel_names, rotation=45, ha='right')
        else:
            plt.xticks(x_pos, [f"é€šé“{i}" for i in range(channels)])
        
        plt.xlabel('ç‰¹å¾é€šé“')
        plt.ylabel('æ³¨æ„åŠ›æƒé‡')
        plt.title(title)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š é€šé“æ³¨æ„åŠ›å›¾å·²ä¿å­˜: {save_path}")
        
        plt.show()

def calculate_model_flops(model: nn.Module, input_shape: Tuple[int, ...]) -> int:
    """
    ä¼°ç®—æ¨¡å‹FLOPs (æµ®ç‚¹è¿ç®—æ¬¡æ•°)
    
    Args:
        model: PyTorchæ¨¡å‹
        input_shape: è¾“å…¥å½¢çŠ¶ (ä¸åŒ…å«batch_size)
        
    Returns:
        ä¼°ç®—çš„FLOPsæ•°é‡
    """
    try:
        from thop import profile
        
        model.eval()
        dummy_input = torch.randn(1, *input_shape)
        flops, params = profile(model, inputs=(dummy_input,), verbose=False)
        
        return int(flops)
    except ImportError:
        print("âš ï¸  éœ€è¦å®‰è£…thopåº“æ¥è®¡ç®—FLOPs: pip install thop")
        return 0
    except Exception as e:
        print(f"âš ï¸  FLOPsè®¡ç®—å¤±è´¥: {e}")
        return 0

def get_model_memory_usage(model: nn.Module, input_shape: Tuple[int, ...], batch_size: int = 1) -> Dict[str, float]:
    """
    ä¼°ç®—æ¨¡å‹å†…å­˜ä½¿ç”¨é‡
    
    Args:
        model: PyTorchæ¨¡å‹
        input_shape: è¾“å…¥å½¢çŠ¶
        batch_size: æ‰¹æ¬¡å¤§å°
        
    Returns:
        å†…å­˜ä½¿ç”¨é‡å­—å…¸ (MB)
    """
    # å‚æ•°å†…å­˜
    param_memory = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024)
    
    # è¾“å…¥å†…å­˜
    input_memory = np.prod([batch_size] + list(input_shape)) * 4 / (1024 * 1024)  # float32
    
    # ä¼°ç®—æ¿€æ´»å†…å­˜ (ç²—ç•¥ä¼°è®¡ä¸ºå‚æ•°çš„2-4å€)
    activation_memory = param_memory * 3
    
    # æ¢¯åº¦å†…å­˜ (è®­ç»ƒæ—¶ï¼Œçº¦ç­‰äºå‚æ•°å†…å­˜)
    gradient_memory = param_memory
    
    return {
        'parameters_mb': param_memory,
        'input_mb': input_memory,
        'activations_mb': activation_memory,
        'gradients_mb': gradient_memory,
        'total_training_mb': param_memory + input_memory + activation_memory + gradient_memory,
        'total_inference_mb': param_memory + input_memory + activation_memory
    }

def test_model_utils():
    """æµ‹è¯•æ¨¡å‹å·¥å…·å‡½æ•°"""
    print("ğŸ§ª æµ‹è¯•æ¨¡å‹å·¥å…·å‡½æ•°...")
    
    # åˆ›å»ºç®€å•æµ‹è¯•æ¨¡å‹
    class SimpleModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.conv = nn.Conv2d(8, 32, 3, padding=1)
            self.fc = nn.Linear(32 * 22 * 24, 12)
        
        def forward(self, x):
            # x shape: [B, 90, 22, 24, 8] -> [B*90, 8, 22, 24]
            B, T, H, W, C = x.shape
            x = x.view(B*T, C, H, W)
            x = F.relu(self.conv(x))
            x = x.view(B*T, -1)
            x = self.fc(x)
            x = x.view(B, T, -1)
            return x.mean(dim=1)  # [B, 12]
    
    model = SimpleModel()
    input_shape = (90, 22, 24, 8)
    
    # æµ‹è¯•å„ç§åŠŸèƒ½
    print("\n1. æ¨¡å‹ä¿¡æ¯:")
    ModelUtils.print_model_info(model, input_shape)
    
    print("\n2. æƒé‡åˆå§‹åŒ–:")
    model.apply(lambda m: init_weights(m, 'xavier_uniform'))
    print("âœ… æƒé‡åˆå§‹åŒ–å®Œæˆ")
    
    print("\n3. å†…å­˜ä½¿ç”¨ä¼°ç®—:")
    memory_usage = get_model_memory_usage(model, input_shape, batch_size=4)
    for key, value in memory_usage.items():
        print(f"  {key}: {value:.2f} MB")
    
    print("\n4. æ³¨æ„åŠ›å¯è§†åŒ–æµ‹è¯•:")
    # æ¨¡æ‹Ÿæ³¨æ„åŠ›æƒé‡ (ä¿®å¤softmaxé—®é¢˜)
    def softmax(x):
        exp_x = np.exp(x - np.max(x))
        return exp_x / np.sum(exp_x)
    
    temporal_weights = softmax(np.random.randn(90))
    spatial_weights = softmax(np.random.randn(22, 24).flatten()).reshape(22, 24)
    channel_weights = softmax(np.random.randn(8))
    
    channel_names = [
        "åœ°éœ‡é¢‘ç‡", "å¹³å‡éœ‡çº§", "æœ€å¤§éœ‡çº§", "èƒ½é‡é‡Šæ”¾",
        "éœ‡çº§æ ‡å‡†å·®", "å¹³å‡æ·±åº¦", "æ—¶é—´å¯†åº¦", "ç©ºé—´ç›¸å…³æ€§"
    ]
    
    # æ³¨æ„: åœ¨å®é™…ä½¿ç”¨ä¸­ä¼šæ˜¾ç¤ºå›¾åƒï¼Œè¿™é‡Œåªæµ‹è¯•å‡½æ•°è°ƒç”¨
    print("  ğŸ“Š æ—¶é—´æ³¨æ„åŠ›æƒé‡å½¢çŠ¶:", temporal_weights.shape)
    print("  ğŸ“Š ç©ºé—´æ³¨æ„åŠ›æƒé‡å½¢çŠ¶:", spatial_weights.shape)
    print("  ğŸ“Š é€šé“æ³¨æ„åŠ›æƒé‡å½¢çŠ¶:", channel_weights.shape)
    
    print("\nâœ… æ¨¡å‹å·¥å…·å‡½æ•°æµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    test_model_utils()