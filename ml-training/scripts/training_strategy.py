"""
åœ°éœ‡é¢„æµ‹æ¨¡å‹è®­ç»ƒç­–ç•¥ - å›å½’ç‰ˆæœ¬
é’ˆå¯¹æ¦‚ç‡é¢„æµ‹ä»»åŠ¡ä¼˜åŒ–
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler
from torch.cuda.amp import autocast, GradScaler
import torch.profiler
import numpy as np
import pandas as pd
import os
import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_absolute_error, mean_squared_error
from scipy import ndimage
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥æ¨¡å‹æ¨¡å—
from models.model_architecture import EarthquakePredictionModel
from models.model_utils import save_checkpoint, load_checkpoint, AttentionVisualizer
# å¯¼å…¥æ”¹è¿›çš„æŸå¤±å‡½æ•°
from models.loss_functions import EarthquakeRegressionLoss

# GPUå†…å­˜ç›‘æ§å™¨ï¼ˆä¿æŒä¸å˜ï¼‰
class GPUMemoryMonitor:
    """GPUå†…å­˜ç›‘æ§å™¨"""
    
    @staticmethod
    def log_memory_usage(stage: str = "", device=None):
        if torch.cuda.is_available():
            if device is None:
                device = torch.cuda.current_device()
            allocated = torch.cuda.memory_allocated(device) / 1024**3
            reserved = torch.cuda.memory_reserved(device) / 1024**3
            max_allocated = torch.cuda.max_memory_allocated(device) / 1024**3
            
            print(f"ğŸ“Š GPUå†…å­˜ {stage}:")
            print(f"   å·²åˆ†é…: {allocated:.2f}GB")
            print(f"   å·²ä¿ç•™: {reserved:.2f}GB") 
            print(f"   å³°å€¼: {max_allocated:.2f}GB")
    
    @staticmethod
    def optimize_memory():
        """ä¼˜åŒ–GPUå†…å­˜ä½¿ç”¨"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

class EarthquakeDataset(Dataset):
    """åœ°éœ‡æ•°æ®é›†ç±» - ä¿æŒä¸å˜ï¼Œä½†ä¿®æ”¹æƒé‡è®¡ç®—"""
    
    def __init__(self, 
                 features_path: str,
                 stat_features_path: str,
                 labels_path: str,
                 timestamps_path: str,
                 sample_weights_path: str = None,
                 augment: bool = False,
                 balance_sampling: bool = True,
                 use_mmap: bool = True):
        """åˆå§‹åŒ–æ•°æ®é›†"""
        # ä½¿ç”¨å†…å­˜æ˜ å°„æ¨¡å¼åŠ è½½å¤§æ–‡ä»¶
        if use_mmap:
            try:
                self.features = np.load(features_path, mmap_mode='r')
                self.stat_features = np.load(stat_features_path, mmap_mode='r')
                self.labels = np.load(labels_path, mmap_mode='r')
                print("  âœ“ ä½¿ç”¨å†…å­˜æ˜ å°„æ¨¡å¼åŠ è½½æ•°æ®")
            except Exception as e:
                print(f"  âš ï¸ å†…å­˜æ˜ å°„å¤±è´¥ï¼Œä½¿ç”¨å¸¸è§„åŠ è½½: {e}")
                self.features = np.load(features_path)
                self.stat_features = np.load(stat_features_path)
                self.labels = np.load(labels_path)
        else:
            self.features = np.load(features_path)
            self.stat_features = np.load(stat_features_path)
            self.labels = np.load(labels_path)
        
        # åŠ è½½æ—¶é—´æˆ³
        timestamps_raw = np.load(timestamps_path)
        if timestamps_raw.dtype == np.float64:
            self.timestamps = pd.to_datetime(timestamps_raw, unit='s')
        else:
            self.timestamps = pd.to_datetime(timestamps_raw)
            
        self.augment = augment
        self.balance_sampling = balance_sampling
        self.use_mmap = use_mmap
        
        # è·å–å®é™…çš„ç½‘æ ¼å°ºå¯¸
        self.grid_height = self.features.shape[2]
        self.grid_width = self.features.shape[3]
        
        # ç¡®ä¿æ˜¯10Ã—8çš„ç»´åº¦
        if self.grid_height != 10 or self.grid_width != 8:
            print(f"âš ï¸ è­¦å‘Šï¼šæ•°æ®ç»´åº¦ä¸æ˜¯10Ã—8ï¼Œè€Œæ˜¯{self.grid_height}Ã—{self.grid_width}")
            print("   è¿™å¯èƒ½æ˜¯æ—§ç‰ˆæœ¬çš„æ•°æ®ï¼Œéœ€è¦é‡æ–°å¤„ç†ï¼")
        
        print(f"ğŸ“Š æ•°æ®é›†åŠ è½½å®Œæˆ:")
        print(f"  æ—¶åºç‰¹å¾å½¢çŠ¶: {self.features.shape}")
        print(f"  ç»Ÿè®¡ç‰¹å¾å½¢çŠ¶: {self.stat_features.shape}")
        print(f"  æ ‡ç­¾å½¢çŠ¶: {self.labels.shape}")
        print(f"  æ ·æœ¬æ•°é‡: {len(self.features)}")
        print(f"  ç½‘æ ¼å°ºå¯¸: {self.grid_height} Ã— {self.grid_width}")
        
        # æ”¹è¿›çš„æ ·æœ¬æƒé‡è®¡ç®— - åŸºäºæ¦‚ç‡å€¼è€ŒéäºŒå€¼åŒ–
        if sample_weights_path and os.path.exists(sample_weights_path):
            if use_mmap:
                try:
                    self.sample_weights = np.load(sample_weights_path, mmap_mode='r')
                    print("  âœ“ ä½¿ç”¨é¢„è®¡ç®—çš„æ ·æœ¬æƒé‡")
                except:
                    self.sample_weights = np.load(sample_weights_path)
            else:
                self.sample_weights = np.load(sample_weights_path)
        elif balance_sampling:
            print("  âš ï¸ æœªæ‰¾åˆ°é¢„è®¡ç®—æƒé‡ï¼Œé‡æ–°è®¡ç®—...")
            self.sample_weights = self._calculate_regression_sample_weights()
        else:
            self.sample_weights = None
    
    def _calculate_regression_sample_weights(self) -> np.ndarray:
        """å›å½’ä»»åŠ¡çš„æ ·æœ¬æƒé‡è®¡ç®— - åŸºäºæ¦‚ç‡å€¼"""
        sample_weights = np.ones(len(self.labels))
        
        # æœ‰æ•ˆä»»åŠ¡ï¼ˆæ‰€æœ‰ä»»åŠ¡ï¼‰
        valid_tasks = list(range(12))
        
        for i in range(len(self.labels)):
            sample_labels = self.labels[i]
            weight = 1.0
            
            # åŸºäºå¹³å‡æ¦‚ç‡å€¼è®¡ç®—æƒé‡
            mean_prob = np.mean(sample_labels)
            
            # å¯¹é«˜æ¦‚ç‡æ ·æœ¬ç»™äºˆæ›´é«˜æƒé‡
            if mean_prob > 0.3:
                weight *= 2.0
            elif mean_prob > 0.2:
                weight *= 1.5
            
            # å¯¹å¤§éœ‡ä»»åŠ¡çš„é«˜æ¦‚ç‡ç»™äºˆé¢å¤–æƒé‡
            for task_idx in [3, 7, 11]:  # M6.5+çš„ä»»åŠ¡
                if sample_labels[task_idx] > 0.3:
                    weight *= 2.0
            
            sample_weights[i] = weight
        
        # å½’ä¸€åŒ–æƒé‡
        sample_weights = sample_weights / sample_weights.mean()
        sample_weights = np.clip(sample_weights, 0.1, 10.0)
        
        return sample_weights
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        # è¯»å–æ•°æ®
        if isinstance(self.features, np.memmap):
            features = np.array(self.features[idx], dtype=np.float32)
            stat_features = np.array(self.stat_features[idx], dtype=np.float32)
            labels = np.array(self.labels[idx], dtype=np.float32)
        else:
            features = self.features[idx].astype(np.float32)
            stat_features = self.stat_features[idx].astype(np.float32)
            labels = self.labels[idx].astype(np.float32)
        
        # æ•°æ®å¢å¼º
        if self.augment:
            features = self._augment_data(features)
        
        # ç¡®ä¿æ•°æ®è¿ç»­
        features = np.ascontiguousarray(features, dtype=np.float32)
        stat_features = np.ascontiguousarray(stat_features, dtype=np.float32)
        labels = np.ascontiguousarray(labels, dtype=np.float32)
        
        return (torch.from_numpy(features).float(), 
                torch.from_numpy(stat_features).float(), 
                torch.from_numpy(labels).float())
    
    def _augment_data(self, features: np.ndarray) -> np.ndarray:
        """æ•°æ®å¢å¼º - é€‚åˆå›å½’ä»»åŠ¡"""
        features = features.astype(np.float32)
        
        # æ—¶é—´æ‰°åŠ¨
        if np.random.random() < 0.3:
            shift = np.random.randint(-2, 3)
            if shift != 0:
                features = np.roll(features, shift, axis=0)
        
        # è½»å¾®çš„ç‰¹å¾å™ªå£°
        if np.random.random() < 0.3:
            noise = np.random.normal(0, 0.01, features.shape)
            features = np.clip(features + noise, 0, 1)
        
        return features.astype(np.float32)
    
    def get_sampler(self):
        """è·å–åŠ æƒé‡‡æ ·å™¨"""
        if self.sample_weights is not None:
            return WeightedRandomSampler(
                weights=self.sample_weights,
                num_samples=len(self.sample_weights),
                replacement=True
            )
        return None

class ImprovedEarthquakeTrainer:
    """æ”¹è¿›çš„åœ°éœ‡é¢„æµ‹æ¨¡å‹è®­ç»ƒå™¨ - å›å½’ç‰ˆæœ¬"""
    
    def __init__(self,
                 model: EarthquakePredictionModel,
                 train_dataset: EarthquakeDataset,
                 val_dataset: EarthquakeDataset,
                 config: Dict[str, Any]):
        """åˆå§‹åŒ–è®­ç»ƒå™¨"""
        self.model = model
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # æ£€æŸ¥æ¨¡å‹è¾“å…¥å°ºå¯¸æ˜¯å¦ä¸æ•°æ®åŒ¹é…
        expected_height = train_dataset.grid_height
        expected_width = train_dataset.grid_width
        # å¯¹äºä¸è§„åˆ™ç½‘æ ¼ï¼ŒæœŸæœ›æ˜¯10Ã—8
        if expected_height != 10 or expected_width != 8:
            print(f"âš ï¸ è­¦å‘Š: æ•°æ®ç»´åº¦({expected_height}Ã—{expected_width})ä¸æ˜¯æœŸæœ›çš„10Ã—8")
            print("   è¯·ç¡®ä¿ä½¿ç”¨æ–°çš„ç½‘æ ¼ç³»ç»Ÿå¤„ç†æ•°æ®ï¼")
    
        model_height = model.input_height if hasattr(model, 'input_height') else None
        model_width = model.input_width if hasattr(model, 'input_width') else None
        
        if model_height != expected_height or model_width != expected_width:
            print(f"âš ï¸ è­¦å‘Š: æ¨¡å‹è¾“å…¥å°ºå¯¸({model_height}Ã—{model_width})ä¸"
                f"æ•°æ®å°ºå¯¸({expected_height}Ã—{expected_width})ä¸åŒ¹é…")
        
        # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
        self.model.to(self.device)
        
        # æ›¿æ¢æ¨¡å‹çš„æŸå¤±å‡½æ•°ä¸ºå›å½’ç‰ˆæœ¬ - ä¿®å¤å‚æ•°
        self.model.loss_function = EarthquakeRegressionLoss(
            loss_type=config.get('loss_type', 'huber'),
            consistency_weight=config.get('consistency_weight', 0.1),
            ignore_tasks=config.get('ignore_tasks', [])
        ).to(self.device)
        
        # åˆå§‹åŒ–æ··åˆç²¾åº¦è®­ç»ƒ
        self.use_amp = torch.cuda.is_available() and config.get('use_amp', True)
        self.scaler = GradScaler() if self.use_amp else None
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self._create_data_loaders()
        
        # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        self._create_optimizer_scheduler()
        
        # åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€
        self.epoch = 0
        self.best_score = float('inf')  # å¯¹äºå›å½’ï¼Œè¶Šå°è¶Šå¥½
        self.train_losses = []
        self.val_losses = []
        self.val_scores = []
        self.val_metrics_history = []  # ä¿å­˜è¯¦ç»†æŒ‡æ ‡å†å²
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.save_dir = config.get('save_dir', 'checkpoints')
        os.makedirs(self.save_dir, exist_ok=True)
        
        # æœ‰æ•ˆä»»åŠ¡åˆ—è¡¨ï¼ˆä¸å†æ’é™¤ä»»ä½•ä»»åŠ¡ï¼‰
        self.valid_tasks = config.get('valid_tasks', list(range(12)))
        
        print(f"ğŸš€ å›å½’è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ:")
        print(f"  è®¾å¤‡: {self.device}")
        print(f"  è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
        print(f"  éªŒè¯æ ·æœ¬: {len(val_dataset)}")
        print(f"  æ‰¹æ¬¡å¤§å°: {config['batch_size']}")
        print(f"  æŸå¤±ç±»å‹: {config.get('loss_type', 'huber')}")
        print(f"  æœ‰æ•ˆä»»åŠ¡: {self.valid_tasks}")
        print(f"  å­¦ä¹ ç‡: {config['learning_rate']}")
        print(f"  æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {config.get('gradient_accumulation_steps', 1)}")
        print(f"  æ··åˆç²¾åº¦è®­ç»ƒ: {self.use_amp}")
        
        if torch.cuda.is_available():
            GPUMemoryMonitor.log_memory_usage("åˆå§‹åŒ–å®Œæˆ", self.device)
    
    def _create_data_loaders(self):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        import platform
        is_windows = platform.system() == 'Windows'
        
        if is_windows:
            num_workers = 0
            print("  âš ï¸ Windowsç³»ç»Ÿæ£€æµ‹åˆ°ï¼Œä½¿ç”¨å•çº¿ç¨‹æ•°æ®åŠ è½½")
        else:
            num_workers = self.config.get('num_workers', 2)
        
        persistent_workers = True if num_workers > 0 else False
        pin_memory = torch.cuda.is_available() and self.config.get('pin_memory', True)
        
        # è®­ç»ƒæ•°æ®åŠ è½½å™¨
        train_sampler = self.train_dataset.get_sampler()
        self.train_loader = DataLoader(
            self.train_dataset,
            batch_size=self.config['batch_size'],
            sampler=train_sampler,
            shuffle=(train_sampler is None),
            num_workers=num_workers,
            pin_memory=pin_memory,
            drop_last=True,
            persistent_workers=persistent_workers,
            prefetch_factor=2 if num_workers > 0 else None
        )
        
        # éªŒè¯æ•°æ®åŠ è½½å™¨
        self.val_loader = DataLoader(
            self.val_dataset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            num_workers=num_workers,
            pin_memory=pin_memory,
            persistent_workers=persistent_workers,
            prefetch_factor=2 if num_workers > 0 else None
        )
    
    def _create_optimizer_scheduler(self):
        """åˆ›å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        base_lr = self.config['learning_rate']
        
        # åˆ†å±‚å­¦ä¹ ç‡
        param_groups = []
        
        # æ£€æŸ¥æ¨¡å‹çš„å„ä¸ªç»„ä»¶æ˜¯å¦å­˜åœ¨
        if hasattr(self.model, 'spatial_encoder'):
            param_groups.append({
                'params': self.model.spatial_encoder.parameters(),
                'lr': base_lr * 1.0,
                'name': 'spatial_encoder'
            })
        
        if hasattr(self.model, 'temporal_encoder'):
            param_groups.append({
                'params': self.model.temporal_encoder.parameters(),
                'lr': base_lr * 0.8,
                'name': 'temporal_encoder'
            })
        
        if hasattr(self.model, 'attention_fusion'):
            param_groups.append({
                'params': self.model.attention_fusion.parameters(),
                'lr': base_lr * 0.6,
                'name': 'attention_fusion'
            })
        
        if hasattr(self.model, 'multi_task_head'):
            param_groups.append({
                'params': self.model.multi_task_head.parameters(),
                'lr': base_lr * 1.2,
                'name': 'multi_task_head'
            })
        
        if hasattr(self.model, 'task_head'):
            param_groups.append({
                'params': self.model.task_head.parameters(),
                'lr': base_lr * 1.2,
                'name': 'task_head'
            })
        
        # æŸå¤±å‡½æ•°å‚æ•°
        if hasattr(self.model.loss_function, 'parameters'):
            param_groups.append({
                'params': self.model.loss_function.parameters(),
                'lr': base_lr * 0.5,
                'name': 'loss_function'
            })
        
        # è¿‡æ»¤ç©ºå‚æ•°ç»„
        param_groups = [g for g in param_groups if len(list(g['params'])) > 0]
        
        # å¦‚æœæ²¡æœ‰åˆ†ç»„ï¼Œä½¿ç”¨æ‰€æœ‰å‚æ•°
        if not param_groups:
            param_groups = self.model.parameters()
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        if self.config['optimizer'] == 'adamw':
            self.optimizer = optim.AdamW(
                param_groups,
                weight_decay=self.config.get('weight_decay', 1e-4),
                betas=(0.9, 0.999),
                eps=1e-8
            )
        elif self.config['optimizer'] == 'adam':
            self.optimizer = optim.Adam(
                param_groups,
                weight_decay=self.config.get('weight_decay', 1e-4)
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {self.config['optimizer']}")
        
        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.config['scheduler'] == 'cosine':
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config['epochs'],
                eta_min=base_lr * 0.01
            )
        elif self.config['scheduler'] == 'plateau':
            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='min',  # å¯¹äºå›å½’ä»»åŠ¡ï¼Œæœ€å°åŒ–æŸå¤±
                factor=0.5,
                patience=self.config.get('scheduler_patience', 5),
                verbose=True
            )
        elif self.config['scheduler'] == 'warmup_cosine':
            self.scheduler = self._create_warmup_cosine_scheduler()
        elif self.config['scheduler'] == 'onecycle':
            self.scheduler = optim.lr_scheduler.OneCycleLR(
                self.optimizer,
                max_lr=base_lr * 10,  # å³°å€¼å­¦ä¹ ç‡
                epochs=self.config['epochs'],
                steps_per_epoch=len(self.train_loader),
                pct_start=0.3,  # 30%ç”¨äºä¸Šå‡
                anneal_strategy='cos'
            )
        else:
            self.scheduler = None
    
    def _create_warmup_cosine_scheduler(self):
        """åˆ›å»ºé¢„çƒ­+ä½™å¼¦é€€ç«è°ƒåº¦å™¨"""
        warmup_epochs = self.config.get('warmup_epochs', 5)
        total_epochs = self.config['epochs']
        
        def lr_lambda(epoch):
            if epoch < warmup_epochs:
                return epoch / warmup_epochs
            else:
                progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)
                return 0.5 * (1 + np.cos(np.pi * progress))
        
        return optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)
    
    def train_epoch(self) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        total_loss = 0.0
        total_mae = 0.0
        total_rmse = 0.0
        num_batches = 0
        
        accumulation_steps = self.config.get('gradient_accumulation_steps', 1)
        empty_cache_freq = self.config.get('empty_cache_freq', 5)
        
        epoch_start_time = time.time()
        batch_times = []
        
        print(f"  å¼€å§‹è®­ç»ƒï¼Œå…± {len(self.train_loader)} ä¸ªæ‰¹æ¬¡...")
        
        self.optimizer.zero_grad(set_to_none=True)
        
        for batch_idx, (features, stat_features, labels) in enumerate(self.train_loader):
            batch_start_time = time.time()
            
            if batch_idx % empty_cache_freq == 0 and torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            features = features.to(self.device, non_blocking=True).float()
            stat_features = stat_features.to(self.device, non_blocking=True).float()
            labels = labels.to(self.device, non_blocking=True).float()
            
            # å‰å‘ä¼ æ’­
            with autocast(enabled=self.use_amp):
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒç»Ÿè®¡ç‰¹å¾
                if hasattr(self.model, 'forward_with_stat_features'):
                    predictions = self.model.forward_with_stat_features(features, stat_features)
                else:
                    predictions = self.model(features)
            
            # è®¡ç®—æŸå¤±
            with autocast(enabled=False):
                loss_info = self._compute_loss_safe(predictions, labels)
                total_loss_batch = loss_info['total_loss'] / accumulation_steps
            
            # åå‘ä¼ æ’­
            if self.scaler:
                self.scaler.scale(total_loss_batch).backward()
            else:
                total_loss_batch.backward()
            
            # æ¢¯åº¦ç´¯ç§¯å’Œä¼˜åŒ–
            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(self.train_loader):
                if self.scaler:
                    self.scaler.unscale_(self.optimizer)
                    grad_norm = torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), 
                        self.config['grad_clip']
                    )
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    # è¿™é‡Œæ˜¯æ¢¯åº¦è£å‰ªå‘ç”Ÿçš„åœ°æ–¹
                    grad_norm = torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), 
                        self.config['grad_clip']
                    )
                    self.optimizer.step()
                
                self.optimizer.zero_grad(set_to_none=True)
                
                # å¦‚æœä½¿ç”¨OneCycleè°ƒåº¦å™¨ï¼Œæ¯æ­¥æ›´æ–°
                if isinstance(self.scheduler, optim.lr_scheduler.OneCycleLR):
                    self.scheduler.step()
            else:
                grad_norm = 0.0
            
            # ç´¯ç§¯æŸå¤±å’ŒæŒ‡æ ‡
            total_loss += total_loss_batch.item() * accumulation_steps
            total_mae += loss_info.get('mae', 0.0)
            total_rmse += loss_info.get('rmse', 0.0)
            num_batches += 1
            
            batch_time = time.time() - batch_start_time
            batch_times.append(batch_time)
            
            # è¿›åº¦æ˜¾ç¤º
            if (batch_idx + 1) % 10 == 0 or batch_idx == 0 or batch_idx == len(self.train_loader) - 1:
                avg_batch_time = np.mean(batch_times)
                eta_minutes = avg_batch_time * (len(self.train_loader) - batch_idx - 1) / 60
                
                print(f"  Batch {batch_idx+1}/{len(self.train_loader)} "
                    f"| Loss: {total_loss_batch.item() * accumulation_steps:.4f} "
                    f"| MAE: {loss_info.get('mae', 0.0):.4f} "
                    f"| æ‰¹æ¬¡è€—æ—¶: {batch_time:.2f}s "
                    f"| ETA: {eta_minutes:.1f}åˆ†é’Ÿ")
        
        # å¹³å‡æŸå¤±
        avg_loss = total_loss / num_batches
        avg_mae = total_mae / num_batches
        avg_rmse = total_rmse / num_batches
        
        epoch_time = time.time() - epoch_start_time
        print(f"\n  è®­ç»ƒé˜¶æ®µå®Œæˆ: è€—æ—¶ {epoch_time:.1f}ç§’")
        
        return {
            'total_loss': avg_loss,
            'mae': avg_mae,
            'rmse': avg_rmse
        }
    
    def validate_epoch(self) -> Dict[str, float]:
        """éªŒè¯ä¸€ä¸ªepoch - å›å½’è¯„ä¼°æŒ‡æ ‡"""
        self.model.eval()
        
        total_loss = 0.0
        all_predictions = []
        all_labels = []
        num_batches = 0
        
        with torch.no_grad():
            for features, stat_features, labels in self.val_loader:
                features = features.to(self.device, non_blocking=True).float()
                stat_features = stat_features.to(self.device, non_blocking=True).float()
                labels = labels.to(self.device, non_blocking=True).float()
                
                with autocast(enabled=self.use_amp):
                    if hasattr(self.model, 'forward_with_stat_features'):
                        predictions = self.model.forward_with_stat_features(features, stat_features)
                    else:
                        predictions = self.model(features)
                
                with autocast(enabled=False):
                    loss_info = self._compute_loss_safe(predictions, labels)
                
                total_loss += loss_info['total_loss'].item()
                num_batches += 1
                
                all_predictions.append(predictions.cpu().numpy())
                all_labels.append(labels.cpu().numpy())
        
        all_predictions = np.concatenate(all_predictions, axis=0)
        all_labels = np.concatenate(all_labels, axis=0)
        
        # è®¡ç®—å›å½’è¯„ä¼°æŒ‡æ ‡
        metrics = self._calculate_regression_metrics(all_predictions, all_labels)
        metrics['total_loss'] = total_loss / num_batches
        
        return metrics
    
    def _compute_loss_safe(self, predictions, labels):
        """å®‰å…¨åœ°è®¡ç®—æŸå¤±ï¼Œå¤„ç†æ··åˆç²¾åº¦å…¼å®¹æ€§é—®é¢˜"""
        # ç¡®ä¿float32ç±»å‹
        if predictions.dtype == torch.float16:
            predictions = predictions.float()
        if labels.dtype == torch.float16:
            labels = labels.float()
        
        # è®¡ç®—æŸå¤±
        loss_dict = self.model.compute_loss(predictions, labels)
        
        return loss_dict
    
    def _calculate_regression_metrics(self, predictions: np.ndarray, labels: np.ndarray) -> Dict[str, float]:
        """è®¡ç®—å›å½’è¯„ä¼°æŒ‡æ ‡"""
        metrics = {}
        
        # ç¡®ä¿é¢„æµ‹å€¼åœ¨åˆç†èŒƒå›´å†…
        predictions = np.clip(predictions, 0.0, 1.0)
        
        # æ•´ä½“æŒ‡æ ‡
        mae = np.mean(np.abs(predictions - labels))
        mse = np.mean((predictions - labels) ** 2)
        rmse = np.sqrt(mse)
        
        metrics['mae'] = mae
        metrics['mse'] = mse
        metrics['rmse'] = rmse
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        if np.std(predictions) > 0 and np.std(labels) > 0:
            correlation = np.corrcoef(predictions.flatten(), labels.flatten())[0, 1]
        else:
            correlation = 0.0
        metrics['correlation'] = correlation
        
        # è®¡ç®—æ¯ä¸ªä»»åŠ¡çš„æŒ‡æ ‡
        task_metrics = {}
        for i, task_idx in enumerate(self.valid_tasks):
            task_preds = predictions[:, task_idx]
            task_labels = labels[:, task_idx]
            
            # ä»»åŠ¡çº§åˆ«çš„æŒ‡æ ‡
            task_mae = np.mean(np.abs(task_preds - task_labels))
            task_rmse = np.sqrt(np.mean((task_preds - task_labels) ** 2))
            
            # ç›¸å…³ç³»æ•°
            if np.std(task_preds) > 0 and np.std(task_labels) > 0:
                task_corr = np.corrcoef(task_preds, task_labels)[0, 1]
            else:
                task_corr = 0.0
            
            # é«˜æ¦‚ç‡äº‹ä»¶çš„å‡†ç¡®æ€§
            high_prob_mask = task_labels > 0.3
            if np.any(high_prob_mask):
                high_prob_mae = np.mean(np.abs(task_preds[high_prob_mask] - task_labels[high_prob_mask]))
            else:
                high_prob_mae = 0.0
            
            task_metrics[f'task_{task_idx}'] = {
                'mae': task_mae,
                'rmse': task_rmse,
                'correlation': task_corr,
                'high_prob_mae': high_prob_mae,
                'mean_pred': np.mean(task_preds),
                'mean_label': np.mean(task_labels),
                'std_pred': np.std(task_preds),
                'std_label': np.std(task_labels)
            }
        
        # æ±‡æ€»æŒ‡æ ‡
        all_maes = [tm['mae'] for tm in task_metrics.values()]
        all_correlations = [tm['correlation'] for tm in task_metrics.values()]
        
        metrics['mean_task_mae'] = np.mean(all_maes)
        metrics['mean_correlation'] = np.mean(all_correlations)
        
        # å¤§åœ°éœ‡æŒ‡æ ‡
        high_magnitude_indices = [3, 7, 11]  # M6.5+çš„ä»»åŠ¡
        high_mag_maes = []
        high_mag_corrs = []
        
        for task_idx in high_magnitude_indices:
            if task_idx in self.valid_tasks:
                task_key = f'task_{task_idx}'
                if task_key in task_metrics:
                    high_mag_maes.append(task_metrics[task_key]['mae'])
                    high_mag_corrs.append(task_metrics[task_key]['correlation'])
        
        metrics['high_magnitude_mae'] = np.mean(high_mag_maes) if high_mag_maes else 0.0
        metrics['high_magnitude_correlation'] = np.mean(high_mag_corrs) if high_mag_corrs else 0.0
        
        # ä¿å­˜ä»»åŠ¡çº§åˆ«çš„æŒ‡æ ‡
        metrics['task_metrics'] = task_metrics
        
        # æ‰“å°è¯¦ç»†æŒ‡æ ‡ï¼ˆæ¯10ä¸ªepochï¼‰
        if hasattr(self, 'epoch') and (self.epoch + 1) % 10 == 0:
            print("\n  ğŸ“Š ä»»åŠ¡çº§åˆ«è¯¦ç»†æŒ‡æ ‡:")
            task_names = [
                "7d_M3-4.5", "7d_M4.5-5.5", "7d_M5.5-6.5", "7d_M6.5+",
                "14d_M3-4.5", "14d_M4.5-5.5", "14d_M5.5-6.5", "14d_M6.5+",
                "30d_M3-4.5", "30d_M4.5-5.5", "30d_M5.5-6.5", "30d_M6.5+"
            ]
            
            for task_idx in self.valid_tasks:
                if f'task_{task_idx}' in task_metrics:
                    tm = task_metrics[f'task_{task_idx}']
                    print(f"    ä»»åŠ¡{task_idx:2d} ({task_names[task_idx]}): "
                          f"MAE={tm['mae']:.4f}, RMSE={tm['rmse']:.4f}, "
                          f"ç›¸å…³æ€§={tm['correlation']:.3f}")
        
        return metrics
    
    def _calculate_comprehensive_score(self, metrics: Dict[str, float]) -> float:
        """è®¡ç®—ç»¼åˆè¯„åˆ† - å›å½’ç‰ˆæœ¬ï¼ˆè¶Šå°è¶Šå¥½ï¼‰"""
        # åŸºäºMAEå’Œç›¸å…³ç³»æ•°çš„ç»¼åˆè¯„åˆ†
        mae_score = metrics.get('mae', 1.0)
        correlation_score = 1.0 - metrics.get('mean_correlation', 0.0)  # è½¬æ¢ä¸ºè¶Šå°è¶Šå¥½
        high_mag_score = metrics.get('high_magnitude_mae', 1.0)
        
        # åŠ æƒç»„åˆ
        weights = {
            'mae': 0.4,
            'correlation': 0.3,
            'high_magnitude': 0.3
        }
        
        score = (mae_score * weights['mae'] + 
                 correlation_score * weights['correlation'] + 
                 high_mag_score * weights['high_magnitude'])
        
        return score
    
    def _should_early_stop(self) -> bool:
        """æ—©åœåˆ¤æ–­ - å›å½’ç‰ˆæœ¬"""
        patience = self.config.get('early_stop_patience', 20)
        
        if len(self.val_scores) < patience:
            return False
        
        # æ£€æŸ¥æœ€è¿‘çš„åˆ†æ•°æ˜¯å¦æœ‰æ”¹å–„ï¼ˆå¯¹äºå›å½’ï¼Œåˆ†æ•°è¶Šå°è¶Šå¥½ï¼‰
        recent_scores = self.val_scores[-patience:]
        best_recent = min(recent_scores)
        
        # å¦‚æœæœ€è¿‘patienceä¸ªepochéƒ½æ²¡æœ‰æ”¹å–„ï¼Œåˆ™æ—©åœ
        if best_recent >= self.best_score:
            return True
        
        return False
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ - æ€»è®¡ {self.config['epochs']} è½®")
        print("="*80)
        
        if torch.cuda.is_available():
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
            GPUMemoryMonitor.log_memory_usage("è®­ç»ƒå¼€å§‹å‰", self.device)
        
        start_time = time.time()
        
        for epoch in range(self.config['epochs']):
            self.epoch = epoch
            epoch_start_time = time.time()
            
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            print(f"\nEpoch {epoch+1}/{self.config['epochs']}")
            print("-" * 50)
            
            # è®­ç»ƒ
            train_metrics = self.train_epoch()
            self.train_losses.append(train_metrics['total_loss'])
            
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # éªŒè¯
            val_metrics = self.validate_epoch()
            self.val_losses.append(val_metrics['total_loss'])
            self.val_metrics_history.append(val_metrics)
            
            # è®¡ç®—ç»¼åˆè¯„åˆ†ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
            comprehensive_score = self._calculate_comprehensive_score(val_metrics)
            self.val_scores.append(comprehensive_score)
            
            # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆé™¤äº†OneCycleï¼‰
            if self.scheduler and not isinstance(self.scheduler, optim.lr_scheduler.OneCycleLR):
                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_metrics['mae'])  # ä½¿ç”¨MAEä½œä¸ºç›‘æ§æŒ‡æ ‡
                else:
                    self.scheduler.step()
            
            # æ‰“å°ç»“æœ
            epoch_time = time.time() - epoch_start_time
            current_lr = self.optimizer.param_groups[0]['lr']
            
            print(f"\nğŸ“Š Epoch {epoch+1} ç»“æœ:")
            print(f"  è®­ç»ƒæŸå¤±: {train_metrics['total_loss']:.4f}")
            print(f"  è®­ç»ƒMAE: {train_metrics['mae']:.4f}")
            print(f"  è®­ç»ƒRMSE: {train_metrics['rmse']:.4f}")
            print(f"  éªŒè¯æŸå¤±: {val_metrics['total_loss']:.4f}")
            print(f"  éªŒè¯æŒ‡æ ‡:")
            print(f"    MAE: {val_metrics['mae']:.4f}")
            print(f"    RMSE: {val_metrics['rmse']:.4f}")
            print(f"    ç›¸å…³ç³»æ•°: {val_metrics['correlation']:.4f}")
            print(f"    å¤§åœ°éœ‡MAE: {val_metrics['high_magnitude_mae']:.4f}")
            print(f"  ç»¼åˆè¯„åˆ†: {comprehensive_score:.4f} (è¶Šå°è¶Šå¥½)")
            print(f"  å­¦ä¹ ç‡: {current_lr:.6f}")
            print(f"  è€—æ—¶: {epoch_time:.1f}ç§’")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåˆ†æ•°è¶Šå°è¶Šå¥½ï¼‰
            is_best = comprehensive_score < self.best_score
            if is_best:
                self.best_score = comprehensive_score
                self._save_best_model(val_metrics)
                print(f"  ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹! è¯„åˆ†: {comprehensive_score:.4f}")
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % self.config.get('save_interval', 10) == 0:
                self._save_checkpoint(val_metrics)
            
            # æ—©åœæ£€æŸ¥
            if self._should_early_stop():
                print(f"â¹ï¸  æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch+1} è½®åœæ­¢è®­ç»ƒ")
                break
        
        total_time = time.time() - start_time
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ! æ€»è€—æ—¶: {total_time/3600:.2f}å°æ—¶")
        print(f"   æœ€ä½³è¯„åˆ†: {self.best_score:.4f}")
        
        if torch.cuda.is_available():
            GPUMemoryMonitor.log_memory_usage("è®­ç»ƒå®Œæˆ", self.device)
        
        self._generate_training_report()
    
    def _save_best_model(self, metrics: Dict[str, float]):
        """ä¿å­˜æœ€ä½³æ¨¡å‹"""
        save_path = os.path.join(self.save_dir, 'best_model.pth')
        
        save_checkpoint(
            model=self.model,
            optimizer=self.optimizer,
            scheduler=self.scheduler,
            epoch=self.epoch,
            loss=metrics['total_loss'],
            metrics=metrics,
            save_path=save_path,
            is_best=True
        )

        # ä¿å­˜è¯¦ç»†çš„ä»»åŠ¡æŒ‡æ ‡
        metrics_path = os.path.join(self.save_dir, 'best_model_metrics.json')
        
        # è½¬æ¢æ‰€æœ‰numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
        def convert_to_native(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {k: convert_to_native(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_to_native(v) for v in obj]
            else:
                return obj
        
        metrics_to_save = {
            'epoch': int(self.epoch),
            'comprehensive_score': float(self.best_score),
            'overall_metrics': {k: convert_to_native(v) for k, v in metrics.items() if k != 'task_metrics'},
            'task_metrics': convert_to_native(metrics.get('task_metrics', {}))
        }
        
        with open(metrics_path, 'w') as f:
            json.dump(metrics_to_save, f, indent=2)
    
    def _save_checkpoint(self, metrics: Dict[str, float]):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        save_path = os.path.join(self.save_dir, f'checkpoint_epoch_{self.epoch+1}.pth')
        
        save_checkpoint(
            model=self.model,
            optimizer=self.optimizer,
            scheduler=self.scheduler,
            epoch=self.epoch,
            loss=metrics['total_loss'],
            metrics=metrics,
            save_path=save_path,
            is_best=False
        )
    
    def _generate_training_report(self):
        """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š"""
        report_dir = os.path.join(self.save_dir, 'training_report')
        os.makedirs(report_dir, exist_ok=True)
        
        # ç»˜åˆ¶æŸå¤±æ›²çº¿
        plt.figure(figsize=(15, 5))
        
        plt.subplot(1, 3, 1)
        plt.plot(self.train_losses, label='è®­ç»ƒæŸå¤±')
        plt.plot(self.val_losses, label='éªŒè¯æŸå¤±')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.title('æŸå¤±æ›²çº¿')
        plt.grid(True)
        
        plt.subplot(1, 3, 2)
        plt.plot(self.val_scores, label='ç»¼åˆè¯„åˆ†')
        plt.xlabel('Epoch')
        plt.ylabel('Score (lower is better)')
        plt.legend()
        plt.title('éªŒè¯è¯„åˆ†')
        plt.grid(True)
        
        # ç»˜åˆ¶MAEå’Œç›¸å…³ç³»æ•°æ›²çº¿
        plt.subplot(1, 3, 3)
        maes = [m.get('mae', 0) for m in self.val_metrics_history]
        correlations = [m.get('correlation', 0) for m in self.val_metrics_history]
        
        ax1 = plt.gca()
        ax2 = ax1.twinx()
        
        ax1.plot(maes, 'b-', label='MAE')
        ax2.plot(correlations, 'r-', label='Correlation')
        
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('MAE', color='b')
        ax2.set_ylabel('Correlation', color='r')
        
        ax1.tick_params(axis='y', labelcolor='b')
        ax2.tick_params(axis='y', labelcolor='r')
        
        plt.title('å›å½’æŒ‡æ ‡')
        
        plt.tight_layout()
        plt.savefig(os.path.join(report_dir, 'training_curves.png'), dpi=300)
        plt.close()
        
        # ä¿å­˜è®­ç»ƒå†å²
        history = {
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'val_scores': self.val_scores,
            'val_metrics_history': self.val_metrics_history,
            'best_score': self.best_score,
            'config': self.config
        }
        
        history_path = os.path.join(report_dir, 'training_history.json')
        with open(history_path, 'w') as f:
            json.dump(history, f, indent=2, default=str)
        
        print(f"ğŸ“ˆ è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_dir}")

def create_adaptive_model(data_shape: Tuple[int, ...]) -> EarthquakePredictionModel:
    """æ ¹æ®æ•°æ®å½¢çŠ¶åˆ›å»ºè‡ªé€‚åº”çš„æ¨¡å‹ - ä¿æŒä¸å˜"""
    _, time_steps, height, width, channels = data_shape
    
    print(f"ğŸ“ æ ¹æ®æ•°æ®å½¢çŠ¶åˆ›å»ºæ¨¡å‹:")
    print(f"  è¾“å…¥é€šé“: {channels}")
    print(f"  æ—¶é—´æ­¥: {time_steps}")
    print(f"  ç½‘æ ¼å°ºå¯¸: {height} Ã— {width}")

    # æ ¹æ®7Ã—8ç½‘æ ¼è°ƒæ•´æ¨¡å‹å®¹é‡
    if height == 7 and width == 8:  # æ˜ç¡®å¤„ç†7Ã—8æƒ…å†µ
        spatial_base_channels = 48
        temporal_hidden_channels = 96
        attention_fusion_dim = 384
    # æ ¹æ®ç½‘æ ¼å¤§å°è°ƒæ•´æ¨¡å‹å®¹é‡
    elif height <= 10 and width <= 10:
        # å°ç½‘æ ¼ï¼Œä½¿ç”¨è¾ƒå¤§çš„æ¨¡å‹
        spatial_base_channels = 48
        temporal_hidden_channels = 96
        attention_fusion_dim = 384
    elif height <= 20 and width <= 20:
        # ä¸­ç­‰ç½‘æ ¼
        spatial_base_channels = 32
        temporal_hidden_channels = 64
        attention_fusion_dim = 256
    else:
        # å¤§ç½‘æ ¼ï¼Œä½¿ç”¨è¾ƒå°çš„æ¨¡å‹
        spatial_base_channels = 24
        temporal_hidden_channels = 48
        attention_fusion_dim = 192
    
    return EarthquakePredictionModel(
        input_channels=channels,
        input_time_steps=time_steps,
        input_height=height,
        input_width=width,
        spatial_base_channels=spatial_base_channels,
        spatial_num_scales=3,
        temporal_hidden_channels=temporal_hidden_channels,
        attention_fusion_dim=attention_fusion_dim,
        shared_feature_dim=64,
        task_hidden_dim=32,
        use_post_processing=True
    )

def create_improved_training_config(mode: str = 'standard') -> Dict[str, Any]:
    """åˆ›å»ºæ”¹è¿›çš„è®­ç»ƒé…ç½®"""
    if mode == 'safe':
        return {
            'batch_size': 2,
            'gradient_accumulation_steps': 16,
            'learning_rate': 2e-4,  # æé«˜å­¦ä¹ ç‡
            'warmup_epochs': 3,
            'optimizer': 'adamw',
            'scheduler': 'cosine',  # æ”¹ä¸ºcosine
            'weight_decay': 1e-4,
            'grad_clip': 1.0,
            'use_amp': False,
            'epochs': 80,
            'early_stop_patience': 20,
            'scheduler_patience': 8,
            'num_workers': 0,
            'pin_memory': True if torch.cuda.is_available() else False,
            'empty_cache_freq': 3,
            'save_interval': 5,
            'save_dir': 'earthquake_model_improved_safe',
            'loss_type': 'bce',  # ä½¿ç”¨BCEæŸå¤±
            'consistency_weight': 0.05,
            'uncertainty_weighting': True,
            'ignore_tasks': [1],  # å¿½ç•¥ä»»åŠ¡1
            'description': 'æ”¹è¿›çš„å®‰å…¨æ¨¡å¼é…ç½®'
        }
    elif mode == 'standard':
        return {
            'batch_size': 4,
            'gradient_accumulation_steps': 8,
            'learning_rate': 5e-4,  # æé«˜å­¦ä¹ ç‡
            'warmup_epochs': 3,
            'optimizer': 'adamw',
            'scheduler': 'onecycle',  # ä½¿ç”¨OneCycle
            'weight_decay': 1e-4,
            'grad_clip': 1.0,
            'use_amp': False,
            'epochs': 100,
            'early_stop_patience': 25,
            'scheduler_patience': 10,
            'num_workers': 0,
            'pin_memory': True if torch.cuda.is_available() else False,
            'empty_cache_freq': 5,
            'save_interval': 5,
            'save_dir': 'earthquake_model_improved_standard',
            'loss_type': 'bce',  # ä½¿ç”¨BCEæŸå¤±
            'consistency_weight': 0.05,
            'uncertainty_weighting': True,
            'ignore_tasks': [1],  # å¿½ç•¥ä»»åŠ¡1
            'description': 'æ”¹è¿›çš„æ ‡å‡†æ¨¡å¼é…ç½®'
        }
    else:  # performance
        return {
            'batch_size': 6,
            'gradient_accumulation_steps': 6,
            'learning_rate': 1e-3,  # æ›´é«˜çš„å­¦ä¹ ç‡
            'warmup_epochs': 2,
            'optimizer': 'adamw',
            'scheduler': 'onecycle',  # ä½¿ç”¨OneCycle
            'weight_decay': 1e-4,
            'grad_clip': 1.0,
            'use_amp': True,  # å¯ç”¨æ··åˆç²¾åº¦
            'epochs': 100,
            'early_stop_patience': 30,
            'scheduler_patience': 10,
            'num_workers': 0,
            'pin_memory': True if torch.cuda.is_available() else False,
            'empty_cache_freq': 10,
            'save_interval': 5,
            'save_dir': 'earthquake_model_improved_performance',
            'loss_type': 'bce',  # ä½¿ç”¨BCEæŸå¤±
            'consistency_weight': 0.05,
            'uncertainty_weighting': True,
            'ignore_tasks': [1],  # å¿½ç•¥ä»»åŠ¡1
            'description': 'æ”¹è¿›çš„é«˜æ€§èƒ½æ¨¡å¼é…ç½®'
        }

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # GPUä¼˜åŒ–è®¾ç½®
    if torch.cuda.is_available():
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
        torch.backends.cudnn.enabled = True
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        torch.cuda.empty_cache()
        torch.cuda.set_per_process_memory_fraction(0.9)
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
    
    print("ğŸŒ åœ°éœ‡é¢„æµ‹æ¨¡å‹è®­ç»ƒç³»ç»Ÿ - æ”¹è¿›ç‰ˆ")
    print("="*60)
    
    # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    print("\nğŸ“Š ç³»ç»Ÿä¿¡æ¯:")
    print(f"  PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"  CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"  GPUè®¾å¤‡: {torch.cuda.get_device_name()}")
        print(f"  GPUæ•°é‡: {torch.cuda.device_count()}")
    
    # æ•°æ®è·¯å¾„
    data_dir = "../data/processed_grid"
    train_features_path = os.path.join(data_dir, "train_features.npy")
    train_stat_features_path = os.path.join(data_dir, "train_stat_features.npy")
    train_labels_path = os.path.join(data_dir, "train_labels.npy")
    train_timestamps_path = os.path.join(data_dir, "train_timestamps.npy")
    train_weighted_labels_path = os.path.join(data_dir, "train_weighted_labels.npy")
    
    val_features_path = os.path.join(data_dir, "val_features.npy")
    val_stat_features_path = os.path.join(data_dir, "val_stat_features.npy")
    val_labels_path = os.path.join(data_dir, "val_labels.npy")
    val_timestamps_path = os.path.join(data_dir, "val_timestamps.npy")
    val_weighted_labels_path = os.path.join(data_dir, "val_weighted_labels.npy")
    
    # æ£€æŸ¥æ–‡ä»¶
    required_files = [
        train_features_path, train_stat_features_path, train_labels_path, train_timestamps_path,
        val_features_path, val_stat_features_path, val_labels_path, val_timestamps_path
    ]
    
    missing_files = [f for f in required_files if not os.path.exists(f)]
    if missing_files:
        print("âŒ ç¼ºå°‘ä»¥ä¸‹æ•°æ®æ–‡ä»¶:")
        for f in missing_files:
            print(f"  - {f}")
        return
    
    # æ£€æŸ¥æ•°æ®å½¢çŠ¶
    print("\nğŸ“ æ£€æŸ¥æ•°æ®å½¢çŠ¶...")
    sample_features = np.load(train_features_path, mmap_mode='r')
    data_shape = sample_features.shape
    print(f"  æ•°æ®å½¢çŠ¶: {data_shape}")
    
    # åˆ›å»ºæ•°æ®é›†
    print("\nğŸ“‚ åŠ è½½æ•°æ®é›†...")
    import platform
    use_mmap = platform.system() == 'Windows'
    
    train_dataset = EarthquakeDataset(
        features_path=train_features_path,
        stat_features_path=train_stat_features_path,
        labels_path=train_labels_path,
        timestamps_path=train_timestamps_path,
        sample_weights_path=val_weighted_labels_path,
        augment=True,
        balance_sampling=True,
        use_mmap=use_mmap
    )
    
    val_dataset = EarthquakeDataset(
        features_path=val_features_path,
        stat_features_path=val_stat_features_path,
        labels_path=val_labels_path,
        timestamps_path=val_timestamps_path,
        sample_weights_path=train_weighted_labels_path,
        augment=False,
        balance_sampling=False,
        use_mmap=use_mmap
    )
    
    # åˆ›å»ºè‡ªé€‚åº”æ¨¡å‹
    print("\nğŸ—ï¸ åˆ›å»ºåœ°éœ‡é¢„æµ‹æ¨¡å‹...")
    model = create_adaptive_model(data_shape)
    
    # é€‰æ‹©è®­ç»ƒæ¨¡å¼
    if torch.cuda.is_available():
        print("\nè¯·é€‰æ‹©è®­ç»ƒæ¨¡å¼:")
        print("1. å®‰å…¨æ¨¡å¼ (æ‰¹æ¬¡2, æ¢¯åº¦ç´¯ç§¯16, å­¦ä¹ ç‡2e-4)")
        print("2. æ ‡å‡†æ¨¡å¼ (æ‰¹æ¬¡4, æ¢¯åº¦ç´¯ç§¯8, å­¦ä¹ ç‡5e-4)")
        print("3. é«˜æ€§èƒ½æ¨¡å¼ (æ‰¹æ¬¡6, æ¢¯åº¦ç´¯ç§¯6, å­¦ä¹ ç‡1e-3)")
        
        mode_input = input("è¯·è¾“å…¥é€‰æ‹© (1/2/3, é»˜è®¤ä¸º2): ").strip() or "2"
        
        mode_map = {'1': 'safe', '2': 'standard', '3': 'performance'}
        mode = mode_map.get(mode_input, 'standard')
    else:
        mode = 'safe'
    
    config = create_improved_training_config(mode)
    
    print(f"\nâš™ï¸ è®­ç»ƒé…ç½® ({mode}æ¨¡å¼):")
    for key, value in config.items():
        if key != 'description':
            print(f"  {key}: {value}")
    
    # æ˜¾ç¤ºæ¨¡å‹å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\nğŸ“Š æ¨¡å‹å‚æ•°é‡:")
    print(f"  æ€»å‚æ•°: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = ImprovedEarthquakeTrainer(
        model=model,
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        config=config
    )
    
    # å¼€å§‹è®­ç»ƒ
    try:
        trainer.train()
        print("\nğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆ!")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        trainer._save_checkpoint({'total_loss': float('inf')})
        
    except RuntimeError as e:
        if "out of memory" in str(e):
            print("\nâŒ GPUå†…å­˜ä¸è¶³ï¼")
            print("å»ºè®®: ä½¿ç”¨å®‰å…¨æ¨¡å¼é‡æ–°è¿è¡Œ")
        else:
            print(f"\nâŒ è¿è¡Œæ—¶é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

if __name__ == "__main__":
    main()