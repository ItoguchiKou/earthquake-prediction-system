"""
ç‹¬ç«‹çš„USGSæ•°æ®æŠ“å–è„šæœ¬
æŒ‰å¹´ä»½ä¿å­˜åŸå§‹æ•°æ®ï¼Œä»æœ€æ–°æ•°æ®å¼€å§‹å€’åºæŠ“å–
"""

import requests
import json
import csv
import io
from datetime import datetime, timedelta
import os
import time
import boto3
from botocore.exceptions import ClientError

class USGSDataFetcher:
    def __init__(self, s3_bucket=None, s3_prefix="earthquake-data/raw", local_mode=False):
        """
        åˆå§‹åŒ–æ•°æ®è·å–å™¨
        
        Args:
            s3_bucket: S3å­˜å‚¨æ¡¶åç§°ï¼ŒNoneè¡¨ç¤ºæœ¬åœ°æ¨¡å¼
            s3_prefix: S3å¯¹è±¡å‰ç¼€
            local_mode: æ˜¯å¦ä½¿ç”¨æœ¬åœ°æ¨¡å¼
        """
        self.local_mode = local_mode or s3_bucket is None
        self.s3_bucket = s3_bucket
        self.s3_prefix = s3_prefix
        
        if self.local_mode:
            self.data_dir = "../../../data"
            self.raw_dir = os.path.join(self.data_dir, "raw")
            os.makedirs(self.raw_dir, exist_ok=True)
        else:
            # åˆå§‹åŒ–S3å®¢æˆ·ç«¯
            self.s3_client = boto3.client('s3')
            print(f"S3æ¨¡å¼: å­˜å‚¨æ¡¶={s3_bucket}, å‰ç¼€={s3_prefix}")
        
        # æ—¥æœ¬åœ°ç†è¾¹ç•Œ
        self.japan_bounds = {
            'min_lat': 24.0,   # æœ€å—ç«¯ï¼ˆæ²–ç¸„ï¼‰
            'max_lat': 45.6,   # æœ€åŒ—ç«¯ï¼ˆåŒ—æµ·é“ï¼‰
            'min_lon': 122.9,  # æœ€è¥¿ç«¯ï¼ˆæ²–ç¸„ï¼‰
            'max_lon': 146.0   # æœ€ä¸œç«¯ï¼ˆåŒ—æµ·é“ï¼‰
        }
    
    def fetch_earthquake_data(self, start_date, end_date, min_magnitude=3.0):
        """
        ä»USGS APIè·å–åœ°éœ‡æ•°æ®
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            min_magnitude: æœ€å°éœ‡çº§
            
        Returns:
            åœ°éœ‡æ•°æ®åˆ—è¡¨
        """
        print(f"æ­£åœ¨è·å– {start_date} åˆ° {end_date} çš„åœ°éœ‡æ•°æ®...")
        
        params = {
            'format': 'geojson',
            'starttime': start_date,
            'endtime': end_date,
            'minmagnitude': min_magnitude,
            'minlatitude': self.japan_bounds['min_lat'],
            'maxlatitude': self.japan_bounds['max_lat'],
            'minlongitude': self.japan_bounds['min_lon'],
            'maxlongitude': self.japan_bounds['max_lon'],
            'limit': 20000  # USGS APIé™åˆ¶
        }
        
        try:
            response = requests.get(
                'https://earthquake.usgs.gov/fdsnws/event/1/query',
                params=params,
                timeout=60
            )
            
            if response.status_code == 200:
                data = response.json()
                earthquakes = []
                
                for feature in data['features']:
                    props = feature['properties']
                    coords = feature['geometry']['coordinates']
                    
                    # æ•°æ®éªŒè¯
                    if (props.get('mag') is not None and 
                        coords[0] is not None and 
                        coords[1] is not None):
                        
                        earthquake = {
                            'magnitude': props['mag'],
                            'longitude': coords[0],
                            'latitude': coords[1],
                            'depth': coords[2] if len(coords) > 2 else 0,
                            'time': datetime.fromtimestamp(props['time'] / 1000).isoformat(),
                            'place': props.get('place', ''),
                            'mag_type': props.get('magType', 'unknown'),
                            'id': props.get('ids', ''),
                            'url': props.get('url', '')
                        }
                        earthquakes.append(earthquake)
                
                print(f"è·å–åˆ° {len(earthquakes)} æ¡åœ°éœ‡è®°å½•")
                return earthquakes
                
            else:
                print(f"APIè¯·æ±‚å¤±è´¥: {response.status_code}")
                return []
                
        except Exception as e:
            print(f"æ•°æ®è·å–é”™è¯¯: {e}")
            return []
    
    def fetch_period_data(self, start_year, end_year, min_magnitude=3.0):
        """
        è·å–æŒ‡å®šå¹´ä»½èŒƒå›´çš„æ‰€æœ‰åœ°éœ‡æ•°æ®ï¼ˆä¸€æ¬¡æ€§è·å–5å¹´ï¼‰
        
        Args:
            start_year: å¼€å§‹å¹´ä»½
            end_year: ç»“æŸå¹´ä»½
            min_magnitude: æœ€å°éœ‡çº§
            
        Returns:
            è¯¥æ—¶é—´æ®µæ‰€æœ‰åœ°éœ‡æ•°æ®åˆ—è¡¨
        """
        print(f"\nğŸ—“ï¸  å¼€å§‹è·å– {start_year}-{end_year} å¹´æ•°æ®...")
        
        start_date = f"{start_year}-01-01"
        end_date = f"{end_year}-12-31"
        
        all_earthquakes = self.fetch_earthquake_data(start_date, end_date, min_magnitude)
        
        if all_earthquakes:
            # ä¿å­˜æ•°æ®
            filename = f"rawData_{start_year}-{end_year}.csv"
            
            if self.local_mode:
                # æœ¬åœ°ä¿å­˜
                filepath = os.path.join(self.raw_dir, filename)
                with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:
                    fieldnames = ['magnitude', 'longitude', 'latitude', 'depth', 'time', 
                                'place', 'mag_type', 'id', 'url']
                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(all_earthquakes)
                print(f"âœ… {start_year}-{end_year}å¹´æ•°æ®å·²ä¿å­˜åˆ°æœ¬åœ°: {filepath} ({len(all_earthquakes)} æ¡è®°å½•)")
            else:
                # S3ä¿å­˜
                if self.save_to_s3(all_earthquakes, filename):
                    print(f"âœ… {start_year}-{end_year}å¹´æ•°æ®å·²ä¿å­˜åˆ°S3: {filename} ({len(all_earthquakes)} æ¡è®°å½•)")
                else:
                    print(f"âŒ {start_year}-{end_year}å¹´æ•°æ®S3ä¿å­˜å¤±è´¥")
        else:
            print(f"âš ï¸  {start_year}-{end_year}å¹´æ— æ•°æ®")
        
        return all_earthquakes
    
    def fetch_all_historical_data(self, min_magnitude=3.0):
        """
        è·å–æ‰€æœ‰å†å²æ•°æ®ï¼ˆæŒ‰5å¹´æ—¶é—´æ®µä»æœ€æ–°å¼€å§‹å€’åºï¼‰
        """
        print("ğŸš€ å¼€å§‹è·å–USGSå†å²åœ°éœ‡æ•°æ®...")
        print(f"å‚æ•°: æœ€å°éœ‡çº§ {min_magnitude}")
        print(f"å­˜å‚¨æ¨¡å¼: {'æœ¬åœ°' if self.local_mode else 'S3'}")
        print("æ•°æ®èŒƒå›´: æ—¥æœ¬åœ°åŒº (çº¬åº¦ {:.1f}-{:.1f}, ç»åº¦ {:.1f}-{:.1f})".format(
            self.japan_bounds['min_lat'], self.japan_bounds['max_lat'],
            self.japan_bounds['min_lon'], self.japan_bounds['max_lon']
        ))
        
        current_year = datetime.now().year
        start_year = 1970
        
        total_records = 0
        successful_periods = []
        
        # æŒ‰5å¹´æ—¶é—´æ®µä»å½“å‰å¹´ä»½å¼€å§‹å€’åºè·å–
        for period_start in range(current_year, start_year - 1, -5):
            period_end = min(period_start + 4, current_year)
            
            try:
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                filename = f"rawData_{period_start}-{period_end}.csv"
                
                if self.local_mode:
                    # æœ¬åœ°æ–‡ä»¶æ£€æŸ¥
                    filepath = os.path.join(self.raw_dir, filename)
                    file_exists = os.path.exists(filepath)
                    if file_exists:
                        with open(filepath, 'r', encoding='utf-8') as f:
                            reader = csv.DictReader(f)
                            existing_data = list(reader)
                        record_count = len(existing_data)
                else:
                    # S3æ–‡ä»¶æ£€æŸ¥
                    file_exists = self.check_s3_file_exists(filename)
                    if file_exists:
                        existing_data = self.load_from_s3(filename)
                        record_count = len(existing_data) if existing_data else 0

                if file_exists and record_count > 0:
                    print(f"ğŸ“ {period_start}-{period_end}å¹´æ•°æ®å·²å­˜åœ¨: {record_count} æ¡è®°å½•")
                    total_records += record_count
                    successful_periods.append((period_start, period_end))
                    continue
                
                # è·å–æ–°æ•°æ®
                period_data = self.fetch_period_data(period_start, period_end, min_magnitude)

                if period_data:  # ä¿®æ”¹ï¼šæ£€æŸ¥åˆ—è¡¨æ˜¯å¦ä¸ºç©ºï¼Œè€Œä¸æ˜¯ .empty
                    total_records += len(period_data)
                    successful_periods.append((period_start, period_end))

                # æ—¶é—´æ®µé—´éš”ï¼ˆé¿å…APIé™åˆ¶ï¼‰
                time.sleep(5)
                
            except Exception as e:
                print(f"âŒ {period_start}-{period_end}å¹´æ•°æ®è·å–å¤±è´¥: {e}")
                continue
        
        print("\n" + "="*50)
        print("ğŸ“Š æ•°æ®è·å–å®Œæˆ!")
        print("="*50)
        print(f"æˆåŠŸè·å–æ—¶é—´æ®µ: {len(successful_periods)} ä¸ª")
        if successful_periods:
            all_years = []
            for start, end in successful_periods:
                all_years.extend(range(start, end + 1))
            print(f"å¹´ä»½èŒƒå›´: {min(all_years)} - {max(all_years)}")
        print(f"æ€»è®°å½•æ•°: {total_records:,} æ¡")
        storage_location = self.raw_dir if self.local_mode else f"s3://{self.s3_bucket}/{self.s3_prefix}"
        print(f"æ•°æ®ä¿å­˜ä½ç½®: {storage_location}")
    
        return successful_periods, total_records  
    
    def save_to_s3(self, data_list, filename):
        """
        ä¿å­˜æ•°æ®åˆ—è¡¨åˆ°S3ä¸ºCSVæ ¼å¼
        
        Args:
            data_list: åœ°éœ‡æ•°æ®åˆ—è¡¨
            filename: æ–‡ä»¶å
            
        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            if not data_list:
                print(f"âš ï¸ æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜: {filename}")
                return False
            
            # åˆ›å»ºCSVå­—ç¬¦ä¸²
            csv_buffer = io.StringIO()
            
            # å†™å…¥CSVå¤´éƒ¨
            fieldnames = ['magnitude', 'longitude', 'latitude', 'depth', 'time', 
                        'place', 'mag_type', 'id', 'url']
            writer = csv.DictWriter(csv_buffer, fieldnames=fieldnames)
            writer.writeheader()
            
            # å†™å…¥æ•°æ®
            for earthquake in data_list:
                writer.writerow(earthquake)
            
            csv_string = csv_buffer.getvalue()
            
            # æ„å»ºS3å¯¹è±¡é”®
            s3_key = f"{self.s3_prefix}/{filename}"
            
            # ä¸Šä¼ åˆ°S3
            self.s3_client.put_object(
                Bucket=self.s3_bucket,
                Key=s3_key,
                Body=csv_string.encode('utf-8'),
                ContentType='text/csv'
            )
            
            print(f"âœ… æ–‡ä»¶å·²ä¿å­˜åˆ°S3: s3://{self.s3_bucket}/{s3_key}")
            return True
            
        except ClientError as e:
            print(f"âŒ S3ä¿å­˜å¤±è´¥: {e}")
            return False

    def load_from_s3(self, filename):
        """
        ä»S3åŠ è½½CSVæ–‡ä»¶
        
        Args:
            filename: æ–‡ä»¶å
            
        Returns:
            æ•°æ®åˆ—è¡¨æˆ–None
        """
        try:
            s3_key = f"{self.s3_prefix}/{filename}"
            response = self.s3_client.get_object(Bucket=self.s3_bucket, Key=s3_key)
            csv_string = response['Body'].read().decode('utf-8')
            
            # è§£æCSV
            csv_reader = csv.DictReader(io.StringIO(csv_string))
            data_list = []
            for row in csv_reader:
                # è½¬æ¢æ•°å€¼ç±»å‹
                try:
                    row['magnitude'] = float(row['magnitude'])
                    row['longitude'] = float(row['longitude'])
                    row['latitude'] = float(row['latitude'])
                    row['depth'] = float(row['depth'])
                except (ValueError, TypeError):
                    continue
                data_list.append(row)
            
            return data_list
            
        except ClientError as e:
            print(f"âŒ ä»S3è¯»å–å¤±è´¥: {e}")
            return None

    def check_s3_file_exists(self, filename):
        """
        æ£€æŸ¥S3ä¸­æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        
        Args:
            filename: æ–‡ä»¶å
            
        Returns:
            bool: æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        """
        try:
            s3_key = f"{self.s3_prefix}/{filename}"
            self.s3_client.head_object(Bucket=self.s3_bucket, Key=s3_key)
            return True
        except ClientError:
            return False

def main(mode):
    """ä¸»å‡½æ•°"""
    print("USGSæ—¥æœ¬åœ°éœ‡æ•°æ®æŠ“å–ç³»ç»Ÿ")
    print("æ”¯æŒæœ¬åœ°æ¨¡å¼å’ŒS3æ¨¡å¼")
    
    # é€‰æ‹©è¿è¡Œæ¨¡å¼ 1: æœ¬åœ°æ¨¡å¼, 2: S3æ¨¡å¼
    if mode == 1:
        # æœ¬åœ°æ¨¡å¼
        print("ä½¿ç”¨æœ¬åœ°æ¨¡å¼...")
        fetcher = USGSDataFetcher(local_mode=True)
    elif mode == 2:
        # S3æ¨¡å¼
        s3_bucket = "earthquake-prediction-shuhao"
        if not s3_bucket:
            print("âŒ å¿…é¡»æä¾›S3å­˜å‚¨æ¡¶åç§°")
            return
        
        s3_prefix = input("è¾“å…¥S3å‰ç¼€ (é»˜è®¤: earthquake-data/raw): ").strip()
        if not s3_prefix:
            s3_prefix = "earthquake-data/raw"
        
        print(f"ä½¿ç”¨S3æ¨¡å¼: {s3_bucket}/{s3_prefix}")
        fetcher = USGSDataFetcher(s3_bucket=s3_bucket, s3_prefix=s3_prefix)
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")
        return
    
    min_magnitude = 3.0  # æœ€å°éœ‡çº§
    
    # å¼€å§‹æŠ“å–
    successful_periods, total_records = fetcher.fetch_all_historical_data(min_magnitude)
    
    print(f"\nğŸ‰ æ•°æ®æŠ“å–å®Œæˆ! å…±è·å– {total_records:,} æ¡è®°å½•")

if __name__ == "__main__":
    main(1)