"""
æ—¥æœ¬åœ°éœ‡æ•°æ®å¤„ç†ä¸»è„šæœ¬
ä»USGS APIè·å–æ•°æ®å¹¶æŒ‰éƒ½é“åºœå¿åˆ†ç±»ä¿å­˜
"""

import pandas as pd
import os
import glob
from japan_regions import get_prefecture_by_coordinates, get_all_prefecture_names
from data_features import create_earthquake_features, prepare_training_data

class JapanEarthquakeProcessor:
    def __init__(self, data_dir="../../../data"):
        self.data_dir = data_dir
        self.raw_dir = os.path.join(data_dir, "raw")
        self.processed_dir = os.path.join(data_dir, "processed")
        
        # åˆ›å»ºç›®å½•
        os.makedirs(self.processed_dir, exist_ok=True)
    
    def load_raw_data(self, years=None, min_magnitude=3.0):
        """
        ä»æœ¬åœ°æ–‡ä»¶åŠ è½½åŸå§‹åœ°éœ‡æ•°æ®
        
        Args:
            years: æŒ‡å®šå¹´ä»½åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºåŠ è½½æ‰€æœ‰å¹´ä»½
            min_magnitude: æœ€å°éœ‡çº§è¿‡æ»¤
            
        Returns:
            åˆå¹¶åçš„åœ°éœ‡æ•°æ®DataFrame
        """
        print("ğŸ“‚ æ­£åœ¨åŠ è½½æœ¬åœ°åŸå§‹æ•°æ®...")
        
        # æŸ¥æ‰¾æ‰€æœ‰åŸå§‹æ•°æ®æ–‡ä»¶ï¼ˆæ”¯æŒä¸¤ç§å‘½åæ ¼å¼ï¼‰
        pattern1 = os.path.join(self.raw_dir, "rawData_*.csv")
        raw_files = glob.glob(pattern1)
        
        if not raw_files:
            print(f"âŒ åœ¨ {self.raw_dir} ç›®å½•ä¸‹æœªæ‰¾åˆ°åŸå§‹æ•°æ®æ–‡ä»¶")
            print("è¯·å…ˆè¿è¡Œ fetch_raw_data.py è·å–åŸå§‹æ•°æ®")
            return pd.DataFrame()
        
        all_data = []
        loaded_years = []
        
        for file_path in sorted(raw_files):
            filename = os.path.basename(file_path)
            year_part = filename.replace('rawData_', '').replace('.csv', '')
            
            try:
                # å¤„ç†å¹´ä»½èŒƒå›´æ ¼å¼ (å¦‚ 2020-2024) å’Œå•å¹´ä»½æ ¼å¼ (å¦‚ 2024)
                if '-' in year_part:
                    start_year, end_year = map(int, year_part.split('-'))
                    file_years = list(range(start_year, end_year + 1))
                else:
                    file_years = [int(year_part)]
                
                # å¦‚æœæŒ‡å®šäº†å¹´ä»½ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰äº¤é›†
                if years is not None:
                    file_years = [y for y in file_years if y in years]
                    if not file_years:
                        continue
                
                df = pd.read_csv(file_path, encoding='utf-8-sig')
                
                if not df.empty:
                    # å¦‚æœæŒ‡å®šäº†å¹´ä»½ä¸”æ–‡ä»¶åŒ…å«å¤šå¹´æ•°æ®ï¼ŒæŒ‰å¹´ä»½è¿‡æ»¤
                    if years is not None and '-' in year_part:
                        df['time'] = pd.to_datetime(df['time'])
                        df = df[df['time'].dt.year.isin(years)]
                    
                    # æ•°æ®æ¸…æ´—
                    df = df.dropna(subset=['latitude', 'longitude', 'magnitude'])
                    df = df[df['magnitude'] >= min_magnitude]
                    
                    if not df.empty:
                        all_data.append(df)
                        loaded_years.extend(file_years)
                        print(f"  âœ… {year_part}: {len(df)} æ¡è®°å½•")
                    else:
                        print(f"  âš ï¸  {year_part}: æ¸…æ´—åæ— æœ‰æ•ˆæ•°æ®")
                else:
                    print(f"  âš ï¸  {year_part}: æ–‡ä»¶ä¸ºç©º")
                    
            except ValueError:
                print(f"  âŒ æ— æ³•è§£æå¹´ä»½: {filename}")
                continue
            except Exception as e:
                print(f"  âŒ åŠ è½½ {filename} å¤±è´¥: {e}")
                continue
        
        if all_data:
            combined_df = pd.concat(all_data, ignore_index=True)
            
            # ç¡®ä¿æ—¶é—´åˆ—æ˜¯datetimeç±»å‹
            combined_df['time'] = pd.to_datetime(combined_df['time'])
            
            # æŒ‰æ—¶é—´æ’åº
            combined_df = combined_df.sort_values('time').reset_index(drop=True)
            
            # å»é‡ï¼ˆå»é™¤å¹´ä»½èŒƒå›´é‡å å¯èƒ½å¯¼è‡´çš„é‡å¤æ•°æ®ï¼‰
            loaded_years = sorted(set(loaded_years))
            
            print(f"\nğŸ“Š æ•°æ®åŠ è½½å®Œæˆ:")
            print(f"  å¹´ä»½èŒƒå›´: {min(loaded_years)} - {max(loaded_years)}")
            print(f"  æ€»è®°å½•æ•°: {len(combined_df):,} æ¡")
            print(f"  æ—¶é—´èŒƒå›´: {combined_df['time'].min()} ~ {combined_df['time'].max()}")
            
            return combined_df
        else:
            print("âŒ æœªåŠ è½½åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®")
            return pd.DataFrame()
    
    def assign_prefectures(self, df):
        """
        ä¸ºåœ°éœ‡æ•°æ®åˆ†é…éƒ½é“åºœå¿
        """
        print("ğŸ—¾ æ­£åœ¨åˆ†é…éƒ½é“åºœçœŒ...")
        
        prefectures = []
        for _, row in df.iterrows():
            prefecture = get_prefecture_by_coordinates(row['latitude'], row['longitude'])
            prefectures.append(prefecture)
        
        df['prefecture'] = prefectures
        return df
    
    def save_by_prefecture(self, df):
        """
        æŒ‰éƒ½é“åºœå¿ä¿å­˜æ•°æ®
        """
        print("ğŸ’¾ æ­£åœ¨æŒ‰éƒ½é“åºœçœŒä¿å­˜æ•°æ®...")
        
        prefecture_stats = {}
        
        for prefecture in get_all_prefecture_names():
            prefecture_data = df[df['prefecture'] == prefecture].copy()
            
            if len(prefecture_data) > 0:
                # åˆ›å»ºç‰¹å¾
                features_df = create_earthquake_features(prefecture_data)
                
                if len(features_df) > 0:
                    # ä¿å­˜åŸå§‹æ•°æ®
                    raw_file = os.path.join(self.processed_dir, f"{prefecture}_åœ°éœ‡ãƒ‡ãƒ¼ã‚¿.csv")
                    prefecture_data.to_csv(raw_file, index=False, encoding='utf-8-sig')
                    
                    # ä¿å­˜ç‰¹å¾æ•°æ®
                    features_file = os.path.join(self.processed_dir, f"{prefecture}_ç‰¹å¾ãƒ‡ãƒ¼ã‚¿.csv")
                    features_df.to_csv(features_file, index=False, encoding='utf-8-sig')
                    
                    # å‡†å¤‡è®­ç»ƒæ•°æ®
                    X, y = prepare_training_data(features_df)
                    
                    prefecture_stats[prefecture] = {
                        'earthquake_count': len(prefecture_data),
                        'feature_count': len(features_df),
                        'trainable': X is not None,
                        'avg_magnitude': prefecture_data['magnitude'].mean(),
                        'max_magnitude': prefecture_data['magnitude'].max(),
                        'date_range': f"{prefecture_data['time'].min()} ~ {prefecture_data['time'].max()}"
                    }
                    
                    print(f"  âœ… {prefecture}: {len(prefecture_data)} æ¡åœ°éœ‡è®°å½•, {len(features_df)} æ¡ç‰¹å¾è®°å½•")
                else:
                    print(f"  âš ï¸  {prefecture}: {len(prefecture_data)} æ¡åœ°éœ‡è®°å½•, ä½†æ— æ³•ç”Ÿæˆç‰¹å¾")
                    prefecture_stats[prefecture] = {
                        'earthquake_count': len(prefecture_data),
                        'feature_count': 0,
                        'trainable': False,
                        'avg_magnitude': prefecture_data['magnitude'].mean(),
                        'max_magnitude': prefecture_data['magnitude'].max(),
                        'date_range': f"{prefecture_data['time'].min()} ~ {prefecture_data['time'].max()}"
                    }
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_df = pd.DataFrame(prefecture_stats).T
        stats_file = os.path.join(self.processed_dir, "prefecture_statistics.csv")
        stats_df.to_csv(stats_file, encoding='utf-8-sig')
        
        print(f"\nğŸ“Š æ•°æ®å¤„ç†å®Œæˆ! ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_file}")
        return prefecture_stats
    
    def process_all_data(self, years=None, min_magnitude=3.0):
        """
        å®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹
        """
        print("ğŸš€ å¼€å§‹æ—¥æœ¬åœ°éœ‡æ•°æ®å¤„ç†æµç¨‹...")
        print(f"å‚æ•°: æœ€å°éœ‡çº§{min_magnitude}")
        if years:
            print(f"æŒ‡å®šå¹´ä»½: {years}")
        
        # 1. åŠ è½½åŸå§‹æ•°æ®
        df = self.load_raw_data(years, min_magnitude)
        
        if df.empty:
            print("âŒ æœªåŠ è½½åˆ°æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ fetch_raw_data.py è·å–åŸå§‹æ•°æ®")
            return None, None
        
        # 2. æ•°æ®æ¸…æ´—
        print(f"åŠ è½½æ•°æ®: {len(df)} æ¡è®°å½•")
        df = df.dropna(subset=['latitude', 'longitude', 'magnitude'])
        df = df[df['magnitude'] >= min_magnitude]
        print(f"æ¸…æ´—åæ•°æ®: {len(df)} æ¡è®°å½•")
        
        # 3. åˆ†é…éƒ½é“åºœå¿
        df = self.assign_prefectures(df)
        
        # 4. æŒ‰éƒ½é“åºœå¿ä¿å­˜
        stats = self.save_by_prefecture(df)
        
        # 5. æ‰“å°ç»Ÿè®¡æ‘˜è¦
        self.print_summary(stats)
        
        print("ğŸ‰ æ•°æ®å¤„ç†å®Œæˆ!")
        
        return df, stats
    
    def print_summary(self, stats):
        """æ‰“å°å¤„ç†ç»“æœæ‘˜è¦"""
        print("\n" + "="*50)
        print("ğŸ“Š å¤„ç†ç»“æœæ‘˜è¦")
        print("="*50)
        
        total_earthquakes = sum(s['earthquake_count'] for s in stats.values())
        trainable_prefectures = sum(1 for s in stats.values() if s['trainable'])
        
        print(f"æ€»åœ°éœ‡è®°å½•æ•°: {total_earthquakes:,}")
        print(f"æ¶‰åŠéƒ½é“åºœçœŒ: {len(stats)}")
        print(f"å¯è®­ç»ƒéƒ½é“åºœçœŒ: {trainable_prefectures}")
        
        print("\nğŸ† åœ°éœ‡è®°å½•æœ€å¤šçš„éƒ½é“åºœçœŒ:")
        sorted_stats = sorted(stats.items(), key=lambda x: x[1]['earthquake_count'], reverse=True)
        for i, (prefecture, stat) in enumerate(sorted_stats[:10]):
            print(f"{i+1:2d}. {prefecture}: {stat['earthquake_count']:4d} æ¡è®°å½•")
        
        print(f"\nğŸ“ æ•°æ®æ–‡ä»¶ä¿å­˜åœ¨: {self.processed_dir}")

def main():
    """ä¸»å‡½æ•°"""
    processor = JapanEarthquakeProcessor()
    
    # å¤„ç†å‚æ•°
    min_magnitude = 3.0  # æœ€å°éœ‡çº§3.0
    
    print("æ—¥æœ¬åœ°éœ‡æ•°æ®å¤„ç†ç³»ç»Ÿ")
    print(f"å°†å¤„ç†æœ¬åœ°åŸå§‹æ•°æ®ï¼Œéœ‡çº§â‰¥{min_magnitude}")
    print("é»˜è®¤å¤„ç†æ‰€æœ‰å¯ç”¨å¹´ä»½æ•°æ®ï¼ˆé€šå¸¸ä¸ºæœ€è¿‘5å¹´ï¼‰")
    
    # å¯é€‰ï¼šæŒ‡å®šå¤„ç†ç‰¹å®šå¹´ä»½
    # years = [2022, 2023, 2024]  # å–æ¶ˆæ³¨é‡Šå¯æŒ‡å®šå¹´ä»½
    years = None  # å¤„ç†æ‰€æœ‰å¯ç”¨å¹´ä»½
    
    # å¼€å§‹å¤„ç†
    df, stats = processor.process_all_data(years, min_magnitude)

if __name__ == "__main__":
    main()