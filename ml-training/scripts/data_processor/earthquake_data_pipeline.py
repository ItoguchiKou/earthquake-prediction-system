"""
åœ°éœ‡é¢„æµ‹æ•°æ®å¤„ç†å®Œæ•´æµæ°´çº¿
æ•´åˆæ‰€æœ‰æ•°æ®å¤„ç†ç»„ä»¶ï¼Œç”Ÿæˆè®­ç»ƒå°±ç»ªçš„æ•°æ®
"""

import numpy as np
import pandas as pd
import os
import json
import time
from datetime import datetime, timedelta
from typing import Tuple, List, Dict, Optional
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥æ•°æ®å¤„ç†æ¨¡å—
from grid_system import JapanGridSystem
from grid_data_processor import GridDataProcessor
from multi_channel_features import MultiChannelFeatureBuilder
from multi_task_labels import MultiTaskLabelGenerator

class EarthquakeDataPipeline:
    """åœ°éœ‡æ•°æ®å¤„ç†å®Œæ•´æµæ°´çº¿"""
    
    def __init__(self, 
                 raw_data_dir: str,
                 output_dir: str = "data/processed_grid",
                 train_split_date: str = "2020-01-01",
                 verbose: bool = False):  # æ·»åŠ verboseå‚æ•°
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†æµæ°´çº¿
        
        Args:
            raw_data_dir: åŸå§‹æ•°æ®ç›®å½•è·¯å¾„ (ç›¸å¯¹æˆ–ç»å¯¹è·¯å¾„)
            output_dir: è¾“å‡ºç›®å½•
            train_split_date: è®­ç»ƒ/éªŒè¯åˆ†å‰²æ—¥æœŸ
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
        """
        self.raw_data_dir = raw_data_dir
        self.output_dir = output_dir
        self.train_split_date = pd.Timestamp(train_split_date)
        self.verbose = verbose  # åˆå§‹åŒ–verboseå±æ€§
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.grid_system = JapanGridSystem()
        self.grid_processor = GridDataProcessor(os.path.dirname(raw_data_dir))
        self.feature_builder = MultiChannelFeatureBuilder(self.grid_system, verbose=verbose)
        self.label_generator = MultiTaskLabelGenerator(self.grid_system)
        
        print(f"ğŸ”§ æ•°æ®å¤„ç†æµæ°´çº¿åˆå§‹åŒ–å®Œæˆ:")
        print(f"  åŸå§‹æ•°æ®ç›®å½•: {self.raw_data_dir}")
        print(f"  è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"  è®­ç»ƒ/éªŒè¯åˆ†å‰²: {train_split_date}")
        print(f"  ç½‘æ ¼ç³»ç»Ÿ: {self.grid_system.lat_grids} Ã— {self.grid_system.lon_grids}")
    
    def run_complete_pipeline(self, 
                            min_magnitude: float = 3.0,
                            history_days: int = 90,
                            step_days: int = 7,
                            prediction_windows: List[int] = [7, 14, 30]) -> bool:
        """
        è¿è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†æµæ°´çº¿
        
        Args:
            min_magnitude: æœ€å°éœ‡çº§
            history_days: å†å²æ•°æ®çª—å£
            step_days: æ»‘åŠ¨æ­¥é•¿
            prediction_windows: é¢„æµ‹æ—¶é—´çª—å£
            
        Returns:
            å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        print("ğŸš€ å¼€å§‹å®Œæ•´æ•°æ®å¤„ç†æµæ°´çº¿...")
        print("=" * 80)
        
        start_time = time.time()
        
        try:
            # é˜¶æ®µ1: åŠ è½½å’Œç½‘æ ¼åŒ–åŸå§‹æ•°æ®
            print("\nğŸ“‚ é˜¶æ®µ1: åŠ è½½å’Œç½‘æ ¼åŒ–åŸå§‹æ•°æ®")
            print("-" * 50)
            df_grid = self._load_and_grid_data(min_magnitude)
            
            if df_grid is None or df_grid.empty:
                print("âŒ æ•°æ®åŠ è½½å¤±è´¥")
                return False
            
            # é˜¶æ®µ2: åˆ›å»ºæ—¶é—´åºåˆ—æ ·æœ¬
            print("\nâ° é˜¶æ®µ2: åˆ›å»ºæ—¶é—´åºåˆ—æ ·æœ¬")
            print("-" * 50)
            samples_data = self._create_time_series_samples(
                df_grid, history_days, step_days, prediction_windows
            )
            
            if samples_data is None:
                print("âŒ æ—¶é—´åºåˆ—åˆ›å»ºå¤±è´¥")
                return False
            
            # é˜¶æ®µ3: åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯é›†
            print("\nâœ‚ï¸ é˜¶æ®µ3: åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯é›†")
            print("-" * 50)
            train_data, val_data = self._split_train_validation(samples_data)
            
            # é˜¶æ®µ4: ä¿å­˜æœ€ç»ˆæ•°æ®
            print("\nğŸ’¾ é˜¶æ®µ4: ä¿å­˜è®­ç»ƒæ•°æ®")
            print("-" * 50)
            self._save_training_data(train_data, val_data)
            
            # ç”Ÿæˆæ•°æ®æŠ¥å‘Š
            self._generate_data_report(train_data, val_data)
            
            total_time = time.time() - start_time
            print(f"\nğŸ‰ æ•°æ®å¤„ç†æµæ°´çº¿å®Œæˆ! æ€»è€—æ—¶: {total_time/60:.1f}åˆ†é’Ÿ")
            print("=" * 80)
            
            return True
            
        except Exception as e:
            print(f"\nâŒ æ•°æ®å¤„ç†æµæ°´çº¿å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _load_and_grid_data(self, min_magnitude: float) -> Optional[pd.DataFrame]:
        """åŠ è½½å’Œç½‘æ ¼åŒ–æ•°æ®"""
        # å‘ç°æ•°æ®æ–‡ä»¶
        files = self.grid_processor.discover_data_files()
        if not files:
            return None
        
        # åŠ è½½åŸå§‹æ•°æ®
        df = self.grid_processor.load_raw_data(files, min_magnitude)
        if df.empty:
            return None
        
        # æ˜ å°„åˆ°ç½‘æ ¼
        df_grid = self.grid_processor.map_to_grids(df)
        
        print(f"  âœ… ç½‘æ ¼åŒ–å®Œæˆ: {len(df_grid)} æ¡è®°å½•")
        return df_grid
    
    def _create_time_series_samples(self, 
                                df_grid: pd.DataFrame,
                                history_days: int,
                                step_days: int,
                                prediction_windows: List[int]) -> Optional[Dict]:
        """åˆ›å»ºæ—¶é—´åºåˆ—æ ·æœ¬"""
        # è®¡ç®—æ—¶é—´èŒƒå›´
        start_time = df_grid['time'].min()
        end_time = df_grid['time'].max()
        
        print(f"  æ•°æ®æ—¶é—´èŒƒå›´: {start_time} ~ {end_time}")
        print(f"  å†å²çª—å£: {history_days} å¤©")
        print(f"  é¢„æµ‹çª—å£: {prediction_windows} å¤©")
        print(f"  æ»‘åŠ¨æ­¥é•¿: {step_days} å¤©")
        
        # ç”Ÿæˆæ—¶é—´èŠ‚ç‚¹
        time_points = []
        current_time = start_time + timedelta(days=history_days)
        max_prediction_window = max(prediction_windows)
        
        while current_time + timedelta(days=max_prediction_window) <= end_time:
            time_points.append(current_time)
            current_time += timedelta(days=step_days)
        
        print(f"  ç”Ÿæˆæ ·æœ¬æ•°: {len(time_points)}")
        
        if len(time_points) == 0:
            print("âŒ æ— æ³•ç”Ÿæˆæœ‰æ•ˆæ ·æœ¬")
            return None
        
        # åˆ›å»ºæ ·æœ¬æ•°æ®
        all_features = []
        stat_features_list = [] # æ–°å¢ï¼šç»Ÿè®¡ç‰¹å¾
        all_labels = []
        all_timestamps = []
        
        print(f"  å¤„ç†æ ·æœ¬è¿›åº¦:")
        
        for i, timestamp in enumerate(time_points):
            # è°ƒæ•´è¿›åº¦æ˜¾ç¤ºé¢‘ç‡
            if (i + 1) % 100 == 0 or i == 0 or i == len(time_points) - 1:
                progress = (i + 1) / len(time_points) * 100
                print(f"    {i+1}/{len(time_points)} ({progress:.1f}%)")
                
                # åªåœ¨è¯¦ç»†æ¨¡å¼ä¸‹æ˜¾ç¤ºæ›´å¤šä¿¡æ¯
                if self.verbose and ((i + 1) % 500 == 0 or i == 0):
                    print(f"      å½“å‰å¤„ç†æ—¶é—´ç‚¹: {timestamp}")
                    history_start = timestamp - timedelta(days=history_days)
                    print(f"      å†å²çª—å£: {history_start} ~ {timestamp}")
            
            # æ„å»ºè¯¥æ ·æœ¬çš„ç‰¹å¾
            sample_features = self._build_sample_features(
                df_grid, timestamp, history_days
            )
            
            # æ„å»ºè¯¥æ ·æœ¬çš„æ ‡ç­¾
            sample_labels = self._build_sample_labels(
                df_grid, timestamp, prediction_windows
            )
            
            if sample_features is not None and sample_labels is not None:
                all_features.append(sample_features)
                all_labels.append(sample_labels)
                all_timestamps.append(timestamp)
        
        if len(all_features) == 0:
            print("âŒ æœªç”Ÿæˆæœ‰æ•ˆæ ·æœ¬")
            return None
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        features_array = np.array(all_features, dtype=np.float32)
        labels_array = np.array(all_labels, dtype=np.float32)
        timestamps_array = np.array(all_timestamps)
        
        # æå–ç»Ÿè®¡ç‰¹å¾
        print("  ğŸ“Š æå–ç»Ÿè®¡ç‰¹å¾...")
        for i in range(len(features_array)):
            stat_features = self.feature_builder.extract_statistical_features(
                features_array[i]  # ä¼ å…¥å•ä¸ªæ ·æœ¬ï¼Œå½¢çŠ¶ä¸º [æ—¶é—´, çº¬åº¦, ç»åº¦, é€šé“]
            )
            stat_features_list.append(stat_features)
        stat_features_array = np.array(stat_features_list, dtype=np.float32)
        
        # åˆ›å»ºåŠ æƒæ ‡ç­¾
        print("  âš–ï¸ åˆ›å»ºåŠ æƒæ ‡ç­¾...")
        weighted_labels = self.label_generator.create_weighted_labels(labels_array)
        
        print(f"  âœ… æ ·æœ¬åˆ›å»ºå®Œæˆ:")
        print(f"    æ—¶åºç‰¹å¾å½¢çŠ¶: {features_array.shape}")
        print(f"    ç»Ÿè®¡ç‰¹å¾å½¢çŠ¶: {stat_features_array.shape}")
        print(f"    æ ‡ç­¾å½¢çŠ¶: {labels_array.shape}")
        print(f"    åŠ æƒæ ‡ç­¾å½¢çŠ¶: {weighted_labels.shape}")
        print(f"    æœ‰æ•ˆæ ·æœ¬æ•°: {len(timestamps_array)}")
        
        return {
            'features': features_array,
            'stat_features': stat_features_array,
            'labels': labels_array,
            'weighted_labels': weighted_labels,
            'timestamps': timestamps_array
        }
    
    def _build_sample_features(self, 
                            df_grid: pd.DataFrame,
                            timestamp: pd.Timestamp,
                            history_days: int) -> Optional[np.ndarray]:
        """æ„å»ºå•ä¸ªæ ·æœ¬çš„ç‰¹å¾"""
        # è·å–å†å²æ•°æ®
        history_start = timestamp - timedelta(days=history_days)
        history_data = df_grid[
            (df_grid['time'] >= history_start) &
            (df_grid['time'] < timestamp)
        ]
        
        # ä½¿ç”¨å¤šé€šé“ç‰¹å¾æ„å»ºå™¨
        try:
            features = self.feature_builder.build_enhanced_time_series(
                history_data, 
                history_days=history_days,
                verbose=False  # å…³é—­è¯¦ç»†æ—¥å¿—
            )
            return features
        except Exception as e:
            if self.verbose:  # ç°åœ¨å¯ä»¥æ­£ç¡®ä½¿ç”¨verboseå±æ€§
                print(f"âš ï¸ æ ·æœ¬ç‰¹å¾æ„å»ºå¤±è´¥ ({timestamp}): {e}")
            return None
    
    def _build_sample_labels(self, 
                           df_grid: pd.DataFrame,
                           timestamp: pd.Timestamp,
                           prediction_windows: List[int]) -> Optional[np.ndarray]:
        """æ„å»ºå•ä¸ªæ ·æœ¬çš„æ ‡ç­¾"""
        try:
            # ä½¿ç”¨å¤šä»»åŠ¡æ ‡ç­¾ç”Ÿæˆå™¨
            sample_labels = self.label_generator._generate_sample_labels(
                df_grid, timestamp, target_region_size=(10, 10)
            )
            return sample_labels
        except Exception as e:
            if self.verbose:
                print(f"âš ï¸ æ ·æœ¬æ ‡ç­¾æ„å»ºå¤±è´¥ ({timestamp}): {e}")
            return None
    
    def _split_train_validation(self, samples_data: Dict) -> Tuple[Dict, Dict]:
        """æŒ‰æ—¶é—´åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯é›†"""
        features = samples_data['features']
        stat_features = samples_data['stat_features']
        labels = samples_data['labels']
        weighted_labels = samples_data['weighted_labels']
        timestamps = samples_data['timestamps']
        
        # æŒ‰æ—¶é—´åˆ†å‰²
        train_mask = timestamps < self.train_split_date
        val_mask = timestamps >= self.train_split_date
        
        train_data = {
            'features': features[train_mask],
            'stat_features': stat_features[train_mask],
            'labels': labels[train_mask],
            'weighted_labels': weighted_labels[train_mask],
            'timestamps': timestamps[train_mask]
        }
        
        val_data = {
            'features': features[val_mask],
            'stat_features': stat_features[val_mask],
            'labels': labels[val_mask],
            'weighted_labels': weighted_labels[val_mask],
            'timestamps': timestamps[val_mask]
        }
        
        print(f"  è®­ç»ƒé›†: {len(train_data['features'])} æ ·æœ¬")
        print(f"    æ—¶é—´èŒƒå›´: {train_data['timestamps'].min()} ~ {train_data['timestamps'].max()}")
        print(f"  éªŒè¯é›†: {len(val_data['features'])} æ ·æœ¬")
        print(f"    æ—¶é—´èŒƒå›´: {val_data['timestamps'].min()} ~ {val_data['timestamps'].max()}")
        
        # æ£€æŸ¥åˆ†å‰²åˆç†æ€§
        if len(train_data['features']) == 0:
            print("âš ï¸ è®­ç»ƒé›†ä¸ºç©ºï¼Œè°ƒæ•´åˆ†å‰²æ—¥æœŸ")
        if len(val_data['features']) == 0:
            print("âš ï¸ éªŒè¯é›†ä¸ºç©ºï¼Œè°ƒæ•´åˆ†å‰²æ—¥æœŸ")
        
        return train_data, val_data
    
    def _save_training_data(self, train_data: Dict, val_data: Dict):
        """ä¿å­˜è®­ç»ƒæ•°æ® - ä¿®å¤æ—¶é—´æˆ³é—®é¢˜"""
        # ä¿å­˜è®­ç»ƒé›†
        train_features_path = os.path.join(self.output_dir, "train_features.npy")
        train_stat_features_path = os.path.join(self.output_dir, "train_stat_features.npy")
        train_labels_path = os.path.join(self.output_dir, "train_labels.npy")
        train_weighted_labels_path = os.path.join(self.output_dir, "train_weighted_labels.npy")
        train_timestamps_path = os.path.join(self.output_dir, "train_timestamps.npy")
        
        np.save(train_features_path, train_data['features'])
        np.save(train_stat_features_path, train_data['stat_features'])
        np.save(train_labels_path, train_data['labels'])
        np.save(train_weighted_labels_path, train_data['weighted_labels'])
        
        # ä¿®å¤ï¼šç¡®ä¿æ—¶é—´æˆ³ä»¥float64æ ¼å¼ä¿å­˜
        train_timestamps_unix = np.array([
            ts.timestamp() if hasattr(ts, 'timestamp') else float(ts) 
            for ts in train_data['timestamps']
        ], dtype=np.float64)
        np.save(train_timestamps_path, train_timestamps_unix)
        
        print(f"  âœ… è®­ç»ƒé›†å·²ä¿å­˜:")
        print(f"    æ—¶åºç‰¹å¾: {train_features_path}")
        print(f"    ç»Ÿè®¡ç‰¹å¾: {train_stat_features_path}")
        print(f"    æ ‡ç­¾: {train_labels_path}")
        print(f"    åŠ æƒæ ‡ç­¾: {train_weighted_labels_path}")
        print(f"    æ—¶é—´æˆ³: {train_timestamps_path} (dtype: {train_timestamps_unix.dtype})")
        
        # ä¿å­˜éªŒè¯é›†
        val_features_path = os.path.join(self.output_dir, "val_features.npy")
        val_stat_features_path = os.path.join(self.output_dir, "val_stat_features.npy")
        val_labels_path = os.path.join(self.output_dir, "val_labels.npy")
        val_weighted_labels_path = os.path.join(self.output_dir, "val_weighted_labels.npy")
        val_timestamps_path = os.path.join(self.output_dir, "val_timestamps.npy")
        
        np.save(val_features_path, val_data['features'])
        np.save(val_stat_features_path, val_data['stat_features'])
        np.save(val_labels_path, val_data['labels'])
        np.save(val_weighted_labels_path, val_data['weighted_labels'])
        
        # ä¿®å¤ï¼šç¡®ä¿æ—¶é—´æˆ³ä»¥float64æ ¼å¼ä¿å­˜
        val_timestamps_unix = np.array([
            ts.timestamp() if hasattr(ts, 'timestamp') else float(ts) 
            for ts in val_data['timestamps']
        ], dtype=np.float64)
        np.save(val_timestamps_path, val_timestamps_unix)
        
        print(f"  âœ… éªŒè¯é›†å·²ä¿å­˜:")
        print(f"    æ—¶åºç‰¹å¾: {val_features_path}")
        print(f"    ç»Ÿè®¡ç‰¹å¾: {val_stat_features_path}")
        print(f"    æ ‡ç­¾: {val_labels_path}")
        print(f"    åŠ æƒæ ‡ç­¾: {val_weighted_labels_path}")
        print(f"    æ—¶é—´æˆ³: {val_timestamps_path} (dtype: {val_timestamps_unix.dtype})")
        
        # ä¿å­˜å¤„ç†é…ç½®
        config = {
            'pipeline_info': {
                'created_at': datetime.now().isoformat(),
                'train_split_date': self.train_split_date.isoformat(),
                'grid_system': self.grid_system.get_grid_info(),
                'feature_info': self.feature_builder.get_feature_info(),
                'task_info': self.label_generator.get_task_info()
            },
            'data_shapes': {
                'train_features': train_data['features'].shape,
                'train_stat_features': train_data['stat_features'].shape,
                'train_labels': train_data['labels'].shape,
                'train_weighted_labels': train_data['weighted_labels'].shape,
                'val_features': val_data['features'].shape,
                'val_stat_features': val_data['stat_features'].shape,
                'val_labels': val_data['labels'].shape,
                'val_weighted_labels': val_data['weighted_labels'].shape
            },
            'file_sizes_mb': {
                'train_features': os.path.getsize(train_features_path) / (1024*1024),
                'train_stat_features': os.path.getsize(train_stat_features_path) / (1024*1024),
                'train_labels': os.path.getsize(train_labels_path) / (1024*1024),
                'train_weighted_labels': os.path.getsize(train_weighted_labels_path) / (1024*1024),
                'val_features': os.path.getsize(val_features_path) / (1024*1024),
                'val_stat_features': os.path.getsize(val_stat_features_path) / (1024*1024),
                'val_labels': os.path.getsize(val_labels_path) / (1024*1024),
                'val_weighted_labels': os.path.getsize(val_weighted_labels_path) / (1024*1024)
            }
        }
        
        config_path = os.path.join(self.output_dir, "pipeline_config.json")
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"  âœ… é…ç½®å·²ä¿å­˜: {config_path}")
    
    def _generate_data_report(self, train_data: Dict, val_data: Dict):
        """ç”Ÿæˆæ•°æ®æŠ¥å‘Š"""
        report_dir = os.path.join(self.output_dir, "data_report")
        os.makedirs(report_dir, exist_ok=True)
        
        # æ•°æ®ç»Ÿè®¡
        train_features = train_data['features']
        train_stat_features = train_data['stat_features']
        train_labels = train_data['labels']
        train_weighted_labels = train_data['weighted_labels']
        val_features = val_data['features']
        val_stat_features = val_data['stat_features']
        val_labels = val_data['labels']
        val_weighted_labels = val_data['weighted_labels']
        
        # åŸºç¡€ç»Ÿè®¡
        report = {
            'dataset_summary': {
                'total_samples': len(train_features) + len(val_features),
                'train_samples': len(train_features),
                'val_samples': len(val_features),
                'train_ratio': len(train_features) / (len(train_features) + len(val_features)),
                'feature_shape': list(train_features.shape[1:]),
                'stat_feature_shape': list(train_stat_features.shape[1:]),
                'label_shape': list(train_labels.shape[1:])
            },
            'time_coverage': {
                'train_start': str(train_data['timestamps'].min()),
                'train_end': str(train_data['timestamps'].max()),
                'val_start': str(val_data['timestamps'].min()),
                'val_end': str(val_data['timestamps'].max())
            },
            'feature_statistics': {
                'train_mean': train_features.mean(axis=(0,1,2,3)).tolist(),
                'train_std': train_features.std(axis=(0,1,2,3)).tolist(),
                'val_mean': val_features.mean(axis=(0,1,2,3)).tolist(),
                'val_std': val_features.std(axis=(0,1,2,3)).tolist()
            },
            'stat_feature_statistics': {
                'train_mean': train_stat_features.mean(axis=(0,1,2)).tolist(),
                'train_std': train_stat_features.std(axis=(0,1,2)).tolist(),
                'val_mean': val_stat_features.mean(axis=(0,1,2)).tolist(),
                'val_std': val_stat_features.std(axis=(0,1,2)).tolist()
            },
            'label_statistics': {
                'train_positive_rates': train_labels.mean(axis=0).tolist(),
                'val_positive_rates': val_labels.mean(axis=0).tolist(),
                'train_weighted_mean': train_weighted_labels.mean(axis=0).tolist(),
                'val_weighted_mean': val_weighted_labels.mean(axis=0).tolist(),
                'task_names': self.label_generator.task_names
            }
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = os.path.join(report_dir, "data_report.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°å…³é”®ç»Ÿè®¡
        print(f"\nğŸ“Š æ•°æ®æŠ¥å‘Š:")
        print(f"  æ€»æ ·æœ¬æ•°: {report['dataset_summary']['total_samples']:,}")
        print(f"  è®­ç»ƒé›†: {report['dataset_summary']['train_samples']:,} ({report['dataset_summary']['train_ratio']:.1%})")
        print(f"  éªŒè¯é›†: {report['dataset_summary']['val_samples']:,}")
        print(f"  æ—¶åºç‰¹å¾ç»´åº¦: {report['dataset_summary']['feature_shape']}")
        print(f"  ç»Ÿè®¡ç‰¹å¾ç»´åº¦: {report['dataset_summary']['stat_feature_shape']}")
        print(f"  æ ‡ç­¾ç»´åº¦: {report['dataset_summary']['label_shape']}")
        
        print(f"\n  è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ (æ­£æ ·æœ¬ç‡):")
        for i, (task_name, rate) in enumerate(zip(report['label_statistics']['task_names'], 
                                                 report['label_statistics']['train_positive_rates'])):
            print(f"    {task_name:15s}: {rate:.3f}")
        
        print(f"\n  æ•°æ®æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    def verify_output_files(self) -> bool:
        """éªŒè¯è¾“å‡ºæ–‡ä»¶çš„å®Œæ•´æ€§"""
        print("\nğŸ” éªŒè¯è¾“å‡ºæ–‡ä»¶...")
        
        required_files = [
            "train_features.npy",
            "train_stat_features.npy",
            "train_labels.npy",
            "train_weighted_labels.npy", 
            "train_timestamps.npy",
            "val_features.npy",
            "val_stat_features.npy",
            "val_labels.npy",
            "val_weighted_labels.npy",
            "val_timestamps.npy"
        ]
        
        all_exists = True
        for filename in required_files:
            filepath = os.path.join(self.output_dir, filename)
            if os.path.exists(filepath):
                file_size = os.path.getsize(filepath) / (1024*1024)
                print(f"  âœ… {filename}: {file_size:.1f} MB")
            else:
                print(f"  âŒ {filename}: æ–‡ä»¶ä¸å­˜åœ¨")
                all_exists = False
        
        if all_exists:
            print("  âœ… æ‰€æœ‰æ–‡ä»¶éªŒè¯é€šè¿‡")
            
            # éªŒè¯æ•°æ®å½¢çŠ¶ä¸€è‡´æ€§
            try:
                train_features = np.load(os.path.join(self.output_dir, "train_features.npy"))
                train_stat_features = np.load(os.path.join(self.output_dir, "train_stat_features.npy"))
                train_labels = np.load(os.path.join(self.output_dir, "train_labels.npy"))
                train_weighted_labels = np.load(os.path.join(self.output_dir, "train_weighted_labels.npy"))
                val_features = np.load(os.path.join(self.output_dir, "val_features.npy"))
                val_stat_features = np.load(os.path.join(self.output_dir, "val_stat_features.npy"))
                val_labels = np.load(os.path.join(self.output_dir, "val_labels.npy"))
                val_weighted_labels = np.load(os.path.join(self.output_dir, "val_weighted_labels.npy"))
                
                print(f"  ğŸ“Š æ•°æ®å½¢çŠ¶éªŒè¯:")
                print(f"    è®­ç»ƒæ—¶åºç‰¹å¾: {train_features.shape}")
                print(f"    è®­ç»ƒç»Ÿè®¡ç‰¹å¾: {train_stat_features.shape}")
                print(f"    è®­ç»ƒæ ‡ç­¾: {train_labels.shape}")
                print(f"    è®­ç»ƒåŠ æƒæ ‡ç­¾: {train_weighted_labels.shape}")
                print(f"    éªŒè¯æ—¶åºç‰¹å¾: {val_features.shape}")
                print(f"    éªŒè¯ç»Ÿè®¡ç‰¹å¾: {val_stat_features.shape}")
                print(f"    éªŒè¯æ ‡ç­¾: {val_labels.shape}")
                print(f"    éªŒè¯åŠ æƒæ ‡ç­¾: {val_weighted_labels.shape}")
                
                # æ£€æŸ¥å½¢çŠ¶ä¸€è‡´æ€§
                assert train_features.shape[1:] == val_features.shape[1:], "æ—¶åºç‰¹å¾å½¢çŠ¶ä¸ä¸€è‡´"
                assert train_stat_features.shape[1:] == val_stat_features.shape[1:], "ç»Ÿè®¡ç‰¹å¾å½¢çŠ¶ä¸ä¸€è‡´"
                assert train_labels.shape[1:] == val_labels.shape[1:], "æ ‡ç­¾å½¢çŠ¶ä¸ä¸€è‡´"
                assert train_weighted_labels.shape[1:] == val_weighted_labels.shape[1:], "åŠ æƒæ ‡ç­¾å½¢çŠ¶ä¸ä¸€è‡´"
                assert train_features.shape[0] == train_labels.shape[0], "è®­ç»ƒé›†ç‰¹å¾æ ‡ç­¾æ•°é‡ä¸åŒ¹é…"
                assert val_features.shape[0] == val_labels.shape[0], "éªŒè¯é›†ç‰¹å¾æ ‡ç­¾æ•°é‡ä¸åŒ¹é…"
                
                print("  âœ… æ•°æ®å½¢çŠ¶éªŒè¯é€šè¿‡")
                
            except Exception as e:
                print(f"  âŒ æ•°æ®éªŒè¯å¤±è´¥: {e}")
                all_exists = False
        
        return all_exists


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒ åœ°éœ‡é¢„æµ‹æ•°æ®å¤„ç†å®Œæ•´æµæ°´çº¿")
    print("=" * 80)
    
    # é…ç½®å‚æ•° - ä½¿ç”¨ç›¸å¯¹è·¯å¾„
    config = {
        'raw_data_dir': "../../data/raw",          # ç›¸å¯¹è·¯å¾„ï¼šå½“å‰ç›®å½•ä¸‹çš„data/raw
        'output_dir': "../../data/processed_grid",       # ç›¸å¯¹è·¯å¾„ï¼šå½“å‰ç›®å½•ä¸‹çš„data/processed_grid
        'train_split_date': "2020-01-01",          # 2020å¹´å‰ä½œä¸ºè®­ç»ƒé›†
        'min_magnitude': 3.0,                      # æœ€å°éœ‡çº§
        'history_days': 90,                        # 90å¤©å†å²çª—å£
        'step_days': 7,                            # 7å¤©æ­¥é•¿
        'prediction_windows': [7, 14, 30]          # é¢„æµ‹æ—¶é—´çª—å£
    }
    
    print("ğŸ”§ æµæ°´çº¿é…ç½®:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    print()
    
    # æ£€æŸ¥åŸå§‹æ•°æ®ç›®å½•
    if not os.path.exists(config['raw_data_dir']):
        print(f"âŒ åŸå§‹æ•°æ®ç›®å½•ä¸å­˜åœ¨: {config['raw_data_dir']}")
        return False
    
    # åˆ›å»ºæµæ°´çº¿
    pipeline = EarthquakeDataPipeline(
        raw_data_dir=config['raw_data_dir'],
        output_dir=config['output_dir'],
        train_split_date=config['train_split_date'],
        verbose=True  # è®¾ç½®è¯¦ç»†æ—¥å¿—çº§åˆ«
    )
    
    # è¿è¡Œå®Œæ•´æµæ°´çº¿
    success = pipeline.run_complete_pipeline(
        min_magnitude=config['min_magnitude'],
        history_days=config['history_days'],
        step_days=config['step_days'],
        prediction_windows=config['prediction_windows']
    )
    
    if success:
        # éªŒè¯è¾“å‡ºæ–‡ä»¶
        verification_passed = pipeline.verify_output_files()
        
        if verification_passed:
            print("\nğŸ‰ æ•°æ®å¤„ç†æµæ°´çº¿æˆåŠŸå®Œæˆ!")
            print("âœ… æ‰€æœ‰æ–‡ä»¶å·²ç”Ÿæˆå¹¶éªŒè¯é€šè¿‡")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {pipeline.output_dir}")
            return True
        else:
            print("\nâŒ æ–‡ä»¶éªŒè¯å¤±è´¥")
            return False
    else:
        print("\nâŒ æ•°æ®å¤„ç†æµæ°´çº¿å¤±è´¥")
        return False


def create_quick_test_pipeline(test_data_dir: str = "test_data"):
    """åˆ›å»ºå¿«é€Ÿæµ‹è¯•æµæ°´çº¿ (ç”¨äºéªŒè¯ä»£ç )"""
    print("ğŸ§ª åˆ›å»ºå¿«é€Ÿæµ‹è¯•æµæ°´çº¿...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®ç›®å½•
    os.makedirs(f"{test_data_dir}/raw", exist_ok=True)
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    print("ğŸ“ ç”Ÿæˆæ¨¡æ‹Ÿæµ‹è¯•æ•°æ®...")
    
    # æ¨¡æ‹ŸCSVæ•°æ®
    dates = pd.date_range('2019-01-01', '2021-12-31', freq='D')
    mock_earthquakes = []
    
    np.random.seed(42)
    for date in dates[::7]:  # æ¯å‘¨ä¸€äº›åœ°éœ‡
        n_quakes = np.random.poisson(2)  # å¹³å‡æ¯å‘¨2ä¸ªåœ°éœ‡
        
        for _ in range(n_quakes):
            # æ—¥æœ¬èŒƒå›´å†…çš„éšæœºä½ç½®
            lat = np.random.uniform(30.0, 40.0)
            lon = np.random.uniform(135.0, 145.0)
            mag = np.random.exponential(1.5) + 3.0  # æŒ‡æ•°åˆ†å¸ƒ + æœ€å°éœ‡çº§
            depth = np.random.uniform(5, 100)
            
            earthquake_time = date + timedelta(
                hours=np.random.randint(0, 24),
                minutes=np.random.randint(0, 60)
            )
            
            mock_earthquakes.append({
                'magnitude': min(mag, 8.0),  # é™åˆ¶æœ€å¤§éœ‡çº§
                'longitude': lon,
                'latitude': lat,
                'depth': depth,
                'time': earthquake_time.isoformat(),
                'place': f"Test location {lat:.1f}, {lon:.1f}",
                'mag_type': 'test',
                'id': f'test_{len(mock_earthquakes)}',
                'url': ''
            })
    
    # ä¿å­˜æµ‹è¯•æ•°æ®
    df_test = pd.DataFrame(mock_earthquakes)
    test_file = f"{test_data_dir}/raw/rawData_2019-2021.csv"
    df_test.to_csv(test_file, index=False)
    
    print(f"  âœ… æµ‹è¯•æ•°æ®å·²ç”Ÿæˆ: {test_file}")
    print(f"     è®°å½•æ•°: {len(df_test)}")
    print(f"     æ—¶é—´èŒƒå›´: {df_test['time'].min()} ~ {df_test['time'].max()}")
    
    # è¿è¡Œæµ‹è¯•æµæ°´çº¿
    pipeline = EarthquakeDataPipeline(
        raw_data_dir=f"{test_data_dir}/raw",
        output_dir=f"{test_data_dir}/processed_grid",
        train_split_date="2020-07-01",
        verbose=False
    )
    
    success = pipeline.run_complete_pipeline(
        min_magnitude=3.0,
        history_days=30,    # å‡å°‘å†å²å¤©æ•°ç”¨äºæµ‹è¯•
        step_days=14,       # å¢å¤§æ­¥é•¿å‡å°‘æ ·æœ¬æ•°
        prediction_windows=[7, 14]  # å‡å°‘é¢„æµ‹çª—å£
    )
    
    if success:
        pipeline.verify_output_files()
    
    return success


if __name__ == "__main__":
    # é€‰æ‹©è¿è¡Œæ¨¡å¼
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        # æµ‹è¯•æ¨¡å¼
        print("ğŸ§ª è¿è¡Œæµ‹è¯•æ¨¡å¼...")
        create_quick_test_pipeline()
    else:
        # æ­£å¸¸æ¨¡å¼
        main()