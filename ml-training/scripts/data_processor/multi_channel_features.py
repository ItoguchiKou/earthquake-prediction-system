"""
å¤šé€šé“ç‰¹å¾æ„å»ºæ¨¡å—
ä¸ºCNNæ¨¡å‹æ„å»ºå¤šç»´åº¦åœ°éœ‡ç‰¹å¾é€šé“ - é€‚é…ä¸è§„åˆ™ç½‘æ ¼
"""

import numpy as np
import pandas as pd
from datetime import timedelta
from typing import Dict, Optional
from scipy import ndimage
import warnings
warnings.filterwarnings('ignore')

from grid_system import JapanGridSystem

class MultiChannelFeatureBuilder:
    """å¤šé€šé“ç‰¹å¾æ„å»ºå™¨ - ä¸è§„åˆ™ç½‘æ ¼ç‰ˆæœ¬"""

    def __init__(self, grid_system: JapanGridSystem, verbose: bool = True):
        """
        åˆå§‹åŒ–ç‰¹å¾æ„å»ºå™¨
        
        Args:
            grid_system: ç½‘æ ¼ç³»ç»Ÿå®ä¾‹
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
        """
        self.grid_system = grid_system
        self.verbose = verbose

        # ç‰¹å¾é€šé“å®šä¹‰
        self.channels = {
            0: "earthquake_frequency",     # åœ°éœ‡é¢‘ç‡å¯†åº¦
            1: "magnitude_mean",          # å¹³å‡éœ‡çº§
            2: "magnitude_max",           # æœ€å¤§éœ‡çº§
            3: "energy_release",          # èƒ½é‡é‡Šæ”¾
            4: "magnitude_std",           # éœ‡çº§æ ‡å‡†å·®
            5: "depth_mean",              # å¹³å‡æ·±åº¦
            6: "temporal_density",        # æ—¶é—´å¯†åº¦å˜åŒ–
            7: "spatial_correlation"      # ç©ºé—´ç›¸å…³æ€§
        }

        self.num_channels = len(self.channels)
        
        # è·å–æœ‰æ•ˆç½‘æ ¼åˆ—è¡¨
        self.valid_grids = grid_system.get_all_valid_grids()
        
        if self.verbose:
            print(f"å¤šé€šé“ç‰¹å¾æ„å»ºå™¨åˆå§‹åŒ–:")
            print(f"  æœ‰æ•ˆç½‘æ ¼æ•°: {len(self.valid_grids)}")
            print(f"  é€šé“æ•°: {self.num_channels}")
            for idx, name in self.channels.items():
                print(f"    é€šé“{idx}: {name}")

    def build_enhanced_time_series(self, df: pd.DataFrame,
                                history_days: int = 90,
                                time_step_hours: int = 24,
                                verbose: Optional[bool] = None) -> np.ndarray:
        """
        æ„å»ºå¢å¼ºçš„å¤šé€šé“æ—¶é—´åºåˆ—ç‰¹å¾ - ä½¿ç”¨å¯†é›†çŸ©é˜µè¡¨ç¤º
        
        Args:
            df: ç½‘æ ¼åŒ–åœ°éœ‡æ•°æ®DataFrame
            history_days: å†å²æ•°æ®å¤©æ•°
            time_step_hours: æ—¶é—´æ­¥é•¿(å°æ—¶)
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—ï¼ˆNoneæ—¶ä½¿ç”¨å®ä¾‹é»˜è®¤å€¼ï¼‰
        
        Returns:
            å¤šé€šé“æ—¶é—´åºåˆ— [æ—¶é—´æ­¥é•¿, 10, 8, é€šé“æ•°] - ä¿æŒå…¼å®¹æ€§
        """
        show_log = self.verbose if verbose is None else verbose
        
        if show_log and not hasattr(self, '_first_run_done'):
            print(f"\nâš™ï¸ æ„å»ºå¢å¼ºå¤šé€šé“ç‰¹å¾...")
            print(f"  å†å²å¤©æ•°: {history_days} å¤©")
            print(f"  æ—¶é—´æ­¥é•¿: {time_step_hours} å°æ—¶")
            print(f"  è¾“å‡ºç»´åº¦: [{history_days}, 10, 8, {self.num_channels}]")
            self._first_run_done = True
            
        # åˆå§‹åŒ–ç‰¹å¾æ•°ç»„ - ä½¿ç”¨10Ã—8çš„å¯†é›†è¡¨ç¤º
        time_steps = history_days
        features = np.zeros((time_steps, 10, 8, self.num_channels))

        if len(df) == 0:
            return features

        # æŒ‰æ—¥æœŸåˆ†ç»„å¤„ç†
        df_copy = df.copy()
        df_copy['date'] = df_copy['time'].dt.date
        start_date = df_copy['time'].min().date()

        # ä¸ºæ¯ä¸€å¤©æ„å»ºç‰¹å¾
        for day_offset in range(time_steps):
            current_date = start_date + timedelta(days=day_offset)
            day_data = df_copy[df_copy['date'] == current_date]

            if len(day_data) > 0:
                # æ„å»ºå½“å¤©çš„å¤šé€šé“ç‰¹å¾
                daily_features = self._build_daily_features(day_data, df_copy, current_date)
                features[day_offset] = daily_features

        # åå¤„ç†ï¼šå¹³æ»‘å’Œå½’ä¸€åŒ–
        features = self._post_process_features(features)

        return features

    def _build_daily_features(self, day_data: pd.DataFrame,
                            all_data: pd.DataFrame,
                            current_date) -> np.ndarray:
        """
        æ„å»ºå•æ—¥å¤šé€šé“ç‰¹å¾ - åªå¤„ç†æœ‰æ•ˆç½‘æ ¼

        Args:
            day_data: å½“æ—¥åœ°éœ‡æ•°æ®
            all_data: æ‰€æœ‰å†å²æ•°æ®
            current_date: å½“å‰æ—¥æœŸ

        Returns:
            å•æ—¥ç‰¹å¾ [10, 8, é€šé“æ•°] - æ— æ•ˆç½‘æ ¼å¡«å……0
        """
        daily_features = np.zeros((10, 8, self.num_channels))

        # æŒ‰ç½‘æ ¼åˆ†ç»„
        grid_groups = day_data.groupby(['lat_idx', 'lon_idx'])

        for (lat_idx, lon_idx), grid_data in grid_groups:
            lat_idx, lon_idx = int(lat_idx), int(lon_idx)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆç½‘æ ¼
            if (lat_idx, lon_idx) in self.valid_grids:
                # æ„å»ºè¯¥ç½‘æ ¼çš„ç‰¹å¾
                grid_features = self._build_grid_features(
                    grid_data, all_data, current_date, lat_idx, lon_idx
                )
                daily_features[lat_idx, lon_idx] = grid_features

        return daily_features

    def _build_grid_features(self, grid_data: pd.DataFrame,
                           all_data: pd.DataFrame,
                           current_date,
                           lat_idx: int,
                           lon_idx: int) -> np.ndarray:
        """
        æ„å»ºå•ä¸ªç½‘æ ¼çš„ç‰¹å¾å‘é‡ - å¢å¼ºç‰ˆæœ¬
        """
        features = np.zeros(self.num_channels)
        
        # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
        magnitudes = grid_data['magnitude'].values
        depths = grid_data['depth'].values if 'depth' in grid_data.columns else np.array([0])
        
        if len(magnitudes) > 0:
            # é€šé“0: åœ°éœ‡é¢‘ç‡å¯†åº¦ï¼ˆå¯¹æ•°å˜æ¢å‡å°‘ç¨€ç–æ€§ï¼‰
            features[0] = np.log1p(len(grid_data))
            
            # é€šé“1: å¹³å‡éœ‡çº§ï¼ˆåŠ æƒå¹³å‡ï¼Œå¤§éœ‡æƒé‡æ›´é«˜ï¼‰
            weights = np.exp(magnitudes - magnitudes.min())
            features[1] = np.average(magnitudes, weights=weights)
            
            # é€šé“2: æœ€å¤§éœ‡çº§
            features[2] = np.max(magnitudes)
            
            # é€šé“3: èƒ½é‡é‡Šæ”¾ï¼ˆä½¿ç”¨æ›´å‡†ç¡®çš„èƒ½é‡å…¬å¼ï¼‰
            energies = 10 ** (1.5 * magnitudes + 4.8)
            features[3] = np.log10(np.sum(energies) + 1)
            
            # é€šé“4: éœ‡çº§æ ‡å‡†å·®ï¼ˆè¡¡é‡åœ°éœ‡æ´»åŠ¨çš„å¤šæ ·æ€§ï¼‰
            features[4] = np.std(magnitudes) if len(magnitudes) > 1 else 0
            
            # é€šé“5: å¹³å‡æ·±åº¦ï¼ˆå½’ä¸€åŒ–åˆ°0-1ï¼‰
            features[5] = np.mean(depths) / 100.0 if len(depths) > 0 else 0
            
            # é€šé“6: æ—¶é—´å¯†åº¦å˜åŒ–ï¼ˆæ”¹è¿›çš„è®¡ç®—æ–¹æ³•ï¼‰
            features[6] = self._calculate_improved_temporal_density(
                all_data, current_date, lat_idx, lon_idx
            )
            
            # é€šé“7: ç©ºé—´ç›¸å…³æ€§ï¼ˆåªè€ƒè™‘æœ‰æ•ˆé‚»åŸŸï¼‰
            features[7] = self._calculate_improved_spatial_correlation(
                all_data, current_date, lat_idx, lon_idx
            )
        
        return features
    
    def _calculate_improved_temporal_density(self, all_data: pd.DataFrame,
                                          current_date,
                                          lat_idx: int,
                                          lon_idx: int) -> float:
        """
        æ”¹è¿›çš„æ—¶é—´å¯†åº¦è®¡ç®— - ä½¿ç”¨æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡
        """
        try:
            # è·å–è¿‡å»30å¤©çš„æ•°æ®
            time_windows = [7, 14, 30]  # å¤šä¸ªæ—¶é—´çª—å£
            density_scores = []
            
            for window in time_windows:
                window_start = current_date - timedelta(days=window)
                window_data = all_data[
                    (all_data['lat_idx'] == lat_idx) &
                    (all_data['lon_idx'] == lon_idx) &
                    (all_data['date'] >= window_start) &
                    (all_data['date'] < current_date)
                ]
                
                # ä½¿ç”¨æŒ‡æ•°åŠ æƒï¼ˆæœ€è¿‘çš„äº‹ä»¶æƒé‡æ›´é«˜ï¼‰
                if len(window_data) > 0:
                    days_ago = (current_date - window_data['date']).dt.days
                    weights = np.exp(-days_ago / window)
                    weighted_count = np.sum(weights)
                    density_scores.append(weighted_count / window)
                else:
                    density_scores.append(0.0)
            
            # ç»¼åˆå¤šä¸ªæ—¶é—´çª—å£çš„å¯†åº¦
            return np.mean(density_scores)
            
        except:
            return 0.0
        
    def _calculate_improved_spatial_correlation(self, all_data: pd.DataFrame,
                                             current_date,
                                             lat_idx: int,
                                             lon_idx: int) -> float:
        """
        æ”¹è¿›çš„ç©ºé—´ç›¸å…³æ€§è®¡ç®— - åªè€ƒè™‘æœ‰æ•ˆé‚»åŸŸç½‘æ ¼
        """
        try:
            # è·å–æœ‰æ•ˆé‚»åŸŸç½‘æ ¼
            neighbors = self.grid_system.get_neighbor_grids(lat_idx, lon_idx, radius=1)
            
            # è¿‡å»30å¤©çš„æ•°æ®
            recent_data = all_data[
                (all_data['date'] >= current_date - timedelta(days=30)) &
                (all_data['date'] <= current_date)
            ]
            
            if len(recent_data) == 0:
                return 0.0
            
            # è®¡ç®—ä¸­å¿ƒç½‘æ ¼çš„æ´»åŠ¨å¼ºåº¦
            center_activity = len(recent_data[
                (recent_data['lat_idx'] == lat_idx) &
                (recent_data['lon_idx'] == lon_idx)
            ])
            
            # è®¡ç®—é‚»åŸŸæ´»åŠ¨å¼ºåº¦ï¼ˆè·ç¦»åŠ æƒï¼‰
            neighbor_activities = []
            for nlat, nlon in neighbors:
                if nlat == lat_idx and nlon == lon_idx:
                    continue
                
                # è®¡ç®—è·ç¦»
                distance = np.sqrt((nlat - lat_idx)**2 + (nlon - lon_idx)**2)
                weight = 1.0 / (1.0 + distance)
                
                neighbor_count = len(recent_data[
                    (recent_data['lat_idx'] == nlat) &
                    (recent_data['lon_idx'] == nlon)
                ])
                
                neighbor_activities.append(neighbor_count * weight)
            
            # ç©ºé—´ç›¸å…³æ€§åˆ†æ•°
            if neighbor_activities:
                avg_neighbor_activity = np.mean(neighbor_activities)
                correlation = center_activity / (avg_neighbor_activity + 1.0)
                return np.tanh(correlation)  # å½’ä¸€åŒ–åˆ°[-1, 1]
            else:
                return 0.0
                
        except:
            return 0.0

    def _post_process_features(self, features: np.ndarray) -> np.ndarray:
        """
        ç‰¹å¾åå¤„ç†ï¼šå¹³æ»‘å’Œå½’ä¸€åŒ– - åªå¤„ç†æœ‰æ•ˆç½‘æ ¼

        Args:
            features: åŸå§‹ç‰¹å¾ [æ—¶é—´æ­¥é•¿, 10, 8, é€šé“æ•°]

        Returns:
            å¤„ç†åç‰¹å¾
        """
        processed_features = features.copy()
        
        # åˆ›å»ºæœ‰æ•ˆç½‘æ ¼æ©ç 
        valid_mask = np.zeros((10, 8), dtype=bool)
        for lat_idx, lon_idx in self.valid_grids:
            valid_mask[lat_idx, lon_idx] = True
        
        # å¯¹æ¯ä¸ªé€šé“è¿›è¡Œå¤„ç†
        for channel in range(self.num_channels):
            channel_data = processed_features[:, :, :, channel]
            
            # 1. å¯¹ç¨€ç–é€šé“è¿›è¡Œç©ºé—´å¹³æ»‘ï¼ˆåªåœ¨æœ‰æ•ˆç½‘æ ¼å†…ï¼‰
            if self.channels[channel] in ["earthquake_frequency", "energy_release"]:
                for t in range(len(channel_data)):
                    frame = channel_data[t]
                    # åªè€ƒè™‘æœ‰æ•ˆç½‘æ ¼çš„éé›¶å€¼
                    valid_nonzero = np.sum((frame > 0) & valid_mask)
                    valid_total = np.sum(valid_mask)
                    
                    if valid_nonzero < 0.1 * valid_total:  # å¦‚æœæœ‰æ•ˆç½‘æ ¼ä¸­éé›¶å€¼å°‘äº10%
                        # ä½¿ç”¨é«˜æ–¯å¹³æ»‘ï¼Œä½†ä¿æŒæ— æ•ˆç½‘æ ¼ä¸º0
                        smoothed = ndimage.gaussian_filter(frame, sigma=1.0)
                        smoothed[~valid_mask] = 0  # æ— æ•ˆç½‘æ ¼ç½®0
                        processed_features[t, :, :, channel] = smoothed
            
            # 2. è‡ªé€‚åº”å½’ä¸€åŒ–ï¼ˆåªåŸºäºæœ‰æ•ˆç½‘æ ¼çš„å€¼ï¼‰
            channel_data = processed_features[:, :, :, channel]
            valid_data = channel_data[valid_mask.reshape(1, 10, 8).repeat(len(channel_data), axis=0)]
            
            if len(valid_data) > 0 and np.max(valid_data) > 0:
                # ä½¿ç”¨æœ‰æ•ˆæ•°æ®çš„ç™¾åˆ†ä½æ•°
                p95 = np.percentile(valid_data[valid_data > 0], 95)
                p5 = np.percentile(valid_data[valid_data > 0], 5)
                
                if p95 > p5:
                    # è£å‰ªæç«¯å€¼
                    channel_data = np.clip(channel_data, 0, p95)  # ä¿æŒä¸‹é™ä¸º0
                    # å½’ä¸€åŒ–åˆ°[0, 1]ï¼Œæ— æ•ˆç½‘æ ¼ä¿æŒä¸º0
                    normalized = (channel_data - p5) / (p95 - p5)
                    normalized[~valid_mask] = 0
                    processed_features[:, :, :, channel] = normalized
                else:
                    # å¦‚æœæ•°æ®èŒƒå›´å¤ªå°ï¼Œä½¿ç”¨æ ‡å‡†å½’ä¸€åŒ–
                    if np.max(valid_data) > 0:
                        normalized = channel_data / np.max(valid_data)
                        normalized[~valid_mask] = 0
                        processed_features[:, :, :, channel] = normalized
        
        return processed_features

    def extract_statistical_features(self, time_series: np.ndarray) -> np.ndarray:
        """
        ä»æ—¶é—´åºåˆ—ä¸­æå–ç»Ÿè®¡ç‰¹å¾ - åªå¤„ç†æœ‰æ•ˆç½‘æ ¼

        Args:
            time_series: è¾“å…¥æ—¶é—´åºåˆ— [æ—¶é—´æ­¥é•¿, 10, 8, é€šé“æ•°]

        Returns:
            ç»Ÿè®¡ç‰¹å¾ [10, 8, ç»Ÿè®¡ç‰¹å¾æ•°] - æ— æ•ˆç½‘æ ¼å¡«å……0
        """
        if self.verbose and not hasattr(self, '_stat_features_logged'):
            print("\nğŸ“Š æå–ç»Ÿè®¡ç‰¹å¾...")
            self._stat_features_logged = True

        time_steps, _, _, channels = time_series.shape

        # ç»Ÿè®¡ç‰¹å¾ç±»å‹
        stat_features = {
            'mean': 0,
            'std': 1,
            'max': 2,
            'min': 3,
            'trend': 4,     # çº¿æ€§è¶‹åŠ¿
            'volatility': 5  # æ³¢åŠ¨æ€§
        }

        num_stat_features = len(stat_features)
        total_features = channels * num_stat_features

        statistical_features = np.zeros((10, 8, total_features))

        # åªå¤„ç†æœ‰æ•ˆç½‘æ ¼
        for lat_idx, lon_idx in self.valid_grids:
            for ch in range(channels):
                # æå–è¯¥ç½‘æ ¼è¯¥é€šé“çš„æ—¶é—´åºåˆ—
                ts = time_series[:, lat_idx, lon_idx, ch]

                base_idx = ch * num_stat_features

                # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
                statistical_features[lat_idx, lon_idx, base_idx + 0] = np.mean(ts)
                statistical_features[lat_idx, lon_idx, base_idx + 1] = np.std(ts)
                statistical_features[lat_idx, lon_idx, base_idx + 2] = np.max(ts)
                statistical_features[lat_idx, lon_idx, base_idx + 3] = np.min(ts)

                # è¶‹åŠ¿ç‰¹å¾ (çº¿æ€§å›å½’æ–œç‡)
                if np.std(ts) > 1e-6:
                    x = np.arange(len(ts))
                    trend = np.polyfit(x, ts, 1)[0]
                    statistical_features[lat_idx, lon_idx, base_idx + 4] = trend

                # æ³¢åŠ¨æ€§ç‰¹å¾ (å˜å¼‚ç³»æ•°)
                mean_val = np.mean(ts)
                if mean_val > 1e-6:
                    volatility = np.std(ts) / mean_val
                    statistical_features[lat_idx, lon_idx, base_idx + 5] = volatility

        if self.verbose and not hasattr(self, '_stat_features_shape_logged'):
            print(f"  âœ… ç»Ÿè®¡ç‰¹å¾æå–å®Œæˆ: {statistical_features.shape}")
            self._stat_features_shape_logged = True
            
        return statistical_features

    def get_feature_info(self) -> Dict:
        """
        è·å–ç‰¹å¾ä¿¡æ¯

        Returns:
            ç‰¹å¾ä¿¡æ¯å­—å…¸
        """
        return {
            'num_channels': self.num_channels,
            'channel_names': self.channels,
            'grid_shape': (10, 8),  # ä¿æŒå…¼å®¹æ€§
            'num_valid_grids': len(self.valid_grids),
            'valid_grids': self.valid_grids,
            'feature_description': {
                0: "åœ°éœ‡é¢‘ç‡å¯†åº¦ - åæ˜ åœ°éœ‡æ´»è·ƒç¨‹åº¦",
                1: "å¹³å‡éœ‡çº§ - åæ˜ åœ°éœ‡å¼ºåº¦æ°´å¹³",
                2: "æœ€å¤§éœ‡çº§ - åæ˜ æœ€å¼ºåœ°éœ‡å¼ºåº¦",
                3: "èƒ½é‡é‡Šæ”¾ - åŸºäºGutenberg-Richterå…³ç³»",
                4: "éœ‡çº§æ ‡å‡†å·® - åæ˜ éœ‡çº§åˆ†å¸ƒçš„ç¦»æ•£ç¨‹åº¦",
                5: "å¹³å‡æ·±åº¦ - åæ˜ éœ‡æºæ·±åº¦ç‰¹å¾",
                6: "æ—¶é—´å¯†åº¦å˜åŒ– - åæ˜ æ´»è·ƒåº¦å˜åŒ–è¶‹åŠ¿",
                7: "ç©ºé—´ç›¸å…³æ€§ - åæ˜ ä¸é‚»åŸŸçš„å…³è”æ€§"
            }
        }