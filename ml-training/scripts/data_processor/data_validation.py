"""
æ•°æ®éªŒè¯è„šæœ¬
æ£€æŸ¥åŸå§‹æ•°æ®å’Œå¢å¼ºåæ•°æ®çš„å®Œæ•´æ€§å’Œæ­£ç¡®æ€§
"""

import numpy as np
import pandas as pd
import os
import json
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

class DataValidator:
    """æ•°æ®éªŒè¯å™¨"""
    
    def __init__(self, data_dir: str, is_augmented: bool = False):
        """
        åˆå§‹åŒ–éªŒè¯å™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•
            is_augmented: æ˜¯å¦æ˜¯å¢å¼ºåçš„æ•°æ®
        """
        self.data_dir = data_dir
        self.is_augmented = is_augmented
        self.suffix = '_aug' if is_augmented else ''
        self.report = {}
        
    def validate_all(self, save_report: bool = True) -> Dict:
        """æ‰§è¡Œæ‰€æœ‰éªŒè¯"""
        print(f"ğŸ” å¼€å§‹éªŒè¯æ•°æ®: {self.data_dir}")
        print("="*60)
        
        # 1. æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
        print("\n1. æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§...")
        self.check_file_existence()
        
        # 2. æ£€æŸ¥æ•°æ®å½¢çŠ¶
        print("\n2. æ£€æŸ¥æ•°æ®å½¢çŠ¶...")
        self.check_data_shapes()
        
        # 3. æ£€æŸ¥æ•°æ®å€¼èŒƒå›´
        print("\n3. æ£€æŸ¥æ•°æ®å€¼èŒƒå›´...")
        self.check_value_ranges()
        
        # 4. æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
        print("\n4. æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ...")
        self.check_label_distribution()
        
        # 5. æ£€æŸ¥æ—¶é—´æˆ³
        print("\n5. æ£€æŸ¥æ—¶é—´æˆ³...")
        self.check_timestamps()
        
        # 6. æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
        print("\n6. æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§...")
        self.check_data_consistency()
        
        # 7. æ£€æŸ¥å†…å­˜ä½¿ç”¨
        print("\n7. æ£€æŸ¥å†…å­˜ä½¿ç”¨...")
        self.check_memory_usage()
        
        # 8. ç”Ÿæˆå¯è§†åŒ–
        print("\n8. ç”Ÿæˆå¯è§†åŒ–...")
        self.generate_visualizations()
        
        # ä¿å­˜æŠ¥å‘Š
        if save_report:
            report_path = os.path.join(self.data_dir, f'validation_report{self.suffix}.json')
            with open(report_path, 'w') as f:
                json.dump(self.report, f, indent=2, default=str)
            print(f"\nğŸ“„ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        return self.report
    
    def check_file_existence(self):
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        splits = ['train', 'val']
        file_types = ['features', 'stat_features', 'labels', 'timestamps']
        
        self.report['file_existence'] = {}
        all_exist = True
        
        for split in splits:
            self.report['file_existence'][split] = {}
            for file_type in file_types:
                filename = f"{split}_{file_type}{self.suffix}.npy"
                filepath = os.path.join(self.data_dir, filename)
                exists = os.path.exists(filepath)
                self.report['file_existence'][split][file_type] = exists
                
                if exists:
                    size_mb = os.path.getsize(filepath) / (1024 * 1024)
                    print(f"  âœ“ {filename}: {size_mb:.1f} MB")
                else:
                    print(f"  âœ— {filename}: ä¸å­˜åœ¨")
                    all_exist = False
        
        self.report['all_files_exist'] = all_exist
        
    def check_data_shapes(self):
        """æ£€æŸ¥æ•°æ®å½¢çŠ¶"""
        self.report['data_shapes'] = {}
        
        for split in ['train', 'val']:
            print(f"\n  {split}é›†:")
            self.report['data_shapes'][split] = {}
            
            try:
                # åŠ è½½æ•°æ®
                features = np.load(os.path.join(self.data_dir, f"{split}_features{self.suffix}.npy"), mmap_mode='r')
                stat_features = np.load(os.path.join(self.data_dir, f"{split}_stat_features{self.suffix}.npy"), mmap_mode='r')
                labels = np.load(os.path.join(self.data_dir, f"{split}_labels{self.suffix}.npy"), mmap_mode='r')
                timestamps = np.load(os.path.join(self.data_dir, f"{split}_timestamps{self.suffix}.npy"), mmap_mode='r')
                
                # è®°å½•å½¢çŠ¶
                shapes = {
                    'features': features.shape,
                    'stat_features': stat_features.shape,
                    'labels': labels.shape,
                    'timestamps': timestamps.shape
                }
                
                self.report['data_shapes'][split] = {k: list(v) for k, v in shapes.items()}
                
                # æ‰“å°å½¢çŠ¶
                print(f"    features: {features.shape}")
                print(f"    stat_features: {stat_features.shape}")
                print(f"    labels: {labels.shape}")
                print(f"    timestamps: {timestamps.shape}")
                
                # æ£€æŸ¥ä¸€è‡´æ€§
                n_samples = features.shape[0]
                consistent = all([
                    stat_features.shape[0] == n_samples,
                    labels.shape[0] == n_samples,
                    timestamps.shape[0] == n_samples
                ])
                
                if consistent:
                    print(f"    âœ“ æ ·æœ¬æ•°ä¸€è‡´: {n_samples}")
                else:
                    print(f"    âœ— æ ·æœ¬æ•°ä¸ä¸€è‡´!")
                
                self.report['data_shapes'][split]['consistent'] = consistent
                self.report['data_shapes'][split]['n_samples'] = int(n_samples)
                
            except Exception as e:
                print(f"    âŒ åŠ è½½å¤±è´¥: {e}")
                self.report['data_shapes'][split]['error'] = str(e)
    
    def check_value_ranges(self):
        """æ£€æŸ¥æ•°æ®å€¼èŒƒå›´"""
        self.report['value_ranges'] = {}
        
        for split in ['train', 'val']:
            print(f"\n  {split}é›†:")
            self.report['value_ranges'][split] = {}
            
            try:
                # åŠ è½½å°æ‰¹é‡æ•°æ®è¿›è¡Œæ£€æŸ¥
                features = np.load(os.path.join(self.data_dir, f"{split}_features{self.suffix}.npy"), mmap_mode='r')
                stat_features = np.load(os.path.join(self.data_dir, f"{split}_stat_features{self.suffix}.npy"), mmap_mode='r')
                labels = np.load(os.path.join(self.data_dir, f"{split}_labels{self.suffix}.npy"), mmap_mode='r')
                
                # é‡‡æ ·æ£€æŸ¥ï¼ˆæœ€å¤š1000ä¸ªæ ·æœ¬ï¼‰
                n_check = min(1000, len(features))
                indices = np.random.choice(len(features), n_check, replace=False)
                
                # ç‰¹å¾å€¼èŒƒå›´
                feat_sample = features[indices]
                feat_stats = {
                    'min': float(np.min(feat_sample)),
                    'max': float(np.max(feat_sample)),
                    'mean': float(np.mean(feat_sample)),
                    'std': float(np.std(feat_sample))
                }
                print(f"    ç‰¹å¾å€¼: [{feat_stats['min']:.3f}, {feat_stats['max']:.3f}], "
                      f"å‡å€¼={feat_stats['mean']:.3f}, æ ‡å‡†å·®={feat_stats['std']:.3f}")
                
                # ç»Ÿè®¡ç‰¹å¾èŒƒå›´
                stat_sample = stat_features[indices]
                stat_stats = {
                    'min': float(np.min(stat_sample)),
                    'max': float(np.max(stat_sample)),
                    'mean': float(np.mean(stat_sample)),
                    'std': float(np.std(stat_sample))
                }
                print(f"    ç»Ÿè®¡ç‰¹å¾: [{stat_stats['min']:.3f}, {stat_stats['max']:.3f}], "
                      f"å‡å€¼={stat_stats['mean']:.3f}, æ ‡å‡†å·®={stat_stats['std']:.3f}")
                
                # æ ‡ç­¾èŒƒå›´
                label_sample = labels[indices]
                label_stats = {
                    'min': float(np.min(label_sample)),
                    'max': float(np.max(label_sample)),
                    'unique_values': sorted(list(np.unique(label_sample)))[:10]  # æœ€å¤šæ˜¾ç¤º10ä¸ª
                }
                print(f"    æ ‡ç­¾å€¼: [{label_stats['min']:.3f}, {label_stats['max']:.3f}]")
                
                # æ£€æŸ¥å¼‚å¸¸å€¼
                has_nan = bool(np.any(np.isnan(feat_sample)) or np.any(np.isnan(stat_sample)))
                has_inf = bool(np.any(np.isinf(feat_sample)) or np.any(np.isinf(stat_sample)))
                
                if has_nan:
                    print(f"    âš ï¸  åŒ…å«NaNå€¼!")
                if has_inf:
                    print(f"    âš ï¸  åŒ…å«Infå€¼!")
                
                self.report['value_ranges'][split] = {
                    'features': feat_stats,
                    'stat_features': stat_stats,
                    'labels': label_stats,
                    'has_nan': has_nan,
                    'has_inf': has_inf
                }
                
            except Exception as e:
                print(f"    âŒ æ£€æŸ¥å¤±è´¥: {e}")
                self.report['value_ranges'][split]['error'] = str(e)
    
    def check_label_distribution(self):
        """æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ"""
        self.report['label_distribution'] = {}
        
        task_names = [
            "7å¤©_M3.0-4.5", "7å¤©_M4.5-5.5", "7å¤©_M5.5-6.5", "7å¤©_M6.5+",
            "14å¤©_M3.0-4.5", "14å¤©_M4.5-5.5", "14å¤©_M5.5-6.5", "14å¤©_M6.5+",
            "30å¤©_M3.0-4.5", "30å¤©_M4.5-5.5", "30å¤©_M5.5-6.5", "30å¤©_M6.5+"
        ]
        
        for split in ['train', 'val']:
            print(f"\n  {split}é›†æ ‡ç­¾åˆ†å¸ƒ:")
            self.report['label_distribution'][split] = {}
            
            try:
                labels = np.load(os.path.join(self.data_dir, f"{split}_labels{self.suffix}.npy"))
                
                # æ•´ä½“æ­£æ ·æœ¬ç‡
                positive_mask = np.any(labels > 0.5, axis=1)
                overall_positive_ratio = np.mean(positive_mask)
                print(f"    æ•´ä½“æ­£æ ·æœ¬ç‡: {overall_positive_ratio:.2%} ({np.sum(positive_mask)}/{len(labels)})")
                
                # æ¯ä¸ªä»»åŠ¡çš„æ­£æ ·æœ¬ç‡
                task_stats = []
                print(f"    ä»»åŠ¡çº§åˆ«æ­£æ ·æœ¬ç‡:")
                for i in range(labels.shape[1]):
                    positive_count = np.sum(labels[:, i] > 0.5)
                    positive_ratio = np.mean(labels[:, i] > 0.5)
                    
                    task_stat = {
                        'task_id': i,
                        'task_name': task_names[i] if i < len(task_names) else f"ä»»åŠ¡{i}",
                        'positive_count': int(positive_count),
                        'positive_ratio': float(positive_ratio)
                    }
                    task_stats.append(task_stat)
                    
                    if positive_ratio > 0:
                        print(f"      ä»»åŠ¡{i:2d} ({task_names[i]:12s}): {positive_ratio:6.2%} ({positive_count:5d}ä¸ª)")
                    else:
                        print(f"      ä»»åŠ¡{i:2d} ({task_names[i]:12s}): {positive_ratio:6.2%} (æ— æ­£æ ·æœ¬)")
                
                self.report['label_distribution'][split] = {
                    'overall_positive_ratio': float(overall_positive_ratio),
                    'overall_positive_count': int(np.sum(positive_mask)),
                    'task_stats': task_stats
                }
                
            except Exception as e:
                print(f"    âŒ æ£€æŸ¥å¤±è´¥: {e}")
                self.report['label_distribution'][split]['error'] = str(e)
    
    def check_timestamps(self):
        """æ£€æŸ¥æ—¶é—´æˆ³"""
        self.report['timestamps'] = {}
        
        for split in ['train', 'val']:
            print(f"\n  {split}é›†æ—¶é—´æˆ³:")
            self.report['timestamps'][split] = {}
            
            try:
                timestamps_raw = np.load(os.path.join(self.data_dir, f"{split}_timestamps{self.suffix}.npy"))
                
                # è½¬æ¢æ—¶é—´æˆ³
                if timestamps_raw.dtype == np.float64:
                    timestamps = pd.to_datetime(timestamps_raw, unit='s')
                else:
                    timestamps = pd.to_datetime(timestamps_raw)
                
                # æ—¶é—´èŒƒå›´
                time_range = {
                    'start': str(timestamps.min()),
                    'end': str(timestamps.max()),
                    'duration_days': (timestamps.max() - timestamps.min()).days
                }
                
                print(f"    æ—¶é—´èŒƒå›´: {time_range['start']} è‡³ {time_range['end']}")
                print(f"    è·¨åº¦: {time_range['duration_days']} å¤©")
                
                # æ£€æŸ¥æ—¶é—´é¡ºåº
                is_sorted = all(timestamps[i] <= timestamps[i+1] for i in range(len(timestamps)-1))
                print(f"    æ—¶é—´é¡ºåº: {'âœ“ å·²æ’åº' if is_sorted else 'âœ— æœªæ’åº'}")
                
                # æ£€æŸ¥é‡å¤
                n_unique = len(np.unique(timestamps))
                n_duplicates = len(timestamps) - n_unique
                print(f"    é‡å¤æ—¶é—´æˆ³: {n_duplicates} ä¸ª")
                
                self.report['timestamps'][split] = {
                    'time_range': time_range,
                    'is_sorted': bool(is_sorted),
                    'n_unique': int(n_unique),
                    'n_duplicates': int(n_duplicates)
                }
                
            except Exception as e:
                print(f"    âŒ æ£€æŸ¥å¤±è´¥: {e}")
                self.report['timestamps'][split]['error'] = str(e)
    
    def check_data_consistency(self):
        """æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§"""
        print("\n  æ£€æŸ¥è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„ä¸€è‡´æ€§...")
        self.report['consistency'] = {}
        
        try:
            # åŠ è½½è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„ç‰¹å¾å½¢çŠ¶
            train_features = np.load(os.path.join(self.data_dir, f"train_features{self.suffix}.npy"), mmap_mode='r')
            val_features = np.load(os.path.join(self.data_dir, f"val_features{self.suffix}.npy"), mmap_mode='r')
            
            # æ£€æŸ¥ç‰¹å¾ç»´åº¦æ˜¯å¦ä¸€è‡´
            train_shape = train_features.shape[1:]  # é™¤äº†æ ·æœ¬æ•°ä¹‹å¤–çš„ç»´åº¦
            val_shape = val_features.shape[1:]
            
            shape_consistent = train_shape == val_shape
            print(f"    ç‰¹å¾ç»´åº¦ä¸€è‡´æ€§: {'âœ“' if shape_consistent else 'âœ—'}")
            
            if not shape_consistent:
                print(f"      è®­ç»ƒé›†: {train_shape}")
                print(f"      éªŒè¯é›†: {val_shape}")
            
            # æ£€æŸ¥æ•°æ®åˆ†å¸ƒï¼ˆé‡‡æ ·ï¼‰
            n_check = min(100, len(train_features), len(val_features))
            train_sample = train_features[np.random.choice(len(train_features), n_check)]
            val_sample = val_features[np.random.choice(len(val_features), n_check)]
            
            train_mean = np.mean(train_sample)
            val_mean = np.mean(val_sample)
            mean_diff = abs(train_mean - val_mean)
            
            print(f"    æ•°æ®åˆ†å¸ƒå·®å¼‚:")
            print(f"      è®­ç»ƒé›†å‡å€¼: {train_mean:.4f}")
            print(f"      éªŒè¯é›†å‡å€¼: {val_mean:.4f}")
            print(f"      å·®å¼‚: {mean_diff:.4f} {'âœ“ æ­£å¸¸' if mean_diff < 0.5 else 'âš ï¸ è¾ƒå¤§'}")
            
            self.report['consistency'] = {
                'shape_consistent': bool(shape_consistent),
                'train_shape': list(train_shape),
                'val_shape': list(val_shape),
                'train_mean': float(train_mean),
                'val_mean': float(val_mean),
                'mean_difference': float(mean_diff)
            }
            
        except Exception as e:
            print(f"    âŒ æ£€æŸ¥å¤±è´¥: {e}")
            self.report['consistency']['error'] = str(e)
    
    def check_memory_usage(self):
        """æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        self.report['memory_usage'] = {}
        total_size_gb = 0
        
        for split in ['train', 'val']:
            split_size = 0
            self.report['memory_usage'][split] = {}
            
            for file_type in ['features', 'stat_features', 'labels', 'timestamps']:
                filename = f"{split}_{file_type}{self.suffix}.npy"
                filepath = os.path.join(self.data_dir, filename)
                
                if os.path.exists(filepath):
                    size_mb = os.path.getsize(filepath) / (1024 * 1024)
                    split_size += size_mb
                    self.report['memory_usage'][split][file_type] = float(size_mb)
            
            self.report['memory_usage'][split]['total_mb'] = float(split_size)
            total_size_gb += split_size / 1024
        
        self.report['memory_usage']['total_gb'] = float(total_size_gb)
        print(f"    æ€»å ç”¨ç©ºé—´: {total_size_gb:.2f} GB")
    
    def generate_visualizations(self):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        try:
            # åˆ›å»ºå›¾è¡¨
            fig, axes = plt.subplots(2, 2, figsize=(12, 10))
            fig.suptitle(f'æ•°æ®éªŒè¯å¯è§†åŒ– - {self.data_dir}', fontsize=16)
            
            # 1. æ ‡ç­¾åˆ†å¸ƒçƒ­åŠ›å›¾
            ax = axes[0, 0]
            train_labels = np.load(os.path.join(self.data_dir, f"train_labels{self.suffix}.npy"))
            task_positive_rates = [np.mean(train_labels[:, i] > 0.5) for i in range(train_labels.shape[1])]
            
            # é‡å¡‘ä¸º3x4çŸ©é˜µï¼ˆ3ä¸ªæ—¶é—´çª—å£ï¼Œ4ä¸ªéœ‡çº§èŒƒå›´ï¼‰
            heatmap_data = np.array(task_positive_rates).reshape(3, 4)
            sns.heatmap(heatmap_data, annot=True, fmt='.1%', cmap='YlOrRd', ax=ax)
            ax.set_title('ä»»åŠ¡æ­£æ ·æœ¬ç‡çƒ­åŠ›å›¾')
            ax.set_xticklabels(['M3.0-4.5', 'M4.5-5.5', 'M5.5-6.5', 'M6.5+'])
            ax.set_yticklabels(['7å¤©', '14å¤©', '30å¤©'])
            
            # 2. æ ·æœ¬æ•°é‡å¯¹æ¯”
            ax = axes[0, 1]
            if self.is_augmented and 'data_shapes' in self.report:
                train_samples = self.report['data_shapes']['train'].get('n_samples', 0)
                val_samples = self.report['data_shapes']['val'].get('n_samples', 0)
                
                ax.bar(['è®­ç»ƒé›†', 'éªŒè¯é›†'], [train_samples, val_samples])
                ax.set_title('æ•°æ®é›†æ ·æœ¬æ•°é‡')
                ax.set_ylabel('æ ·æœ¬æ•°')
                
                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                for i, v in enumerate([train_samples, val_samples]):
                    ax.text(i, v + v*0.01, str(v), ha='center')
            
            # 3. ç‰¹å¾å€¼åˆ†å¸ƒï¼ˆé‡‡æ ·ï¼‰
            ax = axes[1, 0]
            features = np.load(os.path.join(self.data_dir, f"train_features{self.suffix}.npy"), mmap_mode='r')
            sample_features = features[np.random.choice(len(features), min(1000, len(features)))]
            ax.hist(sample_features.flatten(), bins=50, alpha=0.7, edgecolor='black')
            ax.set_title('ç‰¹å¾å€¼åˆ†å¸ƒï¼ˆé‡‡æ ·ï¼‰')
            ax.set_xlabel('ç‰¹å¾å€¼')
            ax.set_ylabel('é¢‘æ•°')
            
            # 4. æ—¶é—´åˆ†å¸ƒ
            ax = axes[1, 1]
            timestamps_raw = np.load(os.path.join(self.data_dir, f"train_timestamps{self.suffix}.npy"))
            if timestamps_raw.dtype == np.float64:
                timestamps = pd.to_datetime(timestamps_raw, unit='s')
            else:
                timestamps = pd.to_datetime(timestamps_raw)
            
            # æŒ‰æœˆç»Ÿè®¡
            monthly_counts = pd.Series(timestamps).dt.to_period('M').value_counts().sort_index()
            monthly_counts.plot(kind='line', ax=ax, marker='o')
            ax.set_title('æ ·æœ¬æ—¶é—´åˆ†å¸ƒ')
            ax.set_xlabel('æœˆä»½')
            ax.set_ylabel('æ ·æœ¬æ•°')
            ax.tick_params(axis='x', rotation=45)
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            viz_path = os.path.join(self.data_dir, f'validation_visualization{self.suffix}.png')
            plt.savefig(viz_path, dpi=150, bbox_inches='tight')
            plt.close()
            
            print(f"    âœ“ å¯è§†åŒ–å·²ä¿å­˜: {viz_path}")
            self.report['visualization_saved'] = True
            
        except Exception as e:
            print(f"    âŒ ç”Ÿæˆå¯è§†åŒ–å¤±è´¥: {e}")
            self.report['visualization_error'] = str(e)

def compare_datasets(original_dir: str, augmented_dir: str):
    """æ¯”è¾ƒåŸå§‹æ•°æ®å’Œå¢å¼ºæ•°æ®"""
    print("\nğŸ“Š æ¯”è¾ƒåŸå§‹æ•°æ®å’Œå¢å¼ºæ•°æ®")
    print("="*60)
    
    # åŠ è½½å¢å¼ºé…ç½®
    config_path = os.path.join(augmented_dir, 'augmentation_config.json')
    if os.path.exists(config_path):
        with open(config_path, 'r') as f:
            aug_config = json.load(f)
        print(f"\nå¢å¼ºé…ç½®:")
        print(f"  å¢å¼ºå› å­: {aug_config.get('augmentation_factor', 'N/A')}")
        print(f"  æœ€å¤§æ­£æ ·æœ¬æ•°: {aug_config.get('max_positive_samples', 'N/A')}")
    
    # æ¯”è¾ƒç»Ÿè®¡ä¿¡æ¯
    for split in ['train', 'val']:
        print(f"\n{split}é›†å¯¹æ¯”:")
        
        try:
            # åŸå§‹æ•°æ®
            orig_labels = np.load(os.path.join(original_dir, f"{split}_labels.npy"))
            orig_features = np.load(os.path.join(original_dir, f"{split}_features.npy"), mmap_mode='r')
            
            # å¢å¼ºæ•°æ®
            aug_labels = np.load(os.path.join(augmented_dir, f"{split}_labels_aug.npy"))
            aug_features = np.load(os.path.join(augmented_dir, f"{split}_features_aug.npy"), mmap_mode='r')
            
            # æ ·æœ¬æ•°å¯¹æ¯”
            print(f"  æ ·æœ¬æ•°: {len(orig_labels)} â†’ {len(aug_labels)} "
                  f"(å¢åŠ  {len(aug_labels) - len(orig_labels)})")
            
            # æ­£æ ·æœ¬ç‡å¯¹æ¯”
            orig_positive_rate = np.mean(np.any(orig_labels > 0.5, axis=1))
            aug_positive_rate = np.mean(np.any(aug_labels > 0.5, axis=1))
            print(f"  æ­£æ ·æœ¬ç‡: {orig_positive_rate:.2%} â†’ {aug_positive_rate:.2%} "
                  f"(+{(aug_positive_rate - orig_positive_rate):.2%})")
            
            # æ–‡ä»¶å¤§å°å¯¹æ¯”
            orig_size = orig_features.nbytes / (1024**3)
            aug_size = aug_features.nbytes / (1024**3)
            print(f"  ç‰¹å¾æ–‡ä»¶å¤§å°: {orig_size:.2f}GB â†’ {aug_size:.2f}GB")
            
        except Exception as e:
            print(f"  âŒ æ¯”è¾ƒå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ•°æ®éªŒè¯å·¥å…·')
    parser.add_argument('--data_dir', type=str, default='../../data/processed_grid',
                       help='æ•°æ®ç›®å½•')
    parser.add_argument('--augmented_dir', type=str, default='../../data/augmented_data',
                       help='å¢å¼ºæ•°æ®ç›®å½•')
    parser.add_argument('--check_augmented', action='store_true',
                       help='æ£€æŸ¥å¢å¼ºåçš„æ•°æ®')
    parser.add_argument('--compare', action='store_true',
                       help='æ¯”è¾ƒåŸå§‹å’Œå¢å¼ºæ•°æ®')
    parser.add_argument('--no_viz', action='store_true',
                       help='ä¸ç”Ÿæˆå¯è§†åŒ–')
    
    args = parser.parse_args()
    
    print("ğŸ” åœ°éœ‡æ•°æ®éªŒè¯å·¥å…·")
    print("="*60)
    
    if args.compare:
        # æ¯”è¾ƒæ¨¡å¼
        compare_datasets(args.data_dir, args.augmented_dir)
    else:
        # éªŒè¯æ¨¡å¼
        if args.check_augmented:
            data_dir = args.augmented_dir
            is_augmented = True
        else:
            data_dir = args.data_dir
            is_augmented = False
        
        validator = DataValidator(data_dir, is_augmented)
        
        # å¦‚æœä¸è¦å¯è§†åŒ–ï¼Œä¸´æ—¶ä¿®æ”¹æ–¹æ³•
        if args.no_viz:
            validator.generate_visualizations = lambda: print("  è·³è¿‡å¯è§†åŒ–ç”Ÿæˆ")
        
        report = validator.validate_all()
        
        # æ‰“å°æ€»ç»“
        print("\n" + "="*60)
        print("ğŸ“‹ éªŒè¯æ€»ç»“:")
        
        if report.get('all_files_exist'):
            print("  âœ“ æ‰€æœ‰æ–‡ä»¶éƒ½å­˜åœ¨")
        else:
            print("  âœ— ç¼ºå°‘éƒ¨åˆ†æ–‡ä»¶")
        
        if 'memory_usage' in report:
            print(f"  ğŸ’¾ æ€»å ç”¨ç©ºé—´: {report['memory_usage']['total_gb']:.2f} GB")
        
        # æ£€æŸ¥é—®é¢˜
        issues = []
        
        # æ£€æŸ¥NaN/Inf
        for split in ['train', 'val']:
            if 'value_ranges' in report and split in report['value_ranges']:
                if report['value_ranges'][split].get('has_nan'):
                    issues.append(f"{split}é›†åŒ…å«NaNå€¼")
                if report['value_ranges'][split].get('has_inf'):
                    issues.append(f"{split}é›†åŒ…å«Infå€¼")
        
        # æ£€æŸ¥ä¸€è‡´æ€§
        if 'consistency' in report and not report['consistency'].get('shape_consistent', True):
            issues.append("è®­ç»ƒé›†å’ŒéªŒè¯é›†å½¢çŠ¶ä¸ä¸€è‡´")
        
        if issues:
            print("\nâš ï¸  å‘ç°çš„é—®é¢˜:")
            for issue in issues:
                print(f"  - {issue}")
        else:
            print("\nâœ… æ•°æ®éªŒè¯é€šè¿‡ï¼Œæœªå‘ç°æ˜æ˜¾é—®é¢˜")

if __name__ == "__main__":
    main()