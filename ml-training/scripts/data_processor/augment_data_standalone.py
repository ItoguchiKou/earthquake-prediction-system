"""
ç‹¬ç«‹çš„æ•°æ®å¢å¼ºè„šæœ¬
å¯ä»¥é¢„å…ˆè¿è¡Œï¼Œç”Ÿæˆå¢å¼ºåçš„æ•°æ®é›†
"""

import numpy as np
import pandas as pd
import os
import sys
import time
from datetime import datetime
from typing import Dict, Optional
import argparse
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from data_aggregation import apply_data_augmentation_strategy

def check_data_statistics(data_dir: str) -> Dict:
    """æ£€æŸ¥æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
    print("\nğŸ“Š æ£€æŸ¥åŸå§‹æ•°æ®...")
    
    # åŠ è½½æ ‡ç­¾ä»¥è®¡ç®—æ­£æ ·æœ¬
    train_labels_path = os.path.join(data_dir, "train_labels.npy")
    train_labels = np.load(train_labels_path)
    
    # è¯¦ç»†åˆ†ææ¯ä¸ªä»»åŠ¡çš„æ­£æ ·æœ¬
    print("\nğŸ“Š å„ä»»åŠ¡æ­£æ ·æœ¬ç»Ÿè®¡:")
    task_names = [
        "7d_M3-4.5", "7d_M4.5-5.5", "7d_M5.5-6.5", "7d_M6.5+",
        "14d_M3-4.5", "14d_M4.5-5.5", "14d_M5.5-6.5", "14d_M6.5+",
        "30d_M3-4.5", "30d_M4.5-5.5", "30d_M5.5-6.5", "30d_M6.5+"
    ]
    
    task_positive_counts = {}
    for task_idx in range(12):
        positive_count = np.sum(train_labels[:, task_idx] > 0.5)
        positive_ratio = positive_count / len(train_labels)
        print(f"  ä»»åŠ¡{task_idx:2d} ({task_names[task_idx]}): {positive_count:4d} ({positive_ratio:.2%})")
        task_positive_counts[f'task_{task_idx}'] = {
            'count': int(positive_count),
            'ratio': float(positive_ratio)
        }
    
    # åˆ†ææ ·æœ¬çº§åˆ«çš„æ­£æ ·æœ¬ï¼ˆä»»ä½•ä»»åŠ¡ä¸ºæ­£ï¼‰
    sample_has_positive = np.any(train_labels > 0.5, axis=1)
    positive_samples = np.sum(sample_has_positive)
    
    print(f"\nğŸ“Š æ ·æœ¬çº§åˆ«ç»Ÿè®¡:")
    print(f"  æœ‰æ­£æ ‡ç­¾çš„æ ·æœ¬æ•°: {positive_samples} ({positive_samples/len(train_labels):.2%})")
    
    # åˆ†æå¤šä»»åŠ¡æ­£æ ·æœ¬
    positive_task_counts_per_sample = np.sum(train_labels > 0.5, axis=1)
    for i in range(1, 13):
        count = np.sum(positive_task_counts_per_sample == i)
        if count > 0:
            print(f"  æœ‰{i}ä¸ªæ­£ä»»åŠ¡çš„æ ·æœ¬: {count}")
    
    # ä¼°ç®—å†…å­˜ä½¿ç”¨
    train_features = np.load(os.path.join(data_dir, "train_features.npy"), mmap_mode='r')
    sample_size_mb = train_features.nbytes / (1024 * 1024) / len(train_features)
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_samples': len(train_labels),
        'positive_samples': int(positive_samples),
        'positive_ratio': float(positive_samples / len(train_labels)),
        'task_positive_counts': task_positive_counts,
        'sample_size_mb': float(sample_size_mb)
    }
    
    return stats

def augment_dataset(data_dir: str, 
                   output_dir: str,
                   augmentation_factor: int = 10,
                   max_positive_samples: int = 1000,
                   validate: bool = True) -> None:
    """
    å¢å¼ºæ•°æ®é›†å¹¶ä¿å­˜
    
    Args:
        data_dir: åŸå§‹æ•°æ®ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        augmentation_factor: å¢å¼ºå€æ•°
        max_positive_samples: æœ€å¤šå¢å¼ºçš„æ­£æ ·æœ¬æ•°
        validate: æ˜¯å¦åŒæ—¶å¢å¼ºéªŒè¯é›†
    """
    print(f"\nğŸš€ å¼€å§‹æ•°æ®å¢å¼ºæµç¨‹")
    print(f"  åŸå§‹æ•°æ®ç›®å½•: {data_dir}")
    print(f"  è¾“å‡ºç›®å½•: {output_dir}")
    print(f"  å¢å¼ºå€æ•°: {augmentation_factor}")
    print(f"  æœ€å¤§æ­£æ ·æœ¬æ•°: {max_positive_samples}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜é…ç½®
    config = {
        'augmentation_factor': augmentation_factor,
        'max_positive_samples': max_positive_samples,
        'timestamp': datetime.now().isoformat(),
        'source_dir': data_dir
    }
    
    with open(os.path.join(output_dir, 'augmentation_config.json'), 'w') as f:
        json.dump(config, f, indent=2)
    
    # å¤„ç†è®­ç»ƒé›†
    print("\nğŸ“‚ å¤„ç†è®­ç»ƒé›†...")
    process_split(
        data_dir=data_dir,
        output_dir=output_dir,
        split='train',
        augmentation_factor=augmentation_factor,
        max_positive_samples=max_positive_samples,
        apply_augmentation=True
    )
    
    # å¤„ç†éªŒè¯é›†ï¼ˆä¸å¢å¼ºï¼‰
    if validate:
        print("\nğŸ“‚ å¤„ç†éªŒè¯é›†...")
        process_split(
            data_dir=data_dir,
            output_dir=output_dir,
            split='val',
            augmentation_factor=1,
            max_positive_samples=None,
            apply_augmentation=False
        )
    
    print("\nâœ… æ•°æ®å¢å¼ºå®Œæˆï¼")
    print(f"  å¢å¼ºåçš„æ•°æ®ä¿å­˜åœ¨: {output_dir}")

def process_split(data_dir: str,
                 output_dir: str,
                 split: str,
                 augmentation_factor: int,
                 max_positive_samples: Optional[int],
                 apply_augmentation: bool) -> None:
    """å¤„ç†å•ä¸ªæ•°æ®é›†åˆ†å‰²"""
    
    # æ–‡ä»¶è·¯å¾„
    features_path = os.path.join(data_dir, f"{split}_features.npy")
    stat_features_path = os.path.join(data_dir, f"{split}_stat_features.npy")
    labels_path = os.path.join(data_dir, f"{split}_labels.npy")
    timestamps_path = os.path.join(data_dir, f"{split}_timestamps.npy")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    required_files = [features_path, stat_features_path, labels_path, timestamps_path]
    missing_files = [f for f in required_files if not os.path.exists(f)]
    
    if missing_files:
        print(f"âŒ ç¼ºå°‘æ–‡ä»¶: {missing_files}")
        return
    
    # åŠ è½½æ•°æ®
    print(f"  åŠ è½½ {split} æ•°æ®...")
    start_time = time.time()
    
    features = np.load(features_path)
    stat_features = np.load(stat_features_path)
    labels = np.load(labels_path)
    timestamps_raw = np.load(timestamps_path)
    
    print(f"  âœ“ åŠ è½½å®Œæˆ ({time.time() - start_time:.1f}ç§’)")
    print(f"  åŸå§‹å½¢çŠ¶: features={features.shape}, labels={labels.shape}")
    
    original_len = len(features)
    
    # åº”ç”¨æ•°æ®å¢å¼º
    if apply_augmentation:
        print(f"\n  åº”ç”¨æ•°æ®å¢å¼º...")
        
        # ä½¿ç”¨æ”¹è¿›çš„æ•°æ®å¢å¼ºæ–¹æ³• - ä¿®å¤å‚æ•°åç§°
        features_aug, labels_aug = apply_data_augmentation_strategy(
            features, 
            labels, 
            augmentation_factor=augmentation_factor,
            probability_threshold=0.2,  # åŸºäºæ¦‚ç‡å€¼è€ŒéäºŒå€¼åŒ–
            max_augment_samples=max_positive_samples,  # ä¿®æ­£å‚æ•°å
            preserve_negatives=True  # ç¡®ä¿ä¿ç•™è´Ÿæ ·æœ¬
        )
        
        # åŒæ­¥å¢å¼ºç»Ÿè®¡ç‰¹å¾å’Œæ—¶é—´æˆ³
        print(f"  åŒæ­¥å¢å¼ºç»Ÿè®¡ç‰¹å¾...")
        num_augmented = len(features_aug) - len(features)
        
        if num_augmented > 0:
            # æ‰¾å‡ºè¢«å¢å¼ºçš„åŸå§‹æ ·æœ¬ç´¢å¼•ï¼ˆåŸºäºå¹³å‡æ¦‚ç‡è€ŒéäºŒå€¼åŒ–ï¼‰
            mean_probs = np.mean(labels, axis=1)
            high_prob_mask = mean_probs > 0.2  # ä½¿ç”¨ä¸å¢å¼ºç›¸åŒçš„é˜ˆå€¼
            high_prob_indices = np.where(high_prob_mask)[0]
            
            if max_positive_samples is not None and len(high_prob_indices) > max_positive_samples:
                # æŒ‰æ¦‚ç‡æ’åºï¼Œé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„æ ·æœ¬
                sorted_indices = high_prob_indices[np.argsort(mean_probs[high_prob_indices])[::-1]]
                high_prob_indices = sorted_indices[:max_positive_samples]
            
            # ç”Ÿæˆå¢å¼ºçš„ç»Ÿè®¡ç‰¹å¾
            augmented_stat_features = []
            
            for idx in high_prob_indices:
                base_stat = stat_features[idx]
                for _ in range(augmentation_factor):
                    if len(augmented_stat_features) >= num_augmented:
                        break
                    noise = np.random.normal(0, 0.03, base_stat.shape)
                    aug_stat = base_stat * (1 + noise)
                    aug_stat = np.maximum(aug_stat, 0)
                    augmented_stat_features.append(aug_stat)
            
            augmented_stat_features = np.array(augmented_stat_features[:num_augmented])
            stat_features_aug = np.concatenate([stat_features, augmented_stat_features], axis=0)
            
            # åŒæ­¥æ—¶é—´æˆ³
            print(f"  åŒæ­¥æ—¶é—´æˆ³...")
            if timestamps_raw.dtype == np.float64:
                timestamps = pd.to_datetime(timestamps_raw, unit='s')
            else:
                timestamps = pd.to_datetime(timestamps_raw)
            
            # ä¸ºå¢å¼ºçš„æ ·æœ¬å¤åˆ¶æ—¶é—´æˆ³
            augmented_timestamps = []
            for idx in high_prob_indices:
                for _ in range(augmentation_factor):
                    if len(augmented_timestamps) >= num_augmented:
                        break
                    augmented_timestamps.append(timestamps[idx])
            
            # åˆå¹¶æ—¶é—´æˆ³
            timestamps_aug = pd.DatetimeIndex(list(timestamps) + augmented_timestamps[:num_augmented])
            
            if timestamps_raw.dtype == np.float64:
                timestamps_aug_raw = timestamps_aug.astype(np.int64) / 1e9
            else:
                timestamps_aug_raw = timestamps_aug.values
        else:
            stat_features_aug = stat_features
            timestamps_aug_raw = timestamps_raw
    else:
        # ä¸å¢å¼ºï¼Œç›´æ¥å¤åˆ¶
        features_aug = features
        stat_features_aug = stat_features
        labels_aug = labels
        timestamps_aug_raw = timestamps_raw
    
    # ä¿å­˜å¢å¼ºåçš„æ•°æ®
    print(f"\n  ä¿å­˜å¢å¼ºåçš„æ•°æ®...")
    
    output_features_path = os.path.join(output_dir, f"{split}_features_aug.npy")
    output_stat_features_path = os.path.join(output_dir, f"{split}_stat_features_aug.npy")
    output_labels_path = os.path.join(output_dir, f"{split}_labels_aug.npy")
    output_timestamps_path = os.path.join(output_dir, f"{split}_timestamps_aug.npy")
    
    np.save(output_features_path, features_aug)
    np.save(output_stat_features_path, stat_features_aug)
    np.save(output_labels_path, labels_aug)
    np.save(output_timestamps_path, timestamps_aug_raw)
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    # è®¡ç®—åŸºäºæ¦‚ç‡å€¼çš„ç»Ÿè®¡ï¼ˆè€ŒéäºŒå€¼åŒ–ï¼‰
    mean_probs_before = np.mean(labels)
    mean_probs_after = np.mean(labels_aug)
    high_prob_before = np.mean(np.mean(labels, axis=1) > 0.2)
    high_prob_after = np.mean(np.mean(labels_aug, axis=1) > 0.2)
    
    stats = {
        'split': split,
        'original_samples': original_len,
        'augmented_samples': len(features_aug),
        'augmentation_ratio': len(features_aug) / original_len,
        'mean_probability_before': float(mean_probs_before),
        'mean_probability_after': float(mean_probs_after),
        'high_prob_ratio_before': float(high_prob_before),
        'high_prob_ratio_after': float(high_prob_after),
        'file_sizes_mb': {
            'features': os.path.getsize(output_features_path) / (1024 * 1024),
            'stat_features': os.path.getsize(output_stat_features_path) / (1024 * 1024),
            'labels': os.path.getsize(output_labels_path) / (1024 * 1024),
            'timestamps': os.path.getsize(output_timestamps_path) / (1024 * 1024)
        }
    }
    
    stats_path = os.path.join(output_dir, f"{split}_augmentation_stats.json")
    with open(stats_path, 'w') as f:
        json.dump(stats, f, indent=2)
    
    print(f"  âœ“ ä¿å­˜å®Œæˆ")
    print(f"  æœ€ç»ˆå½¢çŠ¶: features={features_aug.shape}, labels={labels_aug.shape}")
    print(f"  å¹³å‡æ¦‚ç‡: {stats['mean_probability_before']:.4f} â†’ {stats['mean_probability_after']:.4f}")
    print(f"  é«˜æ¦‚ç‡æ ·æœ¬æ¯”ä¾‹: {stats['high_prob_ratio_before']:.2%} â†’ {stats['high_prob_ratio_after']:.2%}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç‹¬ç«‹æ•°æ®å¢å¼ºå·¥å…·')
    parser.add_argument('--data_dir', type=str, default='../../data/processed_grid',
                       help='åŸå§‹æ•°æ®ç›®å½•')
    parser.add_argument('--output_dir', type=str, default='../../data/augmented_data',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--augment_factor', type=int, default=6,
                       help='å¢å¼ºå€æ•°')
    parser.add_argument('--max_positive', type=int, default=1000,
                       help='æœ€å¤§æ­£æ ·æœ¬æ•°é‡')
    parser.add_argument('--check_only', action='store_true',
                       help='ä»…æ£€æŸ¥æ•°æ®ç»Ÿè®¡ä¿¡æ¯')
    parser.add_argument('--no_validate', action='store_true',
                       help='ä¸å¤„ç†éªŒè¯é›†')
    
    args = parser.parse_args()
    
    print("ğŸŒ åœ°éœ‡æ•°æ®å¢å¼ºå·¥å…·")
    print("="*60)
    
    # æ£€æŸ¥æ•°æ®ç»Ÿè®¡
    stats = check_data_statistics(args.data_dir)
    
    print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"  æ€»æ ·æœ¬æ•°: {stats['total_samples']:,}")
    print(f"  æ­£æ ·æœ¬æ•°: {stats['positive_samples']:,} ({stats['positive_ratio']:.2%})")
    print(f"  æ¯ä¸ªæ ·æœ¬å¤§å°: {stats['sample_size_mb']:.2f} MB")
    
    # ä¼°ç®—å¢å¼ºåçš„å¤§å°
    estimated_positive = min(stats['positive_samples'], args.max_positive)
    estimated_total = stats['total_samples'] + estimated_positive * args.augment_factor
    estimated_size_gb = estimated_total * stats['sample_size_mb'] / 1024
    
    print(f"\nğŸ“ˆ å¢å¼ºé¢„ä¼°:")
    print(f"  å°†å¢å¼ºæ­£æ ·æœ¬æ•°: {estimated_positive:,}")
    print(f"  æ–°å¢æ ·æœ¬æ•°: {estimated_positive * args.augment_factor:,}")
    print(f"  å¢å¼ºåæ€»æ ·æœ¬æ•°: {estimated_total:,}")
    print(f"  é¢„è®¡å ç”¨ç©ºé—´: {estimated_size_gb:.1f} GB")
    
    if args.check_only:
        print("\nä»…æ£€æŸ¥æ¨¡å¼ï¼Œé€€å‡ºã€‚")
        return
    
    # ç¡®è®¤ç»§ç»­
    print(f"\nâš ï¸  æ³¨æ„ï¼šå¢å¼ºåçš„æ•°æ®å°†å ç”¨çº¦ {estimated_size_gb:.1f} GB ç©ºé—´")
    confirm = input("æ˜¯å¦ç»§ç»­? (y/n): ").strip().lower()
    
    if confirm != 'y':
        print("æ“ä½œå·²å–æ¶ˆ")
        return
    
    # æ‰§è¡Œå¢å¼º
    start_time = time.time()
    
    augment_dataset(
        data_dir=args.data_dir,
        output_dir=args.output_dir,
        augmentation_factor=args.augment_factor,
        max_positive_samples=args.max_positive,
        validate=not args.no_validate
    )
    
    elapsed_time = time.time() - start_time
    print(f"\nâ±ï¸  æ€»è€—æ—¶: {elapsed_time/60:.1f} åˆ†é’Ÿ")
    
    # æ˜¾ç¤ºå¦‚ä½•åœ¨è®­ç»ƒä¸­ä½¿ç”¨
    print("\nğŸ’¡ ä½¿ç”¨æç¤º:")
    print(f"  åœ¨è®­ç»ƒæ—¶ï¼Œå°† data_dir è®¾ç½®ä¸º: {args.output_dir}")
    print(f"  å¹¶å°†æ–‡ä»¶åä» 'xxx.npy' æ”¹ä¸º 'xxx_aug.npy'")
    print(f"  æˆ–è€…åœ¨ integrated_training_pipeline.py ä¸­è®¾ç½®:")
    print(f"    config['data_dir'] = '{args.output_dir}'")
    print(f"    config['data_type'] = 'augmented'")

if __name__ == "__main__":
    main()