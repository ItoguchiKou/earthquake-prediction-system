"""
è§£è€¦çš„åœ°éœ‡é¢„æµ‹è®­ç»ƒæµç¨‹
æ•°æ®å¢å¼ºå·²å®Œå…¨åˆ†ç¦»ï¼Œåªä½¿ç”¨é¢„å¤„ç†å¥½çš„æ•°æ®
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
import pandas as pd
import os
import sys
import json
import time
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from models.lightweight_model import create_lightweight_model
from models.loss_functions import EarthquakeRegressionLoss
from models.model_utils import save_checkpoint, load_checkpoint

class EarthquakeDataset(Dataset):
    """ç®€åŒ–çš„åœ°éœ‡æ•°æ®é›† - ä¸åŒ…å«ä»»ä½•å¢å¼ºé€»è¾‘"""
    
    def __init__(self,
                 features_path: str,
                 stat_features_path: str,
                 labels_path: str,
                 timestamps_path: str,
                 use_mmap: bool = True):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            features_path: ç‰¹å¾æ–‡ä»¶è·¯å¾„
            stat_features_path: ç»Ÿè®¡ç‰¹å¾æ–‡ä»¶è·¯å¾„
            labels_path: æ ‡ç­¾æ–‡ä»¶è·¯å¾„
            timestamps_path: æ—¶é—´æˆ³æ–‡ä»¶è·¯å¾„
            use_mmap: æ˜¯å¦ä½¿ç”¨å†…å­˜æ˜ å°„
        """
        # åŠ è½½æ•°æ®
        if use_mmap:
            try:
                self.features = np.load(features_path, mmap_mode='r')
                self.stat_features = np.load(stat_features_path, mmap_mode='r')
                self.labels = np.load(labels_path, mmap_mode='r')
            except:
                self.features = np.load(features_path)
                self.stat_features = np.load(stat_features_path)
                self.labels = np.load(labels_path)
        else:
            self.features = np.load(features_path)
            self.stat_features = np.load(stat_features_path)
            self.labels = np.load(labels_path)
        
        # åŠ è½½æ—¶é—´æˆ³
        timestamps_raw = np.load(timestamps_path)
        if timestamps_raw.dtype == np.float64:
            self.timestamps = pd.to_datetime(timestamps_raw, unit='s')
        else:
            self.timestamps = pd.to_datetime(timestamps_raw)
        
        # æ•°æ®ä¿¡æ¯
        self.grid_height = self.features.shape[2]
        self.grid_width = self.features.shape[3]
        
        # è®¡ç®—æ­£æ ·æœ¬ç‡ï¼ˆè¿™é‡Œæ”¹ä¸ºé«˜æ¦‚ç‡æ ·æœ¬ç‡ï¼‰
        high_prob_mask = np.any(self.labels > 0.3, axis=1)
        high_prob_ratio = np.mean(high_prob_mask)
        
        print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ:")
        print(f"  æ ·æœ¬æ•°: {len(self.features)}")
        print(f"  ç‰¹å¾å½¢çŠ¶: {self.features.shape}")
        print(f"  é«˜æ¦‚ç‡(>0.3)æ ·æœ¬ç‡: {high_prob_ratio:.2%}")
    
    def get_sampler(self):
        """è·å–é‡‡æ ·å™¨ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        # å¯¹äºè§£è€¦çš„ç®¡çº¿ï¼Œä¸ä½¿ç”¨ç‰¹æ®Šé‡‡æ ·å™¨
        # è®©DataLoaderä½¿ç”¨é»˜è®¤çš„éšæœºé‡‡æ ·
        return None
    
    def __len__(self):
        return len(self.features)
    
    def __getitem__(self, idx):
        # ç¡®ä¿è¿”å›è¿ç»­çš„æ•°ç»„
        if isinstance(self.features, np.memmap):
            features = np.array(self.features[idx], dtype=np.float32)
            stat_features = np.array(self.stat_features[idx], dtype=np.float32)
            labels = np.array(self.labels[idx], dtype=np.float32)
        else:
            features = self.features[idx].astype(np.float32)
            stat_features = self.stat_features[idx].astype(np.float32)
            labels = self.labels[idx].astype(np.float32)
        
        return (torch.from_numpy(features).float(),
                torch.from_numpy(stat_features).float(),
                torch.from_numpy(labels).float())

class SimplifiedTrainingPipeline:
    """ç®€åŒ–çš„è®­ç»ƒæµç¨‹ - ä¸“æ³¨äºè®­ç»ƒæœ¬èº«"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–è®­ç»ƒæµç¨‹
        
        Args:
            config: è®­ç»ƒé…ç½®
        """
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # è®¾ç½®éšæœºç§å­
        self._set_random_seed(config.get('random_seed', 42))
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.save_dir = config['save_dir']
        os.makedirs(self.save_dir, exist_ok=True)
        
        # ä¿å­˜é…ç½®
        config_path = os.path.join(self.save_dir, 'training_config.json')
        with open(config_path, 'w') as f:
            json.dump(config, f, indent=2)
        
        # æœ‰æ•ˆä»»åŠ¡ï¼ˆä¸å†æ’é™¤ä»»ä½•ä»»åŠ¡ï¼‰
        self.valid_tasks = config.get('valid_tasks', list(range(12)))
        
        print(f"ğŸš€ è®­ç»ƒæµç¨‹åˆå§‹åŒ–")
        print(f"  è®¾å¤‡: {self.device}")
        print(f"  è¾“å‡ºç›®å½•: {self.save_dir}")
        print(f"  æœ‰æ•ˆä»»åŠ¡: {self.valid_tasks}")
    
    def _set_random_seed(self, seed: int):
        """è®¾ç½®éšæœºç§å­"""
        np.random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
    
    def prepare_data(self) -> Tuple[Dataset, Dataset]:
        """å‡†å¤‡æ•°æ®é›†"""
        print("\nğŸ“‚ å‡†å¤‡æ•°æ®é›†...")
        
        data_dir = self.config['data_dir']
        data_type = self.config.get('data_type', 'augmented')  # 'original' or 'augmented'
        
        # æ ¹æ®æ•°æ®ç±»å‹é€‰æ‹©æ–‡ä»¶åç¼€
        suffix = '_aug' if data_type == 'augmented' else ''
        
        print(f"  æ•°æ®ç±»å‹: {data_type}")
        print(f"  æ•°æ®ç›®å½•: {data_dir}")
        
        # åˆ›å»ºè®­ç»ƒé›†
        train_dataset = EarthquakeDataset(
            features_path=os.path.join(data_dir, f"train_features{suffix}.npy"),
            stat_features_path=os.path.join(data_dir, f"train_stat_features{suffix}.npy"),
            labels_path=os.path.join(data_dir, f"train_labels{suffix}.npy"),
            timestamps_path=os.path.join(data_dir, f"train_timestamps{suffix}.npy"),
            use_mmap=self.config.get('use_mmap', True)
        )
        
        # åˆ›å»ºéªŒè¯é›†
        val_dataset = EarthquakeDataset(
            features_path=os.path.join(data_dir, f"val_features{suffix}.npy"),
            stat_features_path=os.path.join(data_dir, f"val_stat_features{suffix}.npy"),
            labels_path=os.path.join(data_dir, f"val_labels{suffix}.npy"),
            timestamps_path=os.path.join(data_dir, f"val_timestamps{suffix}.npy"),
            use_mmap=self.config.get('use_mmap', True)
        )
        
        return train_dataset, val_dataset
    
    def create_model(self, data_shape: Tuple[int, ...]) -> nn.Module:
        """åˆ›å»ºæ¨¡å‹"""
        print("\nğŸ—ï¸ åˆ›å»ºæ¨¡å‹...")
        
        model_type = self.config.get('model_type', 'standard')
        
        # åˆ›å»ºè½»é‡çº§æ¨¡å‹
        model = create_lightweight_model(
            data_shape=data_shape,
            model_type=model_type,
            valid_tasks=self.valid_tasks
        )
        
        # è®¾ç½®æŸå¤±å‡½æ•° - ä¿®å¤å‚æ•°
        model.loss_function = EarthquakeRegressionLoss(
            loss_type=self.config.get('loss_type', 'huber'),
            consistency_weight=self.config.get('consistency_weight', 0.1),
            ignore_tasks=self.config.get('ignore_tasks', [])
        ).to(self.device)
        
        # æ·»åŠ compute_lossæ–¹æ³•
        model.compute_loss = lambda pred, target: model.loss_function(pred, target)
        
        return model.to(self.device)
    
    def create_optimizer_scheduler(self, model: nn.Module, num_training_steps: int):
        """åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨"""
        print("\nâš™ï¸ è®¾ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨...")
        
        # ä¼˜åŒ–å™¨
        optimizer_type = self.config.get('optimizer', 'adam')
        lr = self.config.get('learning_rate', 1e-3)
        weight_decay = self.config.get('weight_decay', 1e-4)
        
        if optimizer_type == 'adam':
            optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
        elif optimizer_type == 'adamw':
            optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
        else:
            raise ValueError(f"æœªçŸ¥ä¼˜åŒ–å™¨: {optimizer_type}")
        
        # è°ƒåº¦å™¨
        scheduler_type = self.config.get('scheduler', 'cosine')
        
        if scheduler_type == 'cosine':
            scheduler = optim.lr_scheduler.CosineAnnealingLR(
                optimizer,
                T_max=self.config['epochs'],
                eta_min=lr * 0.01
            )
        elif scheduler_type == 'onecycle':
            scheduler = optim.lr_scheduler.OneCycleLR(
                optimizer,
                max_lr=lr,
                total_steps=num_training_steps,
                pct_start=0.3,
                anneal_strategy='cos'
            )
        elif scheduler_type == 'plateau':
            scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                optimizer,
                mode='min',  # å¯¹äºå›å½’ä»»åŠ¡ï¼Œæœ€å°åŒ–æŸå¤±
                factor=0.5,
                patience=5,
                verbose=True
            )
        else:
            scheduler = None
        
        return optimizer, scheduler
    
    def train(self):
        """æ‰§è¡Œè®­ç»ƒ"""
        print("\nğŸ¯ å¼€å§‹è®­ç»ƒæµç¨‹...")
        
        # å‡†å¤‡æ•°æ®
        train_dataset, val_dataset = self.prepare_data()
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        batch_size = self.config.get('batch_size', 16)
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=self.config.get('num_workers', 0),
            pin_memory=torch.cuda.is_available() and not isinstance(train_dataset.features, np.memmap)
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=self.config.get('num_workers', 0),
            pin_memory=torch.cuda.is_available() and not isinstance(val_dataset.features, np.memmap)
        )
        
        # åˆ›å»ºæ¨¡å‹
        sample_shape = (batch_size,) + train_dataset.features.shape[1:]
        model = self.create_model(sample_shape)
        
        # è®¡ç®—æ€»è®­ç»ƒæ­¥æ•°
        gradient_accumulation_steps = self.config.get('gradient_accumulation_steps', 1)
        num_training_steps = len(train_loader) * self.config['epochs'] // gradient_accumulation_steps
        
        # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        optimizer, scheduler = self.create_optimizer_scheduler(model, num_training_steps)
        
        # å¯¼å…¥è®­ç»ƒå™¨
        from training_strategy import ImprovedEarthquakeTrainer
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = ImprovedEarthquakeTrainer(
            model=model,
            train_dataset=train_dataset,
            val_dataset=val_dataset,
            config=self.config
        )
        
        # æ‰§è¡Œè®­ç»ƒ
        trainer.train()
        
        return trainer

def create_training_config(mode: str = 'standard') -> Dict[str, Any]:
    """åˆ›å»ºè®­ç»ƒé…ç½® - å›å½’ç‰ˆæœ¬"""
    
    base_config = {
        'data_dir': '../data/augmented_data',
        'data_type': 'augmented',
        'save_dir': f'earthquake_regression_model_{mode}_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
        'random_seed': 42,
        'ignore_tasks': [],  # ä¸å†å¿½ç•¥ä»»ä½•ä»»åŠ¡
        'valid_tasks': list(range(12)),  # æ‰€æœ‰ä»»åŠ¡éƒ½æœ‰æ•ˆ
        'loss_type': 'huber',  # ä½¿ç”¨HuberæŸå¤±
        'consistency_weight': 0.1,
        'num_workers': 0,
        'use_mmap': True,
        'grad_clip': 1.0,
        'save_interval': 5,
        'optimizer': 'adam',
        'weight_decay': 1e-4
    }
    
    # æ ¹æ®æ¨¡å¼è°ƒæ•´é…ç½®
    configs = {
        'debug': {
            **base_config,
            'model_type': 'minimal',
            'batch_size': 8,
            'learning_rate': 1e-3,
            'epochs': 5,
            'gradient_accumulation_steps': 1,
            'early_stop_patience': 3,
            'scheduler': 'cosine'
        },
        'standard': {
            **base_config,
            'model_type': 'standard',
            'batch_size': 16,
            'learning_rate': 5e-4,
            'epochs': 50,
            'gradient_accumulation_steps': 2,
            'early_stop_patience': 10,
            'scheduler': 'cosine'
        },
        'fast': {
            **base_config,
            'model_type': 'minimal',
            'batch_size': 32,
            'learning_rate': 1e-3,
            'epochs': 20,
            'gradient_accumulation_steps': 1,
            'early_stop_patience': 5,
            'scheduler': 'onecycle'
        },
        'intensive': {
            **base_config,
            'model_type': 'standard',
            'batch_size': 8,
            'learning_rate': 3e-4,
            'epochs': 80,
            'gradient_accumulation_steps': 4,
            'early_stop_patience': 15,
            'scheduler': 'plateau',
            'weight_decay': 5e-4
        },
        'aggressive': {
            **base_config,
            'model_type': 'standard',
            'batch_size': 16,
            'learning_rate': 2e-3,  # ä»5e-4æé«˜åˆ°2e-3
            'epochs': 50,
            'gradient_accumulation_steps': 1,  # ä»2æ”¹ä¸º1
            'early_stop_patience': 10,
            'scheduler': 'onecycle',  # ä»cosineæ”¹ä¸ºonecycle
            'loss_type': 'mse',  # ä»huberæ”¹ä¸ºmse
            'consistency_weight': 0.05  # ä»0.1é™ä½åˆ°0.05
        }
    }
    
    if mode not in configs:
        raise ValueError(f"æœªçŸ¥æ¨¡å¼: {mode}. å¯é€‰: {list(configs.keys())}")
    
    return configs[mode]

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒ åœ°éœ‡é¢„æµ‹è®­ç»ƒç³»ç»Ÿ")
    print("="*60)
    
    # æ£€æŸ¥æ•°æ®
    data_dirs = {
        'original': '../data/processed_grid',
        'augmented': '../data/augmented_data'
    }
    
    print("\nğŸ“Š æ£€æŸ¥å¯ç”¨æ•°æ®:")
    available_data = {}
    
    for data_type, data_dir in data_dirs.items():
        if data_type == 'augmented':
            train_file = os.path.join(data_dir, 'train_features_aug.npy')
        else:
            train_file = os.path.join(data_dir, 'train_features.npy')
        
        if os.path.exists(train_file):
            # å¿«é€Ÿæ£€æŸ¥æ ·æœ¬æ•°
            try:
                features = np.load(train_file, mmap_mode='r')
                n_samples = len(features)
                available_data[data_type] = n_samples
                print(f"  âœ“ {data_type}: {n_samples} ä¸ªè®­ç»ƒæ ·æœ¬")
            except:
                print(f"  âœ— {data_type}: æ— æ³•è¯»å–")
        else:
            print(f"  âœ— {data_type}: ä¸å­˜åœ¨")
    
    # é€‰æ‹©æ•°æ®ç±»å‹
    if len(available_data) == 0:
        print("\nâŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ•°æ®!")
        print("è¯·å…ˆè¿è¡Œæ•°æ®å¤„ç†è„šæœ¬ã€‚")
        return
    
    if len(available_data) == 1:
        data_type = list(available_data.keys())[0]
        print(f"\nè‡ªåŠ¨é€‰æ‹©: {data_type} æ•°æ®")
    else:
        print("\nè¯·é€‰æ‹©æ•°æ®ç±»å‹:")
        print("1. åŸå§‹æ•°æ®")
        print("2. å¢å¼ºæ•°æ® (æ¨è)")
        
        choice = input("è¯·è¾“å…¥é€‰æ‹© (1/2, é»˜è®¤ä¸º2): ").strip() or "2"
        data_type = 'original' if choice == '1' else 'augmented'
    
    # é€‰æ‹©è®­ç»ƒæ¨¡å¼
    print("\nè¯·é€‰æ‹©è®­ç»ƒæ¨¡å¼:")
    print("1. è°ƒè¯•æ¨¡å¼ (5 epochs, å¿«é€Ÿæµ‹è¯•)")
    print("2. æ ‡å‡†æ¨¡å¼ (50 epochs, æ¨è)")
    print("3. å¿«é€Ÿæ¨¡å¼ (20 epochs, å¿«é€Ÿè®­ç»ƒ)")
    print("4. æ·±åº¦æ¨¡å¼ (80 epochs, æœ€ä½³æ•ˆæœ)")
    print("5. æ¿€è¿›æ¨¡å¼ (30 epochs, MSEæŸå¤±)")
    mode_input = input("è¯·è¾“å…¥é€‰æ‹© (1/2/3/4/5, é»˜è®¤ä¸º2): ").strip() or "2"
    mode_map = {'1': 'debug', '2': 'standard', '3': 'fast', '4': 'intensive', '5': 'aggressive'}
    mode = mode_map.get(mode_input, 'standard')
    
    # åˆ›å»ºé…ç½®
    config = create_training_config(mode)
    config['data_dir'] = data_dirs[data_type]
    config['data_type'] = data_type
    
    print(f"\nğŸ“‹ è®­ç»ƒé…ç½® ({mode}æ¨¡å¼):")
    print(f"  æ•°æ®: {data_type} ({available_data.get(data_type, 0)} æ ·æœ¬)")
    print(f"  æ¨¡å‹: {config['model_type']}")
    print(f"  æ‰¹æ¬¡å¤§å°: {config['batch_size']}")
    print(f"  å­¦ä¹ ç‡: {config['learning_rate']}")
    print(f"  è®­ç»ƒè½®æ•°: {config['epochs']}")
    print(f"  æŸå¤±å‡½æ•°: {config['loss_type']}")
    print(f"  è°ƒåº¦å™¨: {config.get('scheduler', 'cosine')}")
    
    # ç¡®è®¤å¼€å§‹è®­ç»ƒ
    confirm = input("\næ˜¯å¦å¼€å§‹è®­ç»ƒ? (y/n): ").strip().lower()
    if confirm != 'y':
        print("è®­ç»ƒå·²å–æ¶ˆ")
        return
    
    # åˆ›å»ºè®­ç»ƒæµç¨‹
    pipeline = SimplifiedTrainingPipeline(config)
    
    try:
        # æ‰§è¡Œè®­ç»ƒ
        trainer = pipeline.train()
        print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

if __name__ == "__main__":
    main()