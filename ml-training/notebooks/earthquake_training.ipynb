{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7701118",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# è·å–SageMakerè§’è‰²å’Œä¼šè¯\n",
    "role = get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# æŒ‡å®šä½ çš„S3å­˜å‚¨æ¡¶å’Œè·¯å¾„\n",
    "bucket = \"sagemaker-earthquake-prediction-shuhao\"\n",
    "data_prefix = \"earthquake-data/processed/2\"\n",
    "model_prefix = \"earthquake-data/models/2\"\n",
    "\n",
    "print(\"ğŸŒ åœ°éœ‡é¢„æµ‹Random Forestè®­ç»ƒç³»ç»Ÿ\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"SageMakerè§’è‰²: {role}\")\n",
    "print(f\"S3å­˜å‚¨æ¡¶: {bucket}\")\n",
    "print(f\"æ•°æ®è·¯å¾„: s3://{bucket}/{data_prefix}/\")\n",
    "print(f\"æ¨¡å‹ä¿å­˜è·¯å¾„: s3://{bucket}/{model_prefix}/\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48afc398",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2: éªŒè¯S3å­˜å‚¨æ¡¶å’Œæ•°æ®\n",
    "import boto3\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "print(\"ğŸ” éªŒè¯S3å­˜å‚¨æ¡¶å’Œæ•°æ®...\")\n",
    "\n",
    "# æ£€æŸ¥å­˜å‚¨æ¡¶æ˜¯å¦å­˜åœ¨\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket=bucket)\n",
    "    print(f\"âœ… å­˜å‚¨æ¡¶ {bucket} è®¿é—®æ­£å¸¸\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ å­˜å‚¨æ¡¶è®¿é—®å¤±è´¥: {e}\")\n",
    "    print(\"è¯·æ£€æŸ¥å­˜å‚¨æ¡¶åç§°å’ŒIAMæƒé™è®¾ç½®\")\n",
    "\n",
    "# æ£€æŸ¥æ•°æ®æ–‡ä»¶\n",
    "try:\n",
    "    response = s3_client.list_objects_v2(\n",
    "        Bucket=bucket,\n",
    "        Prefix=f\"{data_prefix}/\",\n",
    "        MaxKeys=50\n",
    "    )\n",
    "    \n",
    "    if 'Contents' in response:\n",
    "        feature_files = [obj['Key'] for obj in response['Contents'] \n",
    "                        if obj['Key'].endswith('_ç‰¹å¾ãƒ‡ãƒ¼ã‚¿.csv')]\n",
    "        \n",
    "        print(f\"\\nğŸ“ æ‰¾åˆ° {len(feature_files)} ä¸ªç‰¹å¾æ•°æ®æ–‡ä»¶:\")\n",
    "        total_size = 0\n",
    "        \n",
    "        for i, file_key in enumerate(feature_files[:10]):  # æ˜¾ç¤ºå‰10ä¸ª\n",
    "            try:\n",
    "                obj_info = s3_client.head_object(Bucket=bucket, Key=file_key)\n",
    "                size = obj_info['ContentLength']\n",
    "                total_size += size\n",
    "                filename = file_key.split('/')[-1]\n",
    "                print(f\"  {i+1:2d}. {filename} ({size:,} bytes)\")\n",
    "            except:\n",
    "                filename = file_key.split('/')[-1]\n",
    "                print(f\"  {i+1:2d}. {filename}\")\n",
    "        \n",
    "        if len(feature_files) > 10:\n",
    "            print(f\"  ... è¿˜æœ‰ {len(feature_files) - 10} ä¸ªæ–‡ä»¶\")\n",
    "        \n",
    "        print(f\"\\nğŸ“Š æ•°æ®æ–‡ä»¶æ€»è®¡: {len(feature_files)} ä¸ª\")\n",
    "        if total_size > 0:\n",
    "            print(f\"ğŸ“Š å·²æ£€æŸ¥æ–‡ä»¶æ€»å¤§å°: {total_size:,} bytes ({total_size/1024/1024:.2f} MB)\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"âš ï¸  åœ¨ s3://{bucket}/{data_prefix}/ ä¸‹æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶\")\n",
    "        print(\"è¯·ç¡®ä¿ç‰¹å¾æ•°æ®æ–‡ä»¶å·²ä¸Šä¼ åˆ°æ­£ç¡®ä½ç½®\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ£€æŸ¥æ•°æ®æ–‡ä»¶å¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b72e3ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3: æ‰§è¡Œè®­ç»ƒï¼ˆå¸¦å®æ—¶è¿›åº¦ç›‘æ§ï¼‰\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# è®­ç»ƒå‚æ•°\n",
    "bucket_name = bucket\n",
    "data_folder = data_prefix\n",
    "model_folder = model_prefix\n",
    "min_samples = 80  # æœ€å°æ ·æœ¬æ•°\n",
    "\n",
    "print(\"ğŸš€ å‡†å¤‡å¼€å§‹è®­ç»ƒ...\")\n",
    "print(f\"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"å‚æ•°é…ç½®:\")\n",
    "print(f\"  å­˜å‚¨æ¡¶: {bucket_name}\")\n",
    "print(f\"  æ•°æ®æ–‡ä»¶å¤¹: {data_folder}\")\n",
    "print(f\"  æ¨¡å‹ä¿å­˜æ–‡ä»¶å¤¹: {model_folder}\")\n",
    "print(f\"  æœ€å°æ ·æœ¬æ•°: {min_samples}\")\n",
    "\n",
    "# æ„å»ºè®­ç»ƒå‘½ä»¤\n",
    "train_command = [\n",
    "    sys.executable, \"/home/ec2-user/SageMaker/earthquake-training/scripts/sagemaker_train.py\",\n",
    "    \"--bucket-name\", bucket_name,\n",
    "    \"--data-folder\", data_folder,\n",
    "    \"--model-folder\", model_folder,\n",
    "    \"--min-samples\", str(min_samples)\n",
    "]\n",
    "\n",
    "print(f\"\\næ‰§è¡Œå‘½ä»¤:\")\n",
    "print(f\"{' '.join(train_command)}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"å¼€å§‹è®­ç»ƒ...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# æ‰§è¡Œè®­ç»ƒï¼ˆå®æ—¶æ˜¾ç¤ºè¾“å‡ºï¼‰\n",
    "start_time = time.time()\n",
    "process = subprocess.Popen(\n",
    "    train_command, \n",
    "    stdout=subprocess.PIPE, \n",
    "    stderr=subprocess.STDOUT,\n",
    "    universal_newlines=True,\n",
    "    bufsize=1\n",
    ")\n",
    "\n",
    "# å®æ—¶æ˜¾ç¤ºè¾“å‡º\n",
    "while True:\n",
    "    output = process.stdout.readline()\n",
    "    if output == '' and process.poll() is not None:\n",
    "        break\n",
    "    if output:\n",
    "        print(output.strip())\n",
    "\n",
    "# ç­‰å¾…è¿›ç¨‹å®Œæˆ\n",
    "return_code = process.poll()\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"è®­ç»ƒå®Œæˆ!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"è¿”å›ç : {return_code}\")\n",
    "print(f\"æ€»è€—æ—¶: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)\")\n",
    "print(f\"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "if return_code == 0:\n",
    "    print(\"ğŸ‰ è®­ç»ƒæˆåŠŸ!\")\n",
    "else:\n",
    "    print(\"âŒ è®­ç»ƒå¤±è´¥!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea4229a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 4: éªŒè¯è®­ç»ƒç»“æœ\n",
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "print(\"ğŸ” éªŒè¯è®­ç»ƒç»“æœ...\")\n",
    "\n",
    "# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶\n",
    "model_files = [\n",
    "    f\"{model_prefix}/random_forest_model.pkl\",\n",
    "    f\"{model_prefix}/label_encoder.pkl\", \n",
    "    f\"{model_prefix}/model_metadata.json\"\n",
    "]\n",
    "\n",
    "print(f\"\\nğŸ“ æ£€æŸ¥S3ä¸­çš„æ¨¡å‹æ–‡ä»¶:\")\n",
    "total_size = 0\n",
    "files_found = 0\n",
    "\n",
    "for file_key in model_files:\n",
    "    try:\n",
    "        response = s3_client.head_object(Bucket=bucket, Key=file_key)\n",
    "        size = response['ContentLength']\n",
    "        last_modified = response['LastModified']\n",
    "        total_size += size\n",
    "        files_found += 1\n",
    "        \n",
    "        filename = file_key.split('/')[-1]\n",
    "        print(f\"  âœ… {filename}\")\n",
    "        print(f\"     å¤§å°: {size:,} bytes ({size/1024/1024:.2f} MB)\")\n",
    "        print(f\"     ä¿®æ”¹æ—¶é—´: {last_modified}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        filename = file_key.split('/')[-1]\n",
    "        print(f\"  âŒ {filename} (ä¸å­˜åœ¨)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š æ¨¡å‹æ–‡ä»¶ç»Ÿè®¡:\")\n",
    "print(f\"  æ‰¾åˆ°æ–‡ä»¶: {files_found}/{len(model_files)}\")\n",
    "print(f\"  æ€»å¤§å°: {total_size:,} bytes ({total_size/1024/1024:.2f} MB)\")\n",
    "\n",
    "# è¯»å–å¹¶æ˜¾ç¤ºæ¨¡å‹å…ƒæ•°æ®\n",
    "if files_found >= 3:\n",
    "    try:\n",
    "        print(f\"\\nğŸ“‹ è¯»å–æ¨¡å‹å…ƒæ•°æ®...\")\n",
    "        metadata_obj = s3_client.get_object(Bucket=bucket, Key=f\"{model_prefix}/model_metadata.json\")\n",
    "        metadata = json.loads(metadata_obj['Body'].read().decode('utf-8'))\n",
    "        \n",
    "        print(f\"\\nğŸ¯ è®­ç»ƒç»“æœæ‘˜è¦:\")\n",
    "        print(f\"  æ¨¡å‹ç±»å‹: {metadata['model_type']}\")\n",
    "        print(f\"  æ¨¡å‹ç‰ˆæœ¬: {metadata.get('model_version', 'N/A')}\")\n",
    "        print(f\"  è®­ç»ƒæ—¶é—´: {metadata.get('training_time', 'N/A')}\")\n",
    "        print(f\"  è®­ç»ƒè€—æ—¶: {metadata['metrics'].get('training_time_seconds', 0):.1f}ç§’\")\n",
    "        \n",
    "        print(f\"\\nğŸ“Š æ¨¡å‹æ€§èƒ½:\")\n",
    "        metrics = metadata['metrics']\n",
    "        print(f\"  æµ‹è¯•é›†RÂ²å¾—åˆ†: {metrics['test_r2']:.4f}\")\n",
    "        print(f\"  æµ‹è¯•é›†MSE: {metrics['test_mse']:.4f}\")\n",
    "        print(f\"  æµ‹è¯•é›†MAE: {metrics['test_mae']:.4f}\")\n",
    "        if 'oob_score' in metrics and metrics['oob_score']:\n",
    "            print(f\"  OOBå¾—åˆ†: {metrics['oob_score']:.4f}\")\n",
    "        \n",
    "        print(f\"\\nğŸ—¾ è®­ç»ƒæ•°æ®:\")\n",
    "        print(f\"  å‚ä¸è®­ç»ƒçš„éƒ½é“åºœçœŒ: {metadata.get('num_prefectures', 0)}\")\n",
    "        print(f\"  æ€»æ ·æœ¬æ•°: {metadata.get('total_samples', 0):,}\")\n",
    "        print(f\"  ç‰¹å¾æ•°é‡: {len(metadata.get('feature_names', []))}\")\n",
    "        \n",
    "        print(f\"\\nğŸ† å‚ä¸è®­ç»ƒçš„éƒ½é“åºœçœŒ (å‰10ä¸ª):\")\n",
    "        prefecture_info = metadata.get('prefecture_info', [])\n",
    "        for i, (prefecture, count) in enumerate(prefecture_info[:10]):\n",
    "            print(f\"  {i+1:2d}. {prefecture}: {count:,} æ¡è®°å½•\")\n",
    "        \n",
    "        if len(prefecture_info) > 10:\n",
    "            print(f\"  ... è¿˜æœ‰ {len(prefecture_info) - 10} ä¸ªéƒ½é“åºœçœŒ\")\n",
    "        \n",
    "        print(f\"\\nğŸ¯ é‡è¦ç‰¹å¾ (Top 8):\")\n",
    "        feature_importance = metadata.get('feature_importance', {})\n",
    "        sorted_features = sorted(feature_importance.items(), \n",
    "                               key=lambda x: x[1], reverse=True)\n",
    "        for i, (feature, importance) in enumerate(sorted_features[:8]):\n",
    "            print(f\"  {i+1}. {feature}: {importance:.4f}\")\n",
    "        \n",
    "        # æ€§èƒ½è¯„ä¼°\n",
    "        r2_score = metrics['test_r2']\n",
    "        if r2_score >= 0.7:\n",
    "            print(f\"\\nğŸ‰ æ¨¡å‹æ€§èƒ½: ä¼˜ç§€ (RÂ² = {r2_score:.4f})\")\n",
    "        elif r2_score >= 0.5:\n",
    "            print(f\"\\nğŸ‘ æ¨¡å‹æ€§èƒ½: è‰¯å¥½ (RÂ² = {r2_score:.4f})\")\n",
    "        elif r2_score >= 0.3:\n",
    "            print(f\"\\nâš ï¸  æ¨¡å‹æ€§èƒ½: ä¸€èˆ¬ (RÂ² = {r2_score:.4f})\")\n",
    "        else:\n",
    "            print(f\"\\nâŒ æ¨¡å‹æ€§èƒ½: éœ€è¦æ”¹è¿› (RÂ² = {r2_score:.4f})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è¯»å–æ¨¡å‹å…ƒæ•°æ®å¤±è´¥: {e}\")\n",
    "       \n",
    "else:\n",
    "   print(f\"âš ï¸  æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´ï¼Œæ— æ³•è¯»å–å…ƒæ•°æ®\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"è®­ç»ƒéªŒè¯å®Œæˆ!\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
